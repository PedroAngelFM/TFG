\newpage
\section{Métodos de Clasificación Lineales}
\noindent Los métodos de clasificación buscan predecir una variable categórica $G$, por tanto, se tiene una división del espacio de observaciones en tantas partes como categorías tenga la variable objetivo. 
\begin{defi}
Llamaremos \textit{fronteras de decisión} a las fronteras de las regiones que definen la variables categóricas. 
\end{defi}
\noindent Suponiendo que la variable categórica tenga $1\ldots K$ categorías, entonces podemos crear un modelo lineal para cada uno de los indicadores, $\hat{f}_k(\textbf{x})=\hat{\beta}_k0+\hat{\beta}_k^T\textbf{x}$. 
La frontera de decisión entre dos categorías es el conjunto de puntos tal que $\hat{f}_k(\textbf{x})=\hat{f}_l(\textbf{x})$.
\begin{defi}
Llamaremos \textit{función discriminante} $\delta_k(x)$ a una función que dada una nueva observación $\textbf{x}_0$ permite distinguir si es de la $k$-ésima categoría. 
\end{defi}
\subsection{Regresión lineal de una matriz de indicadores}
\noindent Sea una variable categórica con $K$ clases posibles entonces cada observación de dicha categoría tiene $K$ componentes:
\begin{equation}
\begin{cases}
y_k=0 \quad &\text{si la observación no pertenece a la } k \text{-ésima categoría}\\
y_k=1 \quad &\text{si la observación pertenece a la } k \text{-ésima categoría}\\
\end{cases}
\end{equation}
Entonces se definen dos matrices resultado de tomar $n$ observaciones, la matriz de indicadores $\textbf{Y}$ de tamaño $n\times K$ con únicamente 0's y 1's. Además se crea la matriz de datos $\textbf{X}$ de tamaño $n\times p+1$ donde la primera columna es constante 1. Una vez ajustado el modelo lineal, se obtiene el predictor:
\begin{equation}
\hat{\textbf{Y}}=\textbf{X}(\textbf{X}^T \textbf{X} )^{-1}\textbf{X}^T \textbf{Y}  
\end{equation} 
Donde la matriz $\hat{\textbf{B}}=(\textbf{X}^T \textbf{X} )^{-1}\textbf{X}^T \textbf{Y}$ es la matriz de parámetros.

\noindent Para clasificar una nueva observación $\textbf{x}_0$ hay que seguir el siguiente proceso:
\begin{itemize}
\item Se calcula el vector $\hat{f}(\textbf{x}_0)=[(1,\textbf{x}_0)\hat{\textbf{B}}]^T$ un vector de $K$ componentes.
\item  Se toma la categoría $k$ como aquella que 
\end{itemize}
\subsection{Análisis Discriminante}
\noindent El objetivo del análisis discriminante es dada una población separada en varios grupos conocidos a priori, conseguir funciones que permitan determinar dadas nuevas observaciones en que grupo están las nuevas observaciones. 

\noindent Para ello, se busca una o varias combinaciones lineales de las variables $X_1\ldots X_p$ que brinden un criterio por el cual se pueda determinar si una nueva observación de las variables pertenece a alguno de los grupos conocidos. 

\subsection{Formalización del Análisis Discriminante}
 
\noindent Sea \textbf{X} la matriz de datos de tamaño $n \times p$
donde las filas $\textbf{x}_i$ son cada una de las observaciones de las $p$ variables. Dichas observaciones están particionadas en general por $q$ grupos, sea $I_k$ el conjunto de observaciones pertenecientes al $k$-ésimo grupo, sea también $n_k$ el número de observaciones que pertenecen al $k$-ésimo grupo.

\noindent Se definen las medias muestrales $\overline{x}_j=\frac{1}{n}\sum_{i=1}^n x_{ij}$ de la $j$-ésima variable en la población en total. También se define la media muestral dentro de cada grupo que es $\overline{x}_{jk}=\frac{1}{n_k}\sum 
_{i\in I_k} x_{ij}$ 

\noindent Por ende podemos dar la distancia entre dos variables como:
\begin{align}
Cov(X_j,X_{j'})&=\dfrac{1}{n}\sum_{i=1}^n(x_{ij}-\overline{x}_j)(x_{ij'}-\overline{x}_j')
\intertext{Esto se puede particionar por grupos de la siguiente manera: }
Cov(X_j,X_{j'})&=\dfrac{1}{n}\sum_{k=1}^q\sum_{i\in I_k}(x_{ij}-\overline{x}_j)(x_{ij'}-\overline{x}_j')
\intertext{y a su vez cada uno de los $(x_{ij}-\overline{x}_j)$ se pueden dividir en la parte intergrupos e intragrupos: }
(x_{ij}-\overline{x}_j)&=(x_{ij}-\overline{x}_{jk})+(\overline{x}_{jk}-\overline{x}_{j})
\end{align}
Sustituyendo y simplificando lo necesario: 
\begin{equation}
Cov(X_j,X_{j'})=\dfrac{1}{n}\sum_{k=1}^q\sum_{i\in I_k}(x_{ij}-\overline{x}_{jk})(x_{ij'}-\overline{x}_{j'k})+\sum_{k=1}^q\dfrac{n_k}{n}(\overline{x}_{jk}-\overline{x}_{j})(\overline{x}_{j'k}-\overline{x}_{j'})
\end{equation}

\noindent Esto nos permite dar una descomposición de la matriz de covarianzas total de la siguiente forma :
\begin{equation}\label{descomposicion varianza}
\textbf{T}=\textbf{B}+\textbf{W}
\end{equation}
\newpage
Donde:
\begin{itemize}
\item \textbf{T} es la matriz que expresa la covarianza total y sus coeficientes  $t_{jj'}=Cov(X_j,X_j')$
\item \textbf{B} es la matriz que expresa la covarianza entre los grupos y sus coeficientes son $b_{jj'}=\sum_{k=1}^q\dfrac{n_k}{n}(\overline{x}_{jk}-\overline{x}_{j})(\overline{x}_{j'k}-\overline{x}_{j'})$
\item \textbf{W} es la matriz que expresa la covarianza dentro de los grupos y sus coeficientes son $w_{jj'}=\dfrac{1}{n}\sum_{k=1}^q\sum_{i\in I_k}(x_{ij}-\overline{x}_{jk})(x_{ij'}-\overline{x}_{j'k})$
\end{itemize}

\noindent Para cualquier combinación lineal que se quiera hacer de las variables de entrada de la forma $\textbf{a}^T \textbf{x}$, donde el vector $\textbf{a}$ es un vector de $p$ constantes,  entonces la varianza se transforma de la siguiente manera:
\begin{equation}
Var(\textbf{a}^T \textbf{x})=\textbf{a}^T \Sigma \textbf{a}
\end{equation}

\noindent Entonces transformando por el vector $\textbf{a}$ tenemos que la Ecuación \eqref{descomposicion varianza} se transforma de la siguiente manera: 
\begin{equation}
\textbf{a}^T \textbf{T}\textbf{a}= \textbf{a}^T \textbf{B}\textbf{a}+\textbf{a}^T \textbf{W}\textbf{a}
\end{equation}

Recopilando, el objetivo del análisis discriminante lineal es encontrar combinaciones lineales que maximicen la varianza entre grupos y minimicen la varianza dentro de los grupos. Eso es equivalente a encontrar el vector $\textbf{a}$ tal que:
\begin{equation}
f(\textbf{a})=\dfrac{\textbf{a}^T \textbf{B}\textbf{a}}{\textbf{a}^T \textbf{T}\textbf{a}}
\end{equation}
\noindent Es máxima. Si además utilizamos la restricción $\textbf{a}^T \textbf{T}\textbf{a} = 1$. En principio la función objetiva es homogénea, es decir, $f(\mu \textbf{a})=f(\textbf{a})$
Utilizando el método de los multiplicadores de Lagrange derivamos respecto del vector $\textbf{a}$ tendremos que:
\begin{align}
L(\textbf{a})&= \textbf{a}^T \textbf{B}\textbf{a}-\lambda(\textbf{a}^T \textbf{T}\textbf{a}-1) 
\intertext{al derivarla respecto de \textbf{a} se obtiene que: }
\dfrac{\partial L(\textbf{a})}{\partial \textbf{a} } &= 2\textbf{B}\textbf{a}-2\lambda\textbf{T}\textbf{a}
\intertext{En consecuencia: }
\textbf{B}\textbf{a} &= \lambda \textbf{T} \textbf{a}
\intertext{Si además \textbf{T} es no singular}
\textbf{T}^{-1}\textbf{B}\textbf{a}&=\lambda \textbf{a}
\end{align}

Es decir, el vector $\textbf{a}$ es el vector de valor propio $\lambda$, tomando el valor propio máximo de la matriz $\textbf{T}^{-1}\textbf{B}$.

\begin{defi}
Al valor $\lambda$ se le conoce como \textit{potencia discriminante} de la combinación \textbf{a}.
\end{defi}

\noindent  

\noindent \textit{Observación} Esta técnica se diferencia del Análisis de Componentes Principales en que en el Análisis Discriminante se maximiza la distancia o variación entre grupos conocidos, mientras que el Análisis de Componentes Principales únicamente busca las direcciones en las que los datos varían más. 



