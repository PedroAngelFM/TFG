\section{Análisis de Clusters}

\noindent Según \emph{Jain,A.K. y Richard, C.D; }\cite{Jain 1988} el análisis de clusters tiene como objetivo conseguir una clasificación de las observaciones en subconjuntos que tengan sentido de acuerdo al contexto de la investigación. El clustering es un tipo de clasificación de los datos que tiene las siguientes características:
\begin{itemize}
\item \textit{Exclusividad: }Una observación no puede pertenecer a más de un cluster a la vez. 
\item \textit{Es intrínseco: }No hay ninguna etiqueta que permita clasificar las observaciones, son las características de las propias muestras las que servirán como diferenciadores. Lo que hace que sea un método \textit{no supervisado}.
\end{itemize}

\noindent Una vez clarificado las características esenciales del clustering, este se puede dar de manera \emph{jerárquica} o \emph{particional}.

\begin{defi}
Se dice que un método de clustering es \emph{jerárquico} si genera una secuencia de particiones del espacio de observaciones las cuales están anidadas 
\end{defi}

\begin{defi}
Un método de clustering \emph{particional} genera una única partición del espacio. 
\end{defi}

\noindent Por otro lado, los algoritmos se pueden clasificar según  sean \textit{aglomerativos}, en los que se empieza teniendo cada una de las observaciones y se van formando los clusters hasta tener un solo cluster con todas las muestras. Por el contrario, los \textit{algoritmos   divisivos}, se empieza con un solo cluster y se van haciendo subconjuntos del mismo.  

\noindent Otra clasificación puede venir dada por el modo en el que se consideran las  variables, si es \textit{Monotético}, en cada paso se centra en una variable, mientras que si es \textit{Politético} se utilizan todas las variables a la vez.

\noindent También hay que distinguir métodos que usan como fundamento el álgebra matricial, centrándose en los datos que nos puede dar la matriz de distancias entre observaciones o en la Teoría de Grafos. 

\noindent En primer lugar, es necesario definir los conceptos principales de esta sección que es un clustering y un cluster:

\begin{defi}
Se llama \textit{clustering} a la partición que provoca la relación $\mathcal{R}$ y se definen los \textit{clusters} como las clases de equivalencia de dicha relación, $\lbrace c_i\rbrace$. 
\end{defi}

\noindent Por tanto, el análisis de clusters es equivalente a establecer una relación de equivalencia entre las observaciones.  

\subsection{Algoritmos Jerárquicos}
\noindent Los algoritmos jerárquicos son aquellos que utilizan como entrada no la matriz de los datos per sé, en su lugar, estas utilizan como entrada la matriz de distancias o similaridad.

\noindent Uno de los inconvenientes mayores de este tipo de algoritmos es como calcular la matriz de similaridades o distancias. En el caso de que las variables sean numéricas y continuas entonces se puede utilizar la distancia euclídea entre elementos de $\mathbb{R}^p$. Además hay que tener en cuenta si se va a estandarizar ya que en el caso de que si se estandarice se dará el mismo peso a todas las variables, dando igual su escala, en cambio si no se estandariza, la escala provoca que las características con mayores valores tengan mayor peso en las distancias. Por otro lado, si se tienen variables binarias o categóricas hay que tener en cuenta cómo medir sus distancias, todo esto se soluciona con las siguiente definición y consideraciones:

\begin{defi}
Llamaremos similaridad entre dos muestras $i,h$ según la variable $j$ a una función $s_{jih}$ la cual cumpla que:
\begin{itemize}
\item La similaridad de una muestra consigo misma es igual a la unidad $s_{jii}=1 \forall j,i$
\item $0\leq s_{jih} \leq 1\quad \forall j,i,h $
\item Simetría: $s_{jih}=s_{jhi}$
\end{itemize}
\end{defi}

\noindent Una vez considerado esto, se puede crear un coeficiente de similaridad entre dos mediante el coeficiente de \emph{Gower}:
\begin{equation}
s_{ih}=\dfrac{\sum_{j=1}^{p}w_{jih}s_{jih}}{\sum_{j=1}^{p}w_{jih}}
\end{equation}
\noindent Donde la variable $w_{jih}$ puede ser 0 ó 1, dependiendo de si se quiere tomar o no dicha variable o no en la comparación de dichas muestras. 

\noindent Lo más habitual para formalizar las similaridades es tomar lo siguiente:
\begin{itemize}
\item Para continuas se puede tomar el valor $s_{jih}=1-\frac{|x_{ji}-x_{hi}|}{rango(X_j)}$
\item En cambio para variables binarias en el caso de que coincida el atributo $x_{ji}=x_{jh}$ la similaridad es 1 y 0 en el caso contrario. También se puede decir que si hay presencia del atributo y coinciden es 1 y en caso contrario 0. 
\end{itemize}

\noindent Una vez decidida la forma de decir de inicialmente dar la matriz de distancias o similaridades, se da el algoritmo aglomerativo de generación de la jerarquía:
\begin{itemize}
\item Se comienza con un \emph{clustering} que tiene $n$ \emph{clusters} con una observación cada uno y se calcula la matriz de distancias
\item Se toman los elementos que más cercanos están y se forman una nueva clase.
\item Se sustituyen los dos elementos anteriores por la clase y se calcula la distancia entre la clase y el resto de observaciones de acuerdo con uno de los criterios preestablecidos. 
\item Repetir los dos pasos anteriores hasta obtener una única clase. 
\end{itemize}

\noindent Dependiendo de la forma que se de unir los grupos se obtendrá un algoritmo u otro de clustering:
\begin{itemize}
\item Si $f(x,y)=min(x,y)$ es decir, se toma como distancia entre la clase y el resto de observaciones como el mínimo de las que se den, a este algoritmo se le llama \textit{single linkage}, este tipo de enlace tiende a formar.
\item  Si $f(x,y)=max(x,y)$ es análogo al anterior, pero tomando la distancia máxima, a este método se le llama \textit{complete linkage}.
\item  Si $f(x,y)=\dfrac{x+y}{2}$ se hace la media de las distancias de los dos grupos antes de unirlos, a este método se le llama \textit{average linkage}.
\end{itemize}

\noindent Este tipo de métodos aporta una jerarquía en la agrupación de los datos en función de su similitud, de manera que a distintos grados de similitud se pueden establecer distintas reparticiones del espacio de observaciones. Además esto se puede representar fácilmente con un dendograma o diagrama de árboles otorgando una manera sencilla de establecer un criterio de similitud máxima. 

\subsection{Algoritmos Particionales}

\noindent El clustering particional busca dado un conjunto de datos heterogéneos dividirlos en grupos homogéneos, como por ejemplo la segmentación de clientes o votantes.
, buscaremos un criterio por el cual podamos afirmar que una partición del espacio es mejor que la anterior o no. En estos casos la variabilidad total de todas las observaciones se puede descomponer como la variabilidad intra-cluster y la inter-cluster como se detalla en la sección del análisis discriminante.

\begin{defi}
Se llama \emph{centroide} de un cluster, $C_k,\quad k=1\ldots K$ al elemento que resulta de hacer la media de todos los elementos del cluster $C_k$:
\begin{equation}
\overline{x}_{jk}=\dfrac{1}{|C_k|}\sum_{i\in C_k} x_{ij}
\end{equation}
\end{defi}

\begin{defi}
Se llama variabilidad o variación intra-cluster del cluster $C_k\quad k=1\ldots K$ a la siguiente expresión:
\begin{equation}
\mathbf{W}(C_k)=\sum_{i\in C_k}\sum_{j=1}^{p} (x_{ij}-\overline{x}_{jk})^2
\end{equation}
\noindent Esta medida también se podría dar como la media de las diferencias cuadradas entre las observaciones del cluster, pero se seleccionará esta expresión. 
\end{defi}

\noindent Una vez dadas estas definiciones y criterios se puede construir un algoritmo iterativo como es el método de K-means que se puede resumir de la siguiente manera, sabiendo el número inicial de clusters, por ejemplo $K$:
\begin{enumerate}
\item Se inicializa la asignación de manera aleatoria, agrupando en los $K$ clusters observaciones aleatoriamente. 
\item Se itera lo siguiente hasta que no haya cambio en los clusters:
\begin{enumerate}
\item Se calculan los centroides de cada uno de los $K$ clusters
\item Se reasignan las observaciones de acuerdo a qué centroide es más cercano según la distancia euclidiana. 
\end{enumerate}
\end{enumerate}

\begin{propo}
El algoritmo de $K$-means sólo puede mejorar el criterio de la variación dentro de clusters. 
\begin{proof}
El paso 2a minimiza la suma de las desviaciones cuadradas y en el paso 2b la reasignación solo puede mejorar eso ya que se busca que la distancia euclídea al \emph{centroide} sea la mínima. 
\end{proof}
\end{propo}

\noindent Esto provoca que el algoritmo no para hasta que no se da una mejora en el criterio. Sin embargo, este minimo que alcanza la función no tiene por qué ser global, ya que esta no es convexa y por tanto la consecución de este mínimo puede estar condicionada por la asignación inicial. Para solucionar estos problemas, se deben hacer varias ejecuciones del algoritmo para poder compararlas entre sí, por ejemplo \emph{Scikit-Learn} implementa el método \emph{k-means++} que no es más que realizar multiples ejecuciones con asignaciones iniciales distintas y luego las compara y otorga la mejor. 
Para poder compararlas se puede utilizar el parámetro inercia. 

\begin{defi}
Se llama \emph{inercia} de un clustering a la suma de las variaciones intra-clusters:
\begin{equation}
inercia=\sum_{k=1}^K \textbf{W}(C_k)
\end{equation}
La inercia representa como de compacta es nuestra partición a menor inercia más compacta es y por tanto más homogéneos son. 
\end{defi}

\noindent Otro de los problemas de este tipo de algoritmos es que se deben conocer a priori el número de clusters en los que se quieren dividir las observaciones. Para ello \emph{Peña D.}\cite{Peña 2002} propone un contraste de hipótesis  utilizando la inercia haciéndola depender del número de clusters
\begin{equation}
F=\dfrac{inercia(K)-inercia(K+1)}{\frac{inercia(K+1)}{n-K-1}}
\end{equation}
Este cociente compara la disminución de la variación entre una partición con $K$ y $K+1$ clústers, de esta manera, puede compararse con una F con $p,p(n-K-1)$ grados de libertad, pero que en la práctica no se puede asumir las hipótesis, por tanto, en la práctica si se da un valor mayor que 10 se asume que es conveniente añadir un clúster más. 

