\chapter{Análisis de Componentes Principales}
\section{Introducción}

\noindent Dado un conjunto de datos, recogidos en una matriz \textbf{X}



%\noindent Sea una población de la que se han tomado $n$ observaciones de las cuales hemos medido $p$ variables. Del conjunto de datos que resulta, podemos dar la matriz de datos $\mathbf{X}$ de tamaño $n\times p$. 
%
%\noindent Una vez tenemos la matriz de datos, tenemos que cada 
%
%El análisis de componentes principales, aunque normalmente se utiliza con el objetivo de representar los datos de manera sencilla, también podría llegar a ser útil en la reducción de la dimensionalidad para evitar el overfitting en el aprendizaje automático. 
%
%Este método es una técnica matemática que dado un vector aleatorio $\textbf{X}^T=[X_1,\ldots X_p]$, con vector de media $\mu$ y matriz de covarianzas $\Sigma$, utiliza transformaciones ortogonales para conseguir las componentes principales. Aunque también es realizable con la media poblacional $\overline{x}$ y la matriz de varianzas \textbf{S} de manera análoga.  
%
%Estas componentes principales son combinaciones lineales de las variables que forman el vector, de manera que estando correladas las iniciales, las componentes no lo están y se busca calcularlas con la máxima varianza posible. 
%
%Esta transformación se busca ya que si varias variables están altamente correladas, entonces están aportando la misma información, siempre que no sean dependientes. Podemos encontrar por esta razón variables que no estén correladas, lo que implica que no dan la misma información y que en consecuencia pueden aportar mayor información acerca de la variación de los datos, sin tener que ser compartida o repetida por varias variables. 
%
%Al finalizar este Análisis de Componentes Principales, obtendremos un conjunto de variables nuevas no correladas entre sí, que son combinación lineal de las iniciales que maximizan la varianza en cada paso.
%
%Añadir que si las variables no están correladas o están cerca de no estarlo, el Análisis de Componentes Principales no tiene sentido ya que el conjunto de componentes principales será parecido a las variables iniciales, con la única diferencia que estarán ordenadas por orden creciente de varianza.  


\section{Definición y cálculo de las Componentes}

Sea un vector aleatorio $\textbf{X}^T=[X_1,\ldots X_p]$ con media $\mu$ y matriz de covarianzas $\Sigma$. 
\begin{defi}
Las componentes principales son combinaciones lineales de las variables $X_1 \ldots X_p$
\begin{equation}
\textbf{Y}_j=a_{1j}X_1+\ldots a_{pj}X_p=\textbf{a}_j^T\textbf{X}\quad 
\end{equation}

\noindent Donde $\textbf{a}_j$ es un vector de constantes y la variable $\textbf{Y}_j$ cumple lo siguiente:
\begin{itemize}
\item Si $j=1$ $Var(\textbf{Y}_1)$ es máxima restringido a $\textbf{a}_1^T \textbf{a}_1=1$
\item Si $j>1$ debe cumplir:
\begin{itemize}
\item $Cov(\textbf{Y}_j,\textbf{Y}_i)=0\quad \forall i<j $
\item $\textbf{a}_j^T \textbf{a}_j=1$
\item $Var(\textbf{Y}_j)$ es máxima. 
\end{itemize}

\end{itemize}

\end{defi}

\noindent El cálculo de la primera componente principal se lleva a cabo con un proceso de optimización de la función $Var(\textbf{Y}_1)$ sujeto a la restricción de que $\textbf{a}_1^T\textbf{a}_1=1$. Aplicando el método de los multiplicadores de Lagrange, dada una función $f(\textbf{x})=f(x_1\ldots x_p)$ diferenciable con una restricción $g(\textbf{x})=g(x_1\ldots x_p)=c$ entonces existe una constante $\lambda$ de manera la ecuación:
\begin{equation}
\dfrac{\partial f}{\partial x_i}-\lambda\dfrac{\partial g}{\partial x_i}=0 \quad i=1,\ldots p 
\end{equation}

\noindent Tiene como solución los puntos estacionarios de $f(\textbf{x})$

\newpage
Sea ahora la función $L(\textbf{x})= f(\textbf{x})-\lambda[g(x)-c]$  entonces podemos simplificar la expresión anterior a:
\begin{equation}
\dfrac{\partial L}{\partial \textbf{x}}=0
\end{equation}

\noindent En nuestro caso particular $L(\textbf{a}_1)=\textbf{a}_1^T \Sigma \textbf{a}_1 - \lambda[\textbf{a}_1^T \textbf{a}_1-1]$. Al derivarla obtenemos que :
\begin{align*}
\dfrac{\partial L}{\partial \textbf{a}_1} &= 2\Sigma \textbf{a}_1 - 2\lambda\textbf{a}_1\\
& = 2(\Sigma-\lambda)\textbf{a}_1 
\end{align*}

\noindent Igualando a 0 tenemos la siguiente ecuación: 
\begin{equation}
(\Sigma-\lambda I)\textbf{a}_1=0
\end{equation}

\noindent Para que la ecuación tenga una solución que no sea la trivial, tenemos que elegir $\lambda$ de manera que $|\Sigma-\lambda I| = 0$. Luego $\lambda$ es uno de los valores propios de la matriz. Generalmente una matriz $(p\times p)$ tiene $p$ valores propios $\lbrace\lambda_1, \ldots ,\lambda_p \rbrace$ y como $Var(\textbf{Y}_1)=Var(\textbf{a}_1^T\textbf{X})= \textbf{a}_1^T \Sigma \textbf{a}_1 =\textbf{a}_1^T \lambda \textbf{a}_1=\lambda$ que es la variable a maximizar, elegimos $\lambda=\max \lbrace\lambda_1, \ldots ,\lambda_p \rbrace$, por tanto, el vector $\textbf{a}_1$ es el vector propio con valor propio $\lambda=\lambda_1$ reordenando si es necesario.\\

\noindent Una vez calculada la primera componente principal $\textbf{Y}_1$, la segunda componente se calcula de manera análoga, maximizando $Var(\textbf{Y}_2)=Var(\textbf{a}_2^T\textbf{X})$ condicionada por $\textbf{a}_2^T\textbf{a}_2=1$. A esta restricción tenemos que añadir la restricción $Cov(\textbf{Y}_1,\textbf{Y}_2)=0 $

\begin{propo}
La condición $Cov(\textbf{Y}_1,\textbf{Y}_2)=0 $ equivale a la condición $\textbf{a}_2^T\textbf{a}_1 = 0$
\end{propo}
\begin{proof}
Utilizando que $\textbf{Y}_j=\textbf{a}_j^T \textbf{X}\quad \forall j$ , tenemos entonces que:
\begin{align*}
Cov(\textbf{Y}_2,\textbf{Y}_1)&= Cov (\textbf{a}_2^T\textbf{X},\textbf{a}_1^T\textbf{X})\\ 
&= E(\textbf{a}_2^T(\textbf{X}-\mu)(\textbf{X}-\mu)^T \textbf{a}_1)\\
&= \textbf{a}_2^T E((\textbf{X}-\mu)(\textbf{X}-\mu)^T) \textbf{a}_1\\
&= \textbf{a}_2^T \Sigma \textbf{a}_1 \\
&= \textbf{a}_2^T \lambda_1 \textbf{a}_1
\end{align*}
\noindent De manera que, si $a_2^T \lambda_1 a_1 = 0 \Rightarrow a_2^T a_1=0 $, luego son vectores ortogonales entre sí.
\end{proof}

\noindent \emph{Observación: } Esta proposición se puede extender de manera simple al caso de tener que calcular la $i$-ésima componente principal habiendo calculado las anteriores de las cuales sepamos sus valores propios. 

\begin{coro}
Las componentes principales son todas ortogonales entre sí. 
\end{coro}

\noindent Tomando la matriz formada por los $p$ vectores propios como columnas tenemos la matriz ortogonal $\textbf{A}=[\textbf{a}_1,\ldots, \textbf{a}_p]$,de manera que el vector aleatorio 
$$\textbf{Y}=[\textbf{Y}_1,\ldots , Y_p]^T=\textbf{A}\textbf{X}$$

De esta manera también obtenemos la diagonalización de la matriz de covarianzas 

\begin{equation}
\Lambda=\begin{pmatrix}
\lambda_1 & 0 & \cdots & 0\\
0 & \lambda_2 & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & \lambda_p
\end{pmatrix}
\end{equation}

Y también podemos deducir que 

\section{Reducción de la dimensionalidad}

\noindent Sea $\textbf{X}=[X_1,\ldots X_p]$ una matriz de datos multivariantes de tamaño $(n\times p)$ de las cuales ya hemos obtenido las componentes principales $Y_1 \ldots Y_p $. Cada individuo de los n que forman las filas de la matriz puede ser interpretado como un elemento de $\mathbb{R}^p$. 

\begin{defi}
Llamaremos matriz de distancias entre las n filas o individuos, y lo denotaremos $\Delta$ la matriz  
\end{defi}

\input{Documentos Extra/resumenCA.tex}