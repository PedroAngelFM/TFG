\chapter{Análisis de Componentes Principales}
\section{Introducción}

%\input{Documentos Extra/introduccionPCA.tex}

\section{Definición y cálculo de las Componentes}

\noindent Sea un vector aleatorio $\textbf{X}^T=[X_1,\ldots X_p]$ con media $\mu$ y matriz de covarianzas $\Sigma$. Según
\begin{defi}
Las componentes principales son combinaciones lineales de las variables $X_1 \ldots X_p$
\begin{equation}
\textbf{Y}_j=a_{1j}X_1+\ldots a_{pj}X_p=\textbf{a}_j^T\textbf{X}\quad 
\end{equation}

\noindent Donde $\textbf{a}_j$ es un vector de constantes y la variable $\textbf{Y}_j$ cumple lo siguiente:
\begin{itemize}
\item Si $j=1$ $Var(\textbf{Y}_1)$ es máxima restringido a $\textbf{a}_1^T \textbf{a}_1=1$
\item Si $j>1$ debe cumplir:
\begin{itemize}
\item $Cov(\textbf{Y}_j,\textbf{Y}_i)=0\quad \forall i<j $
\item $\textbf{a}_j^T \textbf{a}_j=1$
\item $Var(\textbf{Y}_j)$ es máxima. 
\end{itemize}

\end{itemize}

De esta manera, estamos buscando una nueva base que consiga reunir las direcciones de máxima variación. 
\end{defi}

\noindent El cálculo de la primera componente principal se lleva a cabo con un proceso de optimización de la función $Var(\textbf{Y}_1)$ sujeto a la restricción de que $\textbf{a}_1^T\textbf{a}_1=1$. Aplicando el método de los multiplicadores de Lagrange,  dada una función $f(\textbf{x})=f(x_1\ldots x_p)$ diferenciable con una restricción $g(\textbf{x})=g(x_1\ldots x_p)=c$ entonces existe una constante $\lambda$ de manera la ecuación:
\begin{equation}
\dfrac{\partial f}{\partial x_i}-\lambda\dfrac{\partial g}{\partial x_i}=0 \quad i=1,\ldots p 
\end{equation}

\noindent Tiene como solución los puntos estacionarios de $f(\textbf{x})$

%\newpage
\noindent Sea ahora la función $L(\textbf{x})= f(\textbf{x})-\lambda[g(x)-c]$  entonces podemos simplificar la expresión anterior a:
\begin{equation}
\dfrac{\partial L}{\partial \textbf{x}}=0
\end{equation}

\noindent Para el caso de las componentes principales, la función objetivo es la varianza de la combinación lineal, es decir $$

\noindent Para el cálculo de la $L(\textbf{a}_1)=\textbf{a}_1^T \Sigma \textbf{a}_1 - \lambda[\textbf{a}_1^T \textbf{a}_1-1]$. Al derivarla obtenemos que :
\begin{align*}
\dfrac{\partial L}{\partial \textbf{a}_1} &= 2\Sigma \textbf{a}_1 - 2\lambda\textbf{a}_1\\
& = 2(\Sigma-\lambda)\textbf{a}_1 
\end{align*}

\noindent Igualando a 0 tenemos la siguiente ecuación: 
\begin{equation}
(\Sigma-\lambda I)\textbf{a}_1=0
\end{equation}

\noindent Para que la ecuación tenga una solución que no sea la trivial, tenemos que elegir $\lambda$ de manera que $|\Sigma-\lambda I| = 0$. Luego $\lambda$ es uno de los valores propios de la matriz. Generalmente una matriz $(p\times p)$ tiene $p$ valores propios $\lbrace\lambda_1, \ldots ,\lambda_p \rbrace$ y como $Var(\textbf{Y}_1)=Var(\textbf{a}_1^T\textbf{X})= \textbf{a}_1^T \Sigma \textbf{a}_1 =\textbf{a}_1^T \lambda \textbf{a}_1=\lambda$ que es la variable a maximizar, elegimos $\lambda=\max \lbrace\lambda_1, \ldots ,\lambda_p \rbrace$, por tanto, el vector $\textbf{a}_1$ es el vector propio con valor propio $\lambda=\lambda_1$ reordenando si es necesario.\\

\noindent Una vez calculada la primera componente principal $\textbf{Y}_1$, la segunda componente se calcula de manera análoga, maximizando $Var(\textbf{Y}_2)=Var(\textbf{a}_2^T\textbf{X})$ condicionada por $\textbf{a}_2^T\textbf{a}_2=1$. A esta restricción tenemos que añadir la restricción $Cov(\textbf{Y}_1,\textbf{Y}_2)=0 $

\begin{propo}
La condición $Cov(\textbf{Y}_1,\textbf{Y}_2)=0 $ equivale a la condición $\textbf{a}_2^T\textbf{a}_1 = 0$.

\begin{proof}
Utilizando que $\textbf{Y}_j=\textbf{a}_j^T \textbf{X}\quad \forall j$ , tenemos entonces que:
\begin{align*}
Cov(\textbf{Y}_2,\textbf{Y}_1)&= Cov (\textbf{a}_2^T\textbf{X},\textbf{a}_1^T\textbf{X})\\ 
&= E(\textbf{a}_2^T(\textbf{X}-\mu)(\textbf{X}-\mu)^T \textbf{a}_1)\\
&= \textbf{a}_2^T E((\textbf{X}-\mu)(\textbf{X}-\mu)^T) \textbf{a}_1\\
&= \textbf{a}_2^T \Sigma \textbf{a}_1 \\
&= \textbf{a}_2^T \lambda_1 \textbf{a}_1
\end{align*}
\noindent De manera que, si $a_2^T \lambda_1 a_1 = 0 \Rightarrow a_2^T a_1=0 $, luego son vectores ortogonales entre sí.
\end{proof}
\end{propo}
\noindent \emph{Observación: } Esta proposición se puede extender de manera simple al caso de tener que calcular la $i$-ésima componente principal habiendo calculado las anteriores de las cuales sepamos sus valores propios. 

\begin{coro}
Las componentes principales son todas ortogonales entre sí. 
\end{coro}

\noindent Tomando la matriz formada por los $p$ vectores propios como columnas tenemos la matriz ortogonal $\textbf{A}=[\textbf{a}_1,\ldots, \textbf{a}_p]$,de manera que el vector aleatorio 
$$\textbf{Y}=[\textbf{Y}_1,\ldots , Y_p]^T=\textbf{A}\textbf{X}$$

De esta manera también obtenemos la diagonalización de la matriz de covarianzas 

\begin{equation}
\Lambda=\begin{pmatrix}
\lambda_1 & 0 & \cdots & 0\\
0 & \lambda_2 & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & \lambda_p
\end{pmatrix}
\end{equation}

Y también podemos deducir que 

\section{Reducción de la dimensionalidad}

\noindent Antes de continuar desarrollando, debemos definir unos conceptos previos. 

\begin{defi}
Sea $\textbf{A}\in \mathbb{M}_{n\times m}(\mathbb{R})$ definimos la \textit{norma de Fröbenius} de la matriz \textbf{A} como :
\begin{equation}
||A||_F=(tr(A^T\cdot A))^{\frac{1}{2}}
\end{equation}

\end{defi}

\begin{propo}
La norma de Fröbenius es invariante a transformaciones ortogonales
\end{propo}

\begin{proof}
Sea $U$ una matriz ortogonal, que cumple $U^T\cdot U=U\cdot U^T=I$, sea una matriz cualquiera $A$, entonces:
\begin{align*}\tag{2.7}
||U \cdot A||_F^2&=tr((U A)^T\cdot(U A))\\
&=tr((A^T U^T)\cdot U A))\\
&=tr(A^T A)\\
&=||A||_F^2
\end{align*}
\qedhere
\end{proof}

\begin{coro}
Sea $A$ una matriz y $A=U \Sigma V^T$ su descomposición en valores singulares, entonces $||A||_F=||\Sigma||_F=\Sigma_{i=1}^n \sigma_i$ donde $\sigma_i, i=1 \ldots n$ son los valores singulares de $A$. 
\end{coro}

\begin{teorema}[Teorema de Eckart-Young]

\end{teorema}




%\input{Documentos Extra/resumenCA.tex}