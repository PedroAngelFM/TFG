\chapter{Análisis de Componentes Principales}
\section{Introducción}

\noindent Dado un conjunto de $p$ variables correladas que debemos examinar, hay problemas cuando $p$ es especialmente alto. En aplicaciones como el campo del aprendizaje automático, si el número de variables estudiadas es mayor que el número de muestras recogidas, el modelo tenderá al overfitting y, por tanto, fallará al generalizar sus predicciones.

\noindent La base conceptual del análisis de componentes principales, PCA de ahora en adelante, es que en el caso de que tengamos un conjunto de variables correladas, entonces, dentro de las $p$ habrá variables que dan la misma información y por tanto, podemos reducir la dimensionalidad de nuestro conjunto de datos. 

\noindent El objetivo del análisis de componentes principales es comprobar y estudiar si combinaciones lineales de nuestras $p$ variables iniciales pueden traer consigo la mayor variación de los datos. 

\noindent Por último, el resultado obtenido tras aplicar el PCA, es un conjunto de variables no correladas. En principio, si el conjunto de las $p$ variables son cerca de no estar correladas, no tiene sentido aplicar PCA, ya que este encontrará variables cercanas y ordenadas por varianza. 

\newpage

\section{Definición y obtención de las Componentes Principales}

\noindent Sea un vector aleatorio $\textbf{X}=[X_1,\ldots,X_p]$ con media $\mu$ y matriz de covarianzas $\Sigma= Cov(X_i,X_j)$.\\[0.5 ex]

\noindent Llamaremos primera componente principal $\textbf{Y}_1$ del vector aleatorio \textbf{X} a una combinación lineal de las $p$ componentes:
$$\textbf{Y}_1=a_1^1 \textbf{X}_1 + \ldots a_p^1\textbf{X}_p=\textbf{a}_1^T\textbf{X}$$

\noindent Donde el vector $\textbf{a}_1\in \mathbb{R}^p$ y además se cumple que $\textbf{a}_1^T \textbf{a}_1=1$, es decir, unitario para evitar multiplicar la norma del vector por una constante arbitraria y poder continuar comparando.\\[2ex]
\noindent Este vector $\textbf{Y}_1=\textbf{a}_1^T \textbf{X}$ tiene como varianza $Var(\textbf{Y}_1)= Var (\textbf{a}_1^T \textbf{X})=\textbf{a}_1^T \Sigma \textbf{a}_1$. Esta es la función a maximizar, la cual está sometida a la restricción $\textbf{a}_1^T\textbf{a}_1=1$. 

Por tanto definimos $f(a_1^1,\ldots a_p^1)=Var(\textbf{Y_1})$ y $g(a_1^1,\ldots a_p^1)$