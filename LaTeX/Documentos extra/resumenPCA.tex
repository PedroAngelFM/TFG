\newpage
\section{Análisis de Componentes Principales}
\noindent El Análisis de componentes principales, es una de las técnicas multivariantes más antiguas en primera instancia fueron desarrolladas en paralelo por Pearson (1901) y Hotelling (1933) \cite{Jollife 1986} con distintos enfoques. 

\noindent Pearson en su aproximación, buscó la forma de ajustar de mejor manera puntos de un espacio de dimensión $p$ a una recta o plano. Es decir, Pearson buscó un enfoque de optimización geométrica que llevó a las componentes principales.

\noindent Por el otro lado Hotelling buscaba un conjunto de variables menor que pudiera determinar de igual manera o parecida los datos. Hotelling maximizó la contribución que aportaba cada componente a la varianza. Al utilizar para este fin el método de los multiplicadores de Lagrange, obtuvo el problema de valores principales que se desarrolla en esta sección. Aún así, el trabajo de Hotelling tenia ciertas diferencias con las actuales componentes principales, por ejemplo, consideraba que las variables originales  debían ser combinaciones de las componentes y no al revés. Tampoco usaba la notación matricial ni la matriz de covarianzas, en su lugar utilizaba la de correlaciones. 

\noindent El objetivo cuando se quiere aplicar este método es el de reducir la dimensionalidad o simplificar la estructura de los datos como el resto de métodos no supervisados. En este caso, se buscan las direcciones en las que los datos presentan la mayor variabilidad en orden decreciente de manera que las primeras componentes obtenidas deben ser las que mayor variabilidad expresan. 

\noindent En esta sección se detallará en principio cómo se calculan las componentes principales solo teniendo en cuenta las propiedades del vector aleatorio $\mathbf{x}$ de longitud $p$. Tras esto, tomaremos $N$ observaciones con las que poder construir estimadores de las componentes y se analizará como esta reducción de la dimensionalidad es la que permite una mejor reconstrucción de los datos mediante el Teorema de Eckart-Young. 

\subsection{Definición y cálculo de las Componentes}

\noindent Sea un vector aleatorio de longitud $p$, $\mathbf{x}=[X_1,\ldots X_p]$ con una distribución normal $p$-multivariante $N_p(\mu,\mathbf{\Sigma})$
\begin{defi}
\emph{Cuadras C.M.},\cite{Cuadras 2014} define las componentes principales 
\begin{equation}
Z_j=v_{1j}X_1+\ldots v_{pj}X_p=\mathbf{v}_j^T\mathbf{x}^T \quad j=1\ldots p
\end{equation}

\noindent Donde $\textbf{v}_j$ es un vector columna con $p$ escalares y la nueva variable aleatoria $Z_j$ cumple lo siguiente:
\begin{itemize}
\item Si $j=1$ $Var(\mathbf{z}_1)$ es máxima restringido a $\mathbf{v}_1^T \mathbf{v}_1=1$
\item Si $j>1$ debe cumplir:
\begin{itemize}
\item $Cov(\textbf{z}_j,\textbf{z}_i)=0\quad \forall i\neq j $
\item $\textbf{v}_j^T \textbf{v}_j=1$
\item $Var(Z_j)$ es máxima. 
\end{itemize}
\end{itemize}
\noindent De esta manera lo que se busca es una nueva base que reúna las direcciones de máxima variación y que sean ortogonales respecto a la matriz de covarianzas, es decir, que sean no correladas.
\end{defi}

\noindent \emph{Chatfield C. y Collins A.J} \cite{Chatfield 1989}, utilizan el método de los multiplicadores de Lagrange para resolver el problema de maximizar  $Var(Z_1)$ sujeto a la restricción $\textbf{v}_1^T\textbf{v}_1=1$. Todo esto con el objetivo de calcular el vector $\mathbf{v}_1$, el cálculo de sucesivas componentes cambia en ciertos aspectos. 

\noindent Aplicando el método de los multiplicadores, la función objetivo es la varianza de la combinación lineal, es decir, $f(\mathbf{x})=\mathbf{x}^T \mathbf{\Sigma} \mathbf{x}$ y la restricción aplicada es $g(\textbf{x})=\textbf{x}^T\textbf{x}=1$. 

\noindent Tomando $\textbf{x}=\textbf{v}_1$ se puede establecer $L(\textbf{v}_1)=\textbf{v}_1^T \mathbf{\Sigma} \textbf{v}_1 - \lambda[\textbf{v}_1^T \textbf{v}_1-1]$. Que al derivarla se obtiene:

\begin{align*}
\dfrac{\partial L}{\partial \textbf{v}_1} &= 2\mathbf{\Sigma} \textbf{v}_1 - 2\lambda\textbf{v}_1\\
& = 2(\mathbf{\Sigma}-\lambda)\textbf{v}_1 
\end{align*}

\noindent Igualando a 0 tenemos la siguiente ecuación: 
\begin{equation}
(\mathbf{\Sigma}-\lambda I)\textbf{v}_1=0
\end{equation}

\noindent Para que $\textbf{v}_1$ sea un vector no trivial, se elige $\lambda$ de tal manera que $|\mathbf{\Sigma}-\lambda I| = 0$, es decir, $\lambda$ es un vector propio de la matriz de covarianzas, $\mathbf{\Sigma}$. Al ser ésta una matriz semidefinido positiva y simétrica, los valores propios son reales y positivos. Por tanto, $\textbf{v}_1$ es un vector propio de la matriz de covarianza.

\noindent La función a maximizar es $Var(Z_1)=Var(\textbf{v}_1^T\textbf{x})=\textbf{v}_1^T\mathbf{\Sigma} \textbf{v}_1=\textbf{v}_1^T\lambda \textbf{v}_1=\lambda$, y para maximizarla basta tomar $\lambda=\max{\lbrace\lambda_1\ldots \lambda_p\rbrace}$. Reordenando si es necesario, se tiene que $\lambda=\lambda_1$, por tanto la primera componente es el vector propio $Z_1$ y además la varianza de la nueva variable cumple $Var(Z_1)=\lambda_1$ 

\noindent Una vez calculada la primera componente principal $Z_1$, la segunda componente se calcula de manera análoga, maximizando $Var(Z_2)=Var(\textbf{v}_2^T\textbf{x}^T)$ condicionada por $\textbf{v}_2^T\textbf{v}_2=1$. A esta restricción tenemos que añadir la restricción $Cov(Z_1,Z_2)=0 $

\begin{propo}
La condición $Cov(Z_1,Z_2)=0 $ equivale a la condición $\textbf{v}_2^T\textbf{v}_1 = 0$.
\begin{proof}
Utilizando que $Z_j=\textbf{v}_j^T \textbf{x}^T \quad \forall j=1,\ldots p$ , se tiene entonces que :
\begin{align*}
Cov(Z_2,Z_1)&= Cov (\textbf{v}_2^T\mathbf{x}^T,\mathbf{v}_1^T\mathbf{x})\\ 
&= \mathbb{E}(\mathbf{v}_2^T(\mathbf{x}^T-\mu^T)(\mathbf{x}^T-\mu^T)^T \mathbf{v}_1)\\
&= \textbf{v}_2^T \mathbb{E}((\textbf{x}^T-\mu^T)(\mathbf{x}^T-\mu^T)^T) \textbf{v}_1\\
&= \textbf{v}_2^T \mathbf{\Sigma} \textbf{v}_1 \\
&= \textbf{v}_2^T \lambda_1 \textbf{v}_1
\end{align*}
\noindent De manera que, si $\mathbf{v}_2^T \lambda_1 \mathbf{v}_1 = 0 \Rightarrow \mathbf{v}_2^T \mathbf{v}_1=0 $, luego son vectores ortogonales entre sí.
\end{proof}
\end{propo}


\noindent \emph{Observación: } Esta proposición se puede extender de manera simple al caso de tener que calcular la $j$-ésima componente principal habiendo calculado las anteriores de las cuales se sepan los valores propios asociados. 

\begin{coro}
Las componentes principales son todas ortogonales entre sí. 
\end{coro}

\noindent Para $k=2$, se dan dos restricciones, $\textbf{v}_2^T\textbf{v}_2=1$ y además $\textbf{v}_1^T \textbf{v}_2=0$. Para este caso existen $\lambda, \phi$ de manera que la función a maximizar es:
\begin{equation}
 L(\textbf{v}_2)=\textbf{v}_2^T \mathbf{\Sigma} \textbf{v}_2 - \lambda[\textbf{v}_2^T \textbf{v}_2-1]-\phi(\textbf{v}_1^T \textbf{v}_2)
\end{equation}
Que al ser derivado respecto $\textbf{v}_2$ obtenemos:
\begin{equation}
2\mathbf{\Sigma} \textbf{v}_2 - 2\lambda\textbf{v}_2-\phi \textbf{v}_1=0
\end{equation}
Que al multiplicar todo por $\textbf{v}_1^T$ resulta que $\phi=0$. De esta manera, se obtiene en la ecuación lo mismo que en el cálculo de la primera. 

\noindent Por tanto, $\lambda=\lambda_2$ que es el segundo valor propio más grande, y $\textbf{v}_2$ es el vector propio de valor propio $\lambda_2$.

\noindent El proceso para calcular el resto de componentes principales es análogo únicamente hay que tener en cuenta que se debe dar la ortogonalidad entre las distintas componentes respecto de la varianza. 

\noindent Por ende, se obtiene que las componentes principales vienen dadas por los vectores propios de la matriz de covarianzas $\mathbf{\Sigma} $. Además sabemos que $Var(\mathbf{v}_j^T \mathbf{x}^T)=\lambda_j,$ $ j=1,\ldots p$, donde $\lambda_j$ es el $j$-ésimo valor propio más grande. 

\noindent En esencia, calcular las componentes principales es calcular una base ortonormal que cumple una ciertas condiciones. Por lo tanto, podemos definir lo siguiente: 
\begin{defi}
Se llama matriz de cargas $\mathbf{V}$ a la matriz que tiene por columnas a los $\mathbf{v}_j$.
\end{defi}

\begin{propo}
La matriz $\mathbf{V}$ es una matriz ortonormal, de manera que $\mathbf{V}^T\mathbf{V}=\mathbf{V}\mathbf{V}^T=\mathbf{I}$
\end{propo}

\noindent La demostración de esta proposición es directa teniendo en cuenta la definición de cada una de las componentes principales. 

\noindent Esa matriz nos permite calcular las componentes de la siguiente manera
\begin{equation}
\mathbf{z}=\mathbf{x} \mathbf{V}
\end{equation} 
\noindent Donde el vector $\mathbf{z}=Z_1,\ldots, Z_p$, y donde cada $Z_j=\mathbf{v}_j^T\mathbf{x}$
\subsection{PCA muestral}
\noindent Ahora, veamos como esto se puede aplicar a una matriz de datos con la matriz de cuasivarianzas muestral o con la matriz de covarianzas. 

\noindent Sea $\mathbf{x}$ el vector aleatorio de longitud $p$ definida de la misma manera. A continuación se toman $N$ observaciones independientes de este vector y se obtiene la matriz de datos $\mathbf{X}$ de tamaño $N\times p$, en estos casos las componentes principales se definen de la misma manera. 



\begin{defi}
Dado el vector aleatorio de longitud $p$ del cual se han realizado $N$ observaciones entonces se puede definir la observación $\mathbf{z}=\mathbf{}$
\end{defi}

\noindent Por tanto, el proceso que se detalla para un vector aleatorio \textbf{x} con matriz de covarianzas $\mathbf{\Sigma}$ se puede extender a este caso en el conocemos la matriz de covarianzas muestrales \textbf{S}.

\noindent Con el objetivo de hacer las demostraciones más sencillas y compactas tomaremos la matriz $\textbf{X}$ como la matriz centrada $\overline{\textbf{X}}$, es decir: 
\begin{equation}
\overline{x}_{ij}=x_{ij}-\overline{x}_j
\end{equation}
Donde $\overline{x}_j$ es la media muestral de la $j$-ésima variable. Esto hace que $\textbf{S}=\frac{1}{n-1}\textbf{X}^T\textbf{X}$. Esto permite hablar de los valores y vectores propios de \textbf{S} y de $\textbf{X}^T \textbf{X}$ indistintamente, ya que los vectores propios son los mismos y los valores propios son proporcionales. 

\subsection{Reconstrucción de los datos}

\noindent Uno de los objetivos de las componentes principales es reducir la dimensionalidad de la matriz de datos de tamaño $n\times p$, \textbf{X}. Esta matriz de datos se puede interpretar como un conjunto de puntos del espacio $\mathbb{R}^p$. 

\noindent La intención final es buscar una proyección sobre una subvariedad de dimensión $m<p$ que reduzca la pérdida de información de la matriz y que brinde una mayor capacidad de interpretación de los datos, ya que en el caso de que $m=2$ o $m=3$ se podrán hacer representaciones gráficas de manera sencilla . En virtud de conseguir esto se deben definir los siguientes conceptos:

\begin{defi}
Dada una matriz $\textbf{X}\in  \mathbb{M}_{n\times p}(\mathbb{R})$ existe la descomposición en valores singulares \textit{(SVD en inglés)}:
\begin{equation}
\textbf{X}=\textbf{U}\mathbf{\Sigma}\textbf{V}^T
\end{equation}
Donde:
\begin{itemize}
\item \textbf{U} matriz ortogonal y de tamaño $n \times n$
\item $\mathbf{\Sigma}$ matriz de tamaño $n \times p $ diagonal, cuyos elementos no nulos son los valores singulares $\sigma_1\geq\ldots\geq \sigma_r\geq 0$ que son los valores propios de la matriz $\textbf{X}^T\textbf{X}$ y $r=rg(\textbf{X})$
\item \textbf{V} matriz ortogonal y de tamaño $p \times p$
\end{itemize}
\end{defi}

\begin{propo}
La matriz \textbf{V} de tamaño $(p\times p)$ es la matriz que contiene los vectores para hacer la combinación lineal que definen las componentes principales.
\begin{proof}
La matriz $\textbf{X}^T \textbf{X}$ es la matriz de covarianzas $(p \times p)$  por la descomposición en valores singulares tenemos que:
\begin{align*}
\textbf{X}^T \textbf{X} &= (\textbf{U}\mathbf{\Sigma} \textbf{V}^T)^T (\textbf{U}\mathbf{\Sigma} \textbf{V}^T)\\
&= \textbf{V}\mathbf{\Sigma} ^T \textbf{U}^T \textbf{U}\mathbf{\Sigma} \textbf{V}^T\\
&= \textbf{V}\mathbf{\Sigma} ^T \mathbf{\Sigma} \textbf{V}^T
\end{align*}
Donde la matriz $\mathbf{\Sigma} ^T \mathbf{\Sigma} $ es una matriz diagonal de tamaño $p \times p $ cuyos elementos son los cuadrados de los valores singulares de \textbf{X}, que son a su vez los valores propios de $\textbf{X}^T \textbf{X}$. 

\noindent Añadiendo la condición de ortogonalidad de $\textbf{V}\Rightarrow \textbf{V}^{-1}=V^T$ es fácil ver que la matriz \textbf{V} es la matriz cuyas columnas son los vectores propios de $\textbf{X}^T\textbf{X}$
\end{proof}
\end{propo}

\begin{coro}
El cálculo de las componentes principales de la matriz de datos \textbf{X} es equivalente a calcular la descomposición en valores singulares de la misma. 
\end {coro}

\begin{defi}
Sea $\textbf{A}\in \mathbb{M}_{n\times p}(\mathbb{R})$ definimos la \textit{norma de Frobenius} de la matriz \textbf{A} como :
\begin{equation}
||\textbf{A}||_F=(tr(\textbf{A}^T\cdot \textbf{A}))^{\frac{1}{2}}=\left(\sum_{i=1}^{n}\sum _{j=1}^{m}a_{ij}^2\right)^{\frac{1}{2}}
\end{equation}
\end{defi}

\begin{propo}
La norma de Frobenius es invariante a transformaciones ortogonales
\begin{proof}
Sea $\mathbf{U}$ una matriz ortogonal, que cumple $\mathbf{U}^T\cdot \mathbf{U}=\mathbf{U}\cdot \mathbf{U}^T=\textbf{I}$, sea una matriz cualquiera $\mathbf{A}$, entonces:
\begin{align*}
||\mathbf{U} \cdot \mathbf{A}||_F^2&=tr((\mathbf{U} \mathbf{A})^T\cdot(\mathbf{U} \mathbf{A}))\\
&=tr((\mathbf{A}^T \mathbf{U}^T)\cdot \mathbf{U} \mathbf{A}))\\
&=tr(\mathbf{A}^T \mathbf{A})\\
&=||\mathbf{A}||_F^2\qedhere
\end{align*}
\end{proof}
\end{propo}

\noindent Se ha elegido la norma de Frobenius por la siguiente propiedad
\begin{propo}
Dada una matriz de datos \textbf{X} de tamaño $n\times p$ entonces
\begin{equation}
||\textbf{X}||_F^2=(n-1)\sum_{i=1}^p s_{ii}^2
\end{equation}
Donde las $s_{ii}^2$ son las varianzas muestrales.
\begin{proof}
Debido a la centralidad impuesta a la matriz \textbf{X}, sabemos que la matriz de covarianzas es $\textbf{S}=\frac{1}{n-1}\textbf{X}^T \textbf{X}$ por tanto,se tiene que utilizar la definición de la norma:
\begin{align*}
||\textbf{X}||_F^2 &= tr(\textbf{X}^T\textbf{X})\\
&= (n-1) tr(\textbf{S})\\
&= (n-1) \sum_{i=1}^p s_{ii}^2 \qedhere
\end{align*}
\end{proof}
\end{propo}
\noindent Por tanto, la norma de Frobenius da una imagen del tamaño de la matriz de datos en función de la varianza total de los datos, lo que concuerda con la idea de buscar una matriz que aproxime la matriz de datos con la mínima pérdida de variación de los datos.  

\begin{defi}
Se llama matriz reducida de orden $m\leq p$ de $\textbf{X}$ y se denota como $\textbf{X}_m$, a la matriz $n\times p$ resultado de:
\begin{equation}
\textbf{X}_m=\textbf{U}_m\mathbf{\Sigma}_m\textbf{V}^T_m
\end{equation}
Donde:
\begin{itemize}
\item $\textbf{U}_m$ matriz ortogonal de tamaño $n \times m$, resultado de tomar de \textbf{U} únicamente la matriz las $m$ primeras columnas. 
\item $\mathbf{\Sigma}_m$  matriz cuadrada de tamaño $m$ diagonal con los $m$ primeros valores singulares. 
\item $\textbf{V}_m$ matriz ortogonal de tamaño $p \times m$ obtenida al tomar las $m$ primeras columnas de \textbf{V}.
\end{itemize}
\end{defi}

\begin{teorema}[De Eckart-Young]
Sea \textbf{A} una matriz de coeficientes reales de tamaño $n\times p$ y rango $r$  entonces se cumple que:
\begin{equation}
||\textbf{A}-\textbf{B}||_F\leq ||\textbf{A}-\textbf{A}_m||_F \quad \forall \textbf{B}/ rg(\textbf{B})=m \leq r
\end{equation} 
\end{teorema}

\noindent Por tanto, la matriz reducida brinda la mejor aproximación de la matriz de datos teniendo un criterio de aproximación basado en la variación de los datos. En consecuencia se puede 

\noindent Como conclusión, se puede definir un criterio para elegir el orden de la matriz reducida $m$. Se puede entonces definir la variación acumulada de la siguiente manera:
\begin{equation}
t_m=\dfrac{\sum_{i=1}^{m}\lambda_i}{\sum_{i=1}^{p}\lambda_i}
\end{equation}
\noindent Donde los $\lambda_i$ son los valores propios de la matriz $\textbf{S}=\frac{1}{n-1}\textbf{X}^T\textbf{X}$.

\noindent Cuanto más cercana sea $t_m$ a $1$ manteniendo $m$ lo más pequeña posible mejor por que implicaría que con pocas componentes principales podemos ``explicar'' la mayor parte de la variación de los datos. Según Jollife \cite{Jollife 1986} entre un $0.8$ y $0.9$ es lo más habitual. 