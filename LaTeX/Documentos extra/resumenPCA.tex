\section{Análisis de Componentes Principales}
%\input{Documentos Extra/introduccionPCA.tex}

\subsection{Definición y cálculo de las Componentes}

\noindent Sea un vector aleatorio $\textbf{X}^T=[X_1,\ldots X_p]$ con vector de medias $\mu$ y matriz de covarianzas $\Sigma$.
\begin{defi}
Las componentes principales son combinaciones lineales de las variables $X_1 \ldots X_p$
\begin{equation}
\textbf{Y}_j=a_{1j}X_1+\ldots a_{pj}X_p=\textbf{a}_j^T\textbf{X}\quad 
\end{equation}

\noindent Donde $\textbf{a}_j$ es un vector de constantes y la variable $\textbf{Y}_j$ cumple lo siguiente:
\begin{itemize}
\item Si $j=1$ $Var(\textbf{Y}_1)$ es máxima restringido a $\textbf{a}_1^T \textbf{a}_1=1$
\item Si $j>1$ debe cumplir:
\begin{itemize}
\item $Cov(\textbf{Y}_j,\textbf{Y}_i)=0\quad \forall i<j $
\item $\textbf{a}_j^T \textbf{a}_j=1$
\item $Var(\textbf{Y}_j)$ es máxima. 
\end{itemize}

\end{itemize}

\noindent De esta manera, estamos buscando una nueva base que consiga reunir las direcciones de máxima variación. 
\end{defi}

\noindent El cálculo de la primera componente principal se lleva a cabo con un proceso de optimización de la función $Var(\textbf{Y}_1)$ sujeto a la restricción de que $\textbf{a}_1^T\textbf{a}_1=1$. 

\noindent Aplicando el método de los multiplicadores de Lagrange,  dada una función $f(\textbf{x})=f(x_1,\ldots, x_p)$ diferenciable con una restricción $g(\textbf{x})=g(x_1, \ldots, x_p)=c$ entonces existe una constante $\lambda$ de manera la ecuación:
\begin{equation}
\dfrac{\partial f}{\partial x_i}-\lambda\dfrac{\partial g}{\partial x_i}=0 \quad i=1,\ldots p 
\end{equation}

\noindent Tiene como solución los puntos estacionarios de $f(\textbf{x})$. Entonces, se puede definir la función $L(\textbf{x})= f(\textbf{x})-\lambda[g(\textbf{x})-c]$  que permite simplificar la expresión anterior a:
\begin{equation}
\dfrac{\partial L}{\partial \textbf{x}}=0
\end{equation}

\noindent Para el caso de las componentes principales, la función objetivo es la varianza de la combinación lineal, es decir, $f(\textbf{x})=\textbf{x}^T \Sigma \textbf{x}$ y la restricción aplicada es $g(\textbf{x})=\textbf{x}^T\textbf{x}=1$. 

\noindent Tomando $\textbf{x}=\textbf{a}_1$ se puede establecer $L(\textbf{a}_1)=\textbf{a}_1^T \Sigma \textbf{a}_1 - \lambda[\textbf{a}_1^T \textbf{a}_1-1]$. Que al derivarla se obtiene:
%\noindent Para el cálculo de la $L(\textbf{a}_1)=\textbf{a}_1^T \Sigma \textbf{a}_1 - \lambda[\textbf{a}_1^T \textbf{a}_1-1]$. Al derivarla obtenemos que :
\begin{align*}
\dfrac{\partial L}{\partial \textbf{a}_1} &= 2\Sigma \textbf{a}_1 - 2\lambda\textbf{a}_1\\
& = 2(\Sigma-\lambda)\textbf{a}_1 
\end{align*}

\noindent Igualando a 0 tenemos la siguiente ecuación: 
\begin{equation}
(\Sigma-\lambda I)\textbf{a}_1=0
\end{equation}

\noindent Para que $\textbf{a}_1$ sea un vector no trivial, tenemos que elegir $\lambda$ de tal manera que $|\Sigma-\lambda I| = 0$, es decir, $\lambda$ es un vector propio de la matriz de covarianzas, $\Sigma$. Al ser ésta una matriz semidefinido positiva y simétrica, los valores propios son reales y positivos. Por tanto, $\textbf{a}_1$ es un vector propio de la matriz de covarianza.

\noindent La función a maximizar es $Var(\textbf{Y}_1)=Var(\textbf{a}_1^T\textbf{X})=\textbf{a}_1^T\Sigma \textbf{a}_1=\textbf{a}_1^T\lambda \textbf{a}_1$, y para maximizarla basta tomar $\lambda=\max{\lbrace\lambda_1\ldots \lambda_p\rbrace}$. reordenando si es necesario, se tiene que $\lambda=\lambda_1$ 

%\noindent Para que la ecuación tenga una solución que no sea la trivial, tenemos que elegir $\lambda$ de manera que $|\Sigma-\lambda I| = 0$. Luego $\lambda$ es uno de los valores propios de la matriz de covarianzas. Generalmente una matriz $(p\times p)$ tiene $p$ valores propios $\lbrace\lambda_1, \ldots ,\lambda_p \rbrace$ y como $Var(\textbf{Y}_1)=Var(\textbf{a}_1^T\textbf{X})= \textbf{a}_1^T \Sigma \textbf{a}_1 =\textbf{a}_1^T \lambda \textbf{a}_1=\lambda$ que es la variable a maximizar, elegimos $\lambda=\max \lbrace\lambda_1, \ldots ,\lambda_p \rbrace$, por tanto, el vector $\textbf{a}_1$ es el vector propio con valor propio $\lambda=\lambda_1$ reordenando si es necesario.



\noindent Una vez calculada la primera componente principal $\textbf{Y}_1$, la segunda componente se calcula de manera análoga, maximizando $Var(\textbf{Y}_2)=Var(\textbf{a}_2^T\textbf{X})$ condicionada por $\textbf{a}_2^T\textbf{a}_2=1$. A esta restricción tenemos que añadir la restricción $Cov(\textbf{Y}_1,\textbf{Y}_2)=0 $

\begin{propo}
La condición $Cov(\textbf{Y}_1,\textbf{Y}_2)=0 $ equivale a la condición $\textbf{a}_2^T\textbf{a}_1 = 0$.
\begin{proof}
Utilizando que $\textbf{Y}_j=\textbf{a}_j^T \textbf{X}\quad \forall j$ , tenemos entonces que:
\begin{align*}
Cov(\textbf{Y}_2,\textbf{Y}_1)&= Cov (\textbf{a}_2^T\textbf{X},\textbf{a}_1^T\textbf{X})\\ 
&= \mathbb{E}(\textbf{a}_2^T(\textbf{X}-\mu)(\textbf{X}-\mu)^T \textbf{a}_1)\\
&= \textbf{a}_2^T \mathbb{E}((\textbf{X}-\mu)(\textbf{X}-\mu)^T) \textbf{a}_1\\
&= \textbf{a}_2^T \Sigma \textbf{a}_1 \\
&= \textbf{a}_2^T \lambda_1 \textbf{a}_1
\end{align*}
\noindent De manera que, si $a_2^T \lambda_1 a_1 = 0 \Rightarrow a_2^T a_1=0 $, luego son vectores ortogonales entre sí.
\end{proof}
\end{propo}


\noindent \emph{Observación: } Esta proposición se puede extender de manera simple al caso de tener que calcular la $i$-ésima componente principal habiendo calculado las anteriores de las cuales se sepan los valores propios asociados. 

\begin{coro}
Las componentes principales son todas ortogonales entre sí. 
\end{coro}
%%%ESto está maaaal
\noindent Tomando la matriz formada por los $p$ vectores propios como columnas tenemos la matriz ortogonal $\textbf{A}=[\textbf{a}_1,\ldots, \textbf{a}_p]$, de manera que el vector aleatorio 
$$\textbf{Y}=[Y_1,\ldots , Y_p]^T=\textbf{A}\textbf{X}$$

\noindent Se deduce utilizando la ortogonalidad de $\textbf{A}\Rightarrow \textbf{A}^T\textbf{A}=\textbf{A}\textbf{A}^T=\textbf{I}$ que:
\begin{align*}
Var(\textbf{Y})&=Var(\textbf{AX})\\
&=\mathbb{E}((\textbf{AX})^T(\textbf{AX}))\\
&=\mathbb{E}(\textbf{X}^T\textbf{A}^T\textbf{A}\textbf{X})\\
&=\mathbb{E}(\textbf{X}^T\textbf{X})\\
&=Var(\textbf{X})
\end{align*} 

\noindent Por tanto, se puede afirmar que las componentes principales contienen toda la variación del vector \textbf{X} aleatorio inicial.


\begin{propo}
Para cualquier $1\leq q \leq p $ entero sea la matriz ortonormal $\textbf{B}$, entonces la transformación:
\begin{equation}

\end{equation}
\end{propo}

\noindent Sea ahora la matriz \textbf{X} de datos de tamaño $n \times p$ resultante de tomar $n$ observaciones de las $p$ variables del vector aleatorio. Se supone además que esta es centrada y dividida todos sus elementos por $\sqrt{n-1}$. 

\noindent Dado este marco la matriz \textbf{X} tiene una descomposición en valores singulares $\textbf{X}=\textbf{U}\Sigma \textbf{V}^T$. Donde tanto \textbf{U} como $\textbf{V}$ son matrices ortogonales. Esto implica que $\textbf{U}^T \textbf{U}=\textbf{U}\textbf{U}^T=\textbf{I}$. 

\noindent  En particular, \textbf{U} es la matriz de vectores singulares por la izquierda  de tamaño $n \times r$, \textbf{V} la matriz de vectores singulares por la derecha de tamaño $p \times p$ y la matriz $\Sigma$ es la matriz diagonal de valores singulares generalmente ordenados de forma decreciente, es decir $\Sigma^2$ es la matriz diagonal que contiene todos los valores propios de las matrices $\textbf{X}^T\textbf{X}$ y $\textbf{X}\textbf{X}^T$.

\noindent Por las transformaciones previas realizadas la matriz de estimaciones de las covarianzas es $\hat{S}=\textbf{X}^T\textbf{X}$ en consecuencia, el proceso que se describía en primera instancia para un vector aleatorio en el que conocíamos su matriz de covarianzas se puede aplicar de manera análoga a la matriz de datos.

\noindent Esto implica que la obtención de las componentes principales en el caso de ser calculadas desde la matriz de datos también se obtienen calculando los vectores y valores propios de la matriz de covarianzas muestral en este caso. 

\begin{propo}
La matriz \textbf{V} de tamaño $(p\times p)$ es la matriz que contiene los vectores para hacer la combinación lineal que definen las componentes principales.
\begin{proof}
La matriz $\textbf{X}^T \textbf{X}$ es la matriz de covarianzas $(p \times p)$  por la descomposición en valores singulares tenemos que:
\begin{align*}
\textbf{X}^T \textbf{X} &= (\textbf{U}\Sigma \textbf{V}^T)^T (\textbf{U}\Sigma \textbf{V}^T)\\
&= \textbf{V}\Sigma ^T \textbf{U}^T \textbf{U}\Sigma \textbf{V}^T\\
&= \textbf{V}\Sigma ^T \Sigma \textbf{V}^T \\
&=  \textbf{V}\Sigma ^2  \textbf{V}^T 
\end{align*}

\noindent Que es la diagonalización de la matriz $\textbf{X}^T \textbf{X}$, ya que $\textbf{V}^T=\textbf{V}^{-1}$, donde los elementos de la diagonal de $\Sigma$ son las raíces cuadradas de los valores propios de $\textbf{X}^T \textbf{X}$. A esto también hay que añadir que \textbf{V} es la matriz que tiene como columnas los vectores propios, es decir, es la matriz con los vectores que forman las componentes principales. 
\end{proof}
\end{propo}

\noindent Teniendo en cuenta lo anterior podemos obtener la transformación de la matriz de datos a las componentes principales,\textbf{Y}, multiplicando por la matriz $\textbf{V}$, lo que nos resulta en :
\begin{equation}
\textbf{XV}=\textbf{U}\Sigma \textbf{V}^T \textbf{V}=\textbf{U}\Sigma=\textbf{Y}
\end{equation}
\subsection{Reducción de la dimensionalidad}

\noindent La utilidad de esta técnica es reducir el tamaño de la matriz de datos para poder representar las observaciones como puntos de una subvariedad de dimensión $m < p$. Con este fin en mente, se deben definir los siguientes conceptos.
\begin{defi}
Sea $\textbf{A}\in \mathbb{M}_{n\times m}(\mathbb{R})$ definimos la \textit{norma de Frobenius} de la matriz \textbf{A} como :
\begin{equation}
||\textbf{A}||_F=(tr(\textbf{A}^T\cdot \textbf{A}))^{\frac{1}{2}}=\left(\sum_{i=1}^{n}\sum _{j=1}^{m}a_{ij}^2\right)^{\frac{1}{2}}
\end{equation}
\end{defi}

\begin{propo}
La norma de Frobenius es invariante a transformaciones ortogonales
\begin{proof}
Sea $\mathbf{U}$ una matriz ortogonal, que cumple $\mathbf{U}^T\cdot \mathbf{U}=\mathbf{U}\cdot \mathbf{U}^T=I$, sea una matriz cualquiera $\mathbf{A}$, entonces:
\begin{align*}\tag{2.7}
||\mathbf{U} \cdot \mathbf{A}||_F^2&=tr((\mathbf{U} \mathbf{A})^T\cdot(\mathbf{U} \mathbf{A}))\\
&=tr((\mathbf{A}^T \mathbf{U}^T)\cdot \mathbf{U} \mathbf{A}))\\
&=tr(\mathbf{A}^T \mathbf{A})\\
&=||\mathbf{A}||_F^2
\end{align*}
\end{proof}
\end{propo}

\noindent Aunque hay más normas que son invariantes ante transformaciones ortogonales, nos centraremos en la norma de Frobenius.

\begin{defi}
Dada una matriz $\textbf{A}=\textbf{U}\Sigma \textbf{V}^T$ de tamaño $n\times m $ y de rango $r$, entonces para $l < r$ se define la matriz truncada  $\textbf{A}_l=\textbf{U}_l \Sigma_l \textbf{V}^T_l$ donde $\textbf{U}_l,\textbf{V}_l,\Sigma_l$, son las matrices resultantes de seleccionar las $l$ primeras columnas . 
\end{defi}


\begin{teorema}[De Eckart-Young]
Sea una matriz $\textbf{A}$ de rango $r$, y sea una matriz $\textbf{B}$ rango $l<r$, entonces:
\begin{equation}
||\textbf{A}-\textbf{B}||_F \leq ||\textbf{A}-\textbf{A}_l||_F
\end{equation}
\end{teorema}





