\chapter{Análisis de Componentes Principales}
\section{Introducción}

\noindent Dado un conjunto de $p$ variables correladas que debemos examinar, hay problemas cuando $p$ es especialmente alto. En aplicaciones como el campo del aprendizaje automático, si el número de variables estudiadas es mayor que el número de muestras recogidas, el modelo tenderá al overfitting y, por tanto, fallará al generalizar sus predicciones.

\noindent La base conceptual del análisis de componentes principales, PCA de ahora en adelante, es que en el caso de que tengamos un conjunto de variables correladas, entonces, dentro de las $p$ habrá variables que dan la misma información y por tanto, podemos reducir la dimensionalidad de nuestro conjunto de datos. 

\noindent El objetivo del análisis de componentes principales es comprobar y estudiar si combinaciones lineales de nuestras $p$ variables iniciales pueden traer consigo la mayor variación de los datos. 

\noindent Por último, el resultado obtenido tras aplicar el PCA, es un conjunto de variables no correladas. En principio, si el conjunto de las $p$ variables son cerca de no estar correladas, no tiene sentido aplicar PCA, ya que este encontrará variables cercanas y ordenadas por varianza. 

\newpage

\section{Método PCA}

\noindent Sea $\textbf{X} = [X_1,\ldots X_p]$ un vector aleatorio con media $\mu$ y matriz de covarianzas $\textbf{\Sigma}$. Entonces podemos plantear el problema de la siguiente manera. 
