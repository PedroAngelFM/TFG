\chapter{Conceptos Estadística Multivariante}

A lo largo de esta primera parte estableceremos los conceptos básicos que se usarán a lo largo de la memoria todo ello basado en \cite{Chatfield 1989}

%\section{Variables Aleatorias}


\begin{defi}
Dada una población de $n$ individuos de los que estudiamos $p$ variables, estableceremos que:
$$
x_{rj}=\textit{Valor que toma la j-ésima variable en el r-ésimo individuo}
$$
\end{defi}

\begin{defi}
La matriz $X=x_{ij}$ de dimensión $(n \times p)$ es conocida como la \textit{matriz de datos}
\end{defi}

\begin{defi}
Dadas $X_1\ldots X_p$ variables aleatorias, llamaremos vector aleatorio al vector $\textbf{X}^T=[X_1 \ldots X_p]$.
\end{defi}

\noindent \textit{Observación: } La matriz de datos y el vector aleatorio no son lo mismo, de ahí la diferencia en la tipografía. 

\begin{defi}
Dado un vector aleatorio $\textbf{X}$, llamaremos vector de medias $\mathbf{\mu}^T=[\mu_1\ldots \mu_p]$ donde $\mu_i=\mathbb{E}(X_i)$
\end{defi}

\begin{defi}
Dado un vector aleatorio $\textbf{X}$, llamamos matriz de covarianzas a la matriz $\Sigma $ donde los coeficientes son los siguientes $s_{ij}=Cov(X_i,X_j)$ por tanto, la diagonal de esta matriz son las varianzas de las variables. Análogamente podemos definir la \textit{matriz de correlaciones}, $P$. 
\end{defi}

\begin{propo}
Dada la matriz covarianzas $\Sigma$
\end{propo}