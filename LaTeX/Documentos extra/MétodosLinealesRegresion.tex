\section{Métodos Lineales para regresión}
\noindent El objetivo de la regresión es conseguir una relación funcional que permita predecir los valores de una variable respuesta cuantitativa en función de los valores de unas variables de entrada. En el caso, de la regresión lineal, ya sea simple o múltiple se asume que esa relación funcional es lineal. 

\noindent Para ello, se tendrá en cuenta el modelo aditivo que se detallaba anteriormente, tendremos en cuenta que la variable de respuesta $Y$ viene dada por una relación de la forma:
\begin{equation}
Y=f(\textbf{x})+\varepsilon
\end{equation}

\noindent Como se mencionaba anteriormente, en el caso de los modelos lineales se supone que la función de regresión, $f$ es una función lineal de las variables de entrada del vector aleatorio $\textbf{x}$,  $X_1\ldots X_p$. De esta manera se tiene. 
\begin{equation}
f(\textbf{x})=\beta_0+\sum_{j=1}^p X_j\beta_j
\end{equation}

\noindent Por tanto, la predicción de la variable de salida $\hat{y
}_i$ para una observación $\textbf{x}_i$ del vector aleatorio  se puede expresar de la siguiente manera:
\begin{equation}
\hat{y}_i=f(\textbf{x}_i)=\beta_0+\sum_{j=1}^p x_{ij}\beta_j
\end{equation}

\noindent De esta manera, se tiene un vector de $p$ parámetros $\beta_1 \ldots \beta_p$ además de otro $\beta_0$. Esto se puede simplificar añadiendo al vector aleatorio una variable aleatoria que sea constantemente 1. De esta manera, sea $\beta^T=[\beta_0,\beta_1\ldots \beta_p]$ y una observación $\textbf{x}_i^T=[1,x_{i1},\ldots x_{ip}]$ la expresión anterior se simplifica de la siguiente manera: 
\begin{equation}
\hat{y}_i= f(\textbf{x}_i)=\textbf{x}_i^T\beta
\end{equation}

\noindent Y en general, para una matriz de datos $\textbf{X}$ de tamaño $n\times p $ al que se le añade una primera columna de 1's por tanto, acaba siendo una matriz de tamaño $n\times (p+1)$ y sea $\hat{\textbf{y}}=[\hat{y}_1,\ldots \hat{y}_p]$ el vector de predicciones. Entonces, se obtiene la siguiente expresión  simplificada:
\begin{equation}
\hat{\textbf{y}}=\textbf{X}\beta
\end{equation} 

\noindent En las siguientes secciones se detallarán el calculo de los parámetros $\beta$. 

\newpage
\subsection{Ajuste de los parámetros por mínimos cuadrados}
\noindent Sea una matriz de datos $\textbf{X}$ de tamaño $n\times p+1$ resultado de hacer $n$ observaciones de $p$ variables aleatorias y añadir en primera instancia de cada observación un 1. Sea también un vector de respuestas $\textbf{y}^T=[y_1,\ldots y_p]$ de tamaño $n$. 

\noindent Tomando la suma de los errores cuadrados y minimizando se puede obtener una estimación de máxima verosimilitud del vector de parámetros $\beta$. Utilizando las expresiones anteriores:
\begin{align}
RSS(\beta)=\sum_{i=1}^n(y_i-\hat{y}_i) &= \sum_{i=1}^n(y_i-\textbf{x}_i^T\beta )=\textbf{y}-\textbf{X}\beta\\
\intertext{Derivando respecto al vector de parámetros $\beta$:}
\dfrac{\partial RSS (\beta)}{\partial \beta}&=-2\textbf{X}^T(\textbf{y}-\textbf{X}\beta)\\
\intertext{Y la segunda derivada respecto de $\beta^T$: }
\dfrac{\partial^2 RSS (\beta)}{\partial \beta \partial \beta^T} &=  -2 \textbf{X}^T\textbf{X}
\end{align}

\noindent Asumiendo que $\textbf{X}$ es una matriz de rango máximo, de lo contrario, habría dos columnas o más que son combinación lineal la una de la otra, la matriz $\textbf{X}^T\textbf{X}$ es definida positiva, por tanto la solución de la siguiente ecuación:
\begin{equation}
\textbf{X}^T(\textbf{y}-\textbf{X}\beta)=0
\end{equation}
Es un mínimo local de $RSS(\beta)$ , si además, $\textbf{X}^T\textbf{X}$ es una matriz invertible, entonces tiene solución única y es :
\begin{equation}
\hat{\beta}=(\textbf{X}^T\textbf{X})^{-1}\textbf{X}^T\textbf{y}
\end{equation}

\begin{propo}
Esta estimación de los parámetros $\beta$ es equivalente a la estimación de estos mediante el método de máxima verosimilitud
\begin{proof}

\end{proof}
\end{propo}

\noindent Por tanto, los valores predichos $\hat{\textbf{y}}$ se calculan de la siguiente manera:
\begin{equation}
\hat{\textbf{y}}=\textbf{X}\hat{\beta}=\textbf{X}^T(\textbf{X}^T\textbf{X})^{-1}\textbf{X}^T\textbf{y}
\end{equation}

\noindent Si se toma el espacio $\mathbb{R}^n$ generado por las columnas de la matriz de datos $\textbf{x}_0,\textbf{x}_1,\ldots \textbf{x}_p$, entonces $\hat{\textbf{y}}$ es tal que $\textbf{y}-\hat{\textbf{y}}$ es ortogonal a ese subespacio. 

\subsection*{Inferencias Estadísticas sobre $\hat{\beta}$}

\noindent Sean las observaciones $y_i$ no correlacionadas y con varianza constante $\sigma^2$, sean los $\textbf{x}_i$ no aleatorios ya conocidos, entonces:
\begin{equation}
Var(\hat{\beta})=(\textbf{X}^T\textbf{X})^{-1} \sigma^2
\end{equation}

Supóngase además que se sabe que el modelo lineal es correcto, de esta manera se tiene que la media condicional es lineal y tendremos que los datos son de la forma:
\begin{equation}
Y=\beta_0+\sum_{j=1}^p X_j\beta_j + \varepsilon
\end{equation}
\noindent Teniendo en cuenta las propiedades de $\varepsilon$ que se asumían al tomar un modelo aditivo.
 
\noindent Un estimador insesgado que se puede dar de la varianza $\sigma^2$ es el siguiente:
\begin{align}
\hat{\sigma}^2 &= \dfrac{1}{n-p-1}\sum_{i=1}^n(y_i-\hat{y}_i)^2\\
\intertext{Por tanto la distribución de este estimador es:}
(n-p-1)\hat{\sigma}^2&\thicksim \sigma^2 \chi_{n-p-1}^2
\end{align}

\noindent Además el vector de parámetros estimados tendrá una distribución $\hat{\beta}\thicksim \mathcal{N}(\beta, (\textbf{X}^T\textbf{X})^{-1}\sigma^2)$.\\
Teniendo en cuenta lo anterior, la construcción del siguiente estadígrafo de contraste para el parámetro estimado $\hat{\beta}_j$ es relativamente sencilla:
\begin{equation}
z_j=\dfrac{\hat{\beta}_j}{\hat{\sigma}\sqrt{v_j}}\thicksim t_{n-p-1}
\end{equation}
\noindent Donde $v_j$ es el elemento $j$-ésimo de la matriz $(\textbf{X}^T\textbf{X})^{-1}$. Por tanto, podemos generar un intervalo de confianza a un nivel de confianza de $1-2\alpha$
\begin{align}
(\hat{\beta}_j-z^{(1-\alpha)}v_j\hat{\sigma},\hat{\beta}_j+z^{(1-\alpha)}v_j\hat{\sigma})
\intertext{Esto permite hacer un conjunto de confianza para el vector de parámetros $\beta$}
C_{\beta}=\lbrace \beta| (\hat{\beta}-\beta)^T \textbf{X}^T\textbf{X}(\hat{\beta}-\beta)\leq \hat{\sigma}^2 \chi_{p+1}^{2\quad (1-\alpha)}\rbrace
\end{align}

\noindent Otro contraste que se puede realizar es comprobar si un subconjunto de variables es más significativo estadísticamente que otro, de manera que se pueda eliminar variables que no aporten información en pos de la sencillez del modelo y su posterior interpretación. 

\noindent Sea $p_1+1$ el número del conjunto más grande de parámetros y $RSS_1$ su error cuadrático respectivo, sean respectivamente $RSS_0$ y $p_0+1$ para el segundo conjunto o subconjunto menor entonces el estadigráfo:
\begin{equation}
F=\dfrac{\dfrac{(RSS_0-RSS_1)}{p_1-p_0}}{\dfrac{RSS_1}{N-p_1-1}} \thicksim F_{(p_1-p_0),(N-p_1-1)}
\end{equation}

\noindent Este estadígrafo mide si la diferencia entre los errores cuadráticos es lo suficientemente grande para que hacer el cambio de $p_1$ a $p_0$ parámetros sea útil. 

\subsection{Regresión Múltiple mediante Ortogonalización sucesiva }














