\section{Métodos Lineales para regresión}

\noindent El objetivo de la regresión es conseguir una relación funcional que permita predecir los valores de una variable respuesta cuantitativa en función de los valores de unas variables de entrada. En el caso de la regresión lineal, ya sea simple o múltiple, se asume que esa relación funcional es lineal. 

\noindent Para ello, se tendrá en cuenta que las observaciones de la variable respuesta siguen la relación que se detallaba anteriormente:
\begin{equation}
Y=f(\textbf{x})+\varepsilon
\end{equation}

\noindent Donde $\varepsilon$ es una variable aleatoria con $\mathbb{E}(\varepsilon)=0$ y $Var(\varepsilon)=\sigma^2$.

\noindent En este tipo de métodos se supone que $f$ es una función lineal de las variables de entrada del vector aleatorio $\textbf{x}^T=[X_1\ldots X_p]$. De esta manera, se tiene que:  
\begin{equation}
f(\textbf{x})=\beta_0+\sum_{j=1}^p X_j\beta_j
\end{equation}

\noindent Por tanto, la predicción de la variable de salida $\hat{y
}_i$ para una observación $\textbf{x}_i$ del vector aleatorio  se puede expresar de la siguiente manera:
\begin{equation}
\hat{y}_i=f(\textbf{x}_i)=\beta_0+\sum_{j=1}^p x_{ij}\beta_j
\end{equation}

\noindent De esta manera, se tiene un vector de $p$ parámetros $\beta_1 \ldots \beta_p$ además de otro $\beta_0$. Esto se puede simplificar añadiendo al vector aleatorio una variable aleatoria que sea constantemente 1. De esta manera, sea $\beta^T=[\beta_0,\beta_1\ldots \beta_p]$ y una observación $\textbf{x}_i^T=[1,x_{i1},\ldots x_{ip}]$ la expresión anterior se simplifica de la siguiente manera: 
\begin{equation}
\hat{y}_i= f(\textbf{x}_i)=\textbf{x}_i^T\beta
\end{equation}

\noindent Y en general, para una matriz de datos $\textbf{X}$ de tamaño $n\times p $ al que se le añade una primera columna de 1's por tanto, acaba siendo una matriz de tamaño $n\times (p+1)$ y sea $\hat{\textbf{y}}=[\hat{y}_1,\ldots \hat{y}_p]$ el vector de predicciones. Entonces, se obtiene la siguiente expresión  simplificada:
\begin{equation}
\hat{\textbf{y}}=\textbf{X}\beta
\end{equation} 

\noindent Los parámetros $\beta$ son parámetros poblacionales y desconocidos, por ende, se deben estimar conociendo únicamente una muestra de la población general. En las siguientes secciones se detallan la obtención de dichas estimaciones mediante el método de mínimos cuadrados y las propiedades inferenciales que estos estimadores $\hat{\beta}$ tienen. 

\newpage
\subsection{Ajuste de los parámetros por mínimos cuadrados}
\noindent Sea una matriz de datos $\textbf{X}$ de tamaño $n\times p+1$ resultado de hacer $n$ observaciones de $p$ variables aleatorias y añadir a la primera componente de cada observación un 1. Sea también un vector de respuestas $\textbf{y}^T=[y_1,\ldots y_n]$ de tamaño $n$, resultado de observar la variable respuesta $Y$. 

\noindent Tomando la suma de los errores cuadrados y minimizando se puede obtener una estimación de máxima verosimilitud del vector de parámetros $\beta$. Utilizando las expresiones anteriores:
\begin{align}
RSS(\beta)=\sum_{i=1}^n(y_i-\hat{y}_i) &= \sum_{i=1}^n(y_i-\textbf{x}_i^T\beta )=\textbf{y}-\textbf{X}\beta\\
\intertext{Derivando respecto al vector de parámetros $\beta$:}
\dfrac{\partial RSS (\beta)}{\partial \beta}&=-2\textbf{X}^T(\textbf{y}-\textbf{X}\beta)\\
\intertext{Y la segunda derivada respecto de $\beta^T$: }
\dfrac{\partial^2 RSS (\beta)}{\partial \beta \partial \beta^T} &=  -2 \textbf{X}^T\textbf{X}
\end{align}

\noindent Asumiendo que $\textbf{X}$ es una matriz de rango máximo, de lo contrario, habría dos columnas o más que son combinación lineal la una de la otra, la matriz $\textbf{X}^T\textbf{X}$ es definida positiva, por tanto la solución de la siguiente ecuación:
\begin{equation}
\textbf{X}^T(\textbf{y}-\textbf{X}\beta)=0
\end{equation}
Es un mínimo local de $RSS(\beta)$ , si además, $\textbf{X}^T\textbf{X}$ es una matriz invertible, entonces tiene solución única y es :
\begin{equation}
\hat{\beta}=(\textbf{X}^T\textbf{X})^{-1}\textbf{X}^T\textbf{y}
\end{equation}
\begin{propo}
Esta estimación de los parámetros $\beta$ por mínimos cuadrados es equivalente a la estimación de estos mediante el método de máxima verosimilitud.
\begin{proof}
Sea una muestra de tamaño $n$, $y_i, i=1,\ldots n$ de una variable aleatoria $Y$, de manera que su función de probabilidad, $\mathbb{P}_\theta(y)$ depende de los parámetros $\theta$. Entonces el método de máxima verosimilitud busca maximizar la siguiente función.
\begin{equation}
L(\theta)=\sum_{i=1}^n log( \mathbb{P}_{\theta}(y_i))
\end{equation}

\noindent Suponiendo que la variable respuesta cumple como antes que  $Y=f_\theta (\textbf{x})+\varepsilon$, donde $\varepsilon \thicksim \mathcal{N}(0,\sigma^2)$, provoca que si se suponen conocidos a priori el parámetro $\theta$ y el vector aleatorio $\textbf{x}$ entonces :
\begin{equation}
\mathbb{P}(Y|\textbf{x},\theta)=\mathcal{N}(f_\theta(\textbf{x}), \sigma^2)
\end{equation}

\noindent Teniendo esto en cuenta, la función $L(\theta)$ tiene la siguiente expresión:
\begin{equation}
L(\theta)=-\dfrac{n}{2}log(2\pi)-n log(\sigma)-\dfrac{1}{2\sigma^2}\sum_{i=1}^n (y_i-f_\theta (x_i))^2
\end{equation}
Por tanto, teniendo en cuenta que el último término es $RSS(\theta)$, entonces maximizar $L(\theta)$ es equivalente a minimizar $RSS(\theta)$
\end{proof}
\end{propo}
\begin{coro}
Las estimaciones para cualquier $f_{\theta}$ obtenidas por el método de los mínimos cuadrados son equivalentes a las del método de máxima verosimilitud.  
\end{coro}
\noindent Por tanto, los valores predichos $\hat{\textbf{y}}$ se calculan de la siguiente manera:
\begin{equation}
\hat{\textbf{y}}=\textbf{X}\hat{\beta}=\textbf{X}^T(\textbf{X}^T\textbf{X})^{-1}\textbf{X}^T\textbf{y}
\end{equation}

\noindent Si se toma el espacio $\mathbb{R}^n$ generado por las columnas de la matriz de datos $\textbf{x}_0,\textbf{x}_1,\ldots \textbf{x}_p$, entonces $\hat{\textbf{y}}$ es tal que $\textbf{y}-\hat{\textbf{y}}$ es ortogonal a ese subespacio. 

\subsection*{Inferencias Estadísticas sobre $\hat{\beta}$}

\noindent Sean las observaciones $y_i$ no correlacionadas y con varianza constante $\sigma^2$, sean los $\textbf{x}_i$ no aleatorios ya conocidos, entonces:
\begin{equation}
Var(\hat{\beta})=(\textbf{X}^T\textbf{X})^{-1} \sigma^2
\end{equation}

\noindent Supóngase además que se sabe que el modelo lineal es correcto, de esta manera se tiene que la media condicional es lineal y tendremos que los datos son de la forma:
\begin{equation}
Y=\beta_0+\sum_{j=1}^p X_j\beta_j + \varepsilon
\end{equation}
\noindent Teniendo en cuenta las propiedades de $\varepsilon$ que se asumían al tomar un modelo aditivo.
 
\noindent Un estimador insesgado que se puede dar de la varianza $\sigma^2$ es el siguiente:
\begin{align}
\hat{\sigma}^2 = \dfrac{1}{n-p-1}&\sum_{i=1}^n(y_i-\hat{y}_i)^2\\
\intertext{Por tanto la distribución de este estimador es:}
(n-p-1)\hat{\sigma}^2&\thicksim \sigma^2 \chi_{n-p-1}^2
\end{align}

\noindent Teniendo que el vector de parámetros estimados es un estimador insesgado tendrá una distribución $\hat{\beta}\thicksim \mathcal{N}(\beta, (\textbf{X}^T\textbf{X})^{-1}\sigma^2)$.\\
Teniendo en cuenta lo anterior, la construcción del siguiente estadígrafo de contraste para el parámetro estimado $\hat{\beta}_j$ es relativamente sencilla:
\begin{equation}
z_j=\dfrac{\hat{\beta}_j}{\hat{\sigma}\sqrt{v_j}}\thicksim t_{n-p-1}
\end{equation}
\noindent Donde $v_j$ es el elemento $j$-ésimo de la matriz $(\textbf{X}^T\textbf{X})^{-1}$. Por tanto, podemos generar un intervalo de confianza a un nivel de confianza de $1-2\alpha$
\begin{align}
(\hat{\beta}_j-z^{(1-\alpha)}v_j\hat{\sigma}&,\hat{\beta}_j+z^{(1-\alpha)}v_j\hat{\sigma})
\intertext{Esto permite hacer un conjunto de confianza para el vector de parámetros $\beta$}
C_{\beta}=\lbrace \beta| (\hat{\beta}-\beta)^T \textbf{X}^T\textbf{X}&(\hat{\beta}-\beta)\leq \hat{\sigma}^2 \chi_{p+1}^{2\quad (1-\alpha)}\rbrace
\end{align}

\noindent Otro contraste que se puede realizar es comprobar si un subconjunto de variables es más significativo estadísticamente que otro, de manera que se pueda eliminar variables que no aporten información en pos de la sencillez del modelo y su posterior interpretación. 

\noindent Sea $p_1+1$ el número del conjunto más grande de parámetros y $RSS_1$ su error cuadrático respectivo, sean respectivamente $RSS_0$ y $p_0+1$ para el segundo conjunto o subconjunto menor entonces el estadigráfo:
\begin{equation}\label{ec.F}
F=\dfrac{\dfrac{(RSS_0-RSS_1)}{p_1-p_0}}{\dfrac{RSS_1}{N-p_1-1}} \thicksim F_{(p_1-p_0),(N-p_1-1)}
\end{equation}

\noindent Este estadígrafo mide si la diferencia entre los errores cuadráticos es lo suficientemente grande para que hacer el cambio de $p_1$ a $p_0$ parámetros sea útil. 

\subsection{Regresión Múltiple mediante Ortogonalización sucesiva }

\noindent Otra manera de afrontar el problema de ajuste es hacerlo de una manera geométrica, se busca un subespacio que se ajuste lo mejor posible a un vector de respuestas, en particular, se buscan proyecciones ortogonales del vector respuesta o del subespacio respuesta en el subespacio de los datos. 


\noindent Teniendo en cuenta que el vector de respuestas $\textbf{y}$ sigue el modelo $\textbf{y}=\textbf{X}\beta+\varepsilon$, entonces, $\hat{\textbf{y}}=\textbf{X}\hat{\beta}$ pertenece al subespacio generado por las columnas de la matriz de datos $\textbf{X}$. En caso de que $\textbf{y}\in Col(\textbf{X})$, se tiene que existe un $\beta$ tal que $\textbf{y}=\textbf{X}\beta$. 

\begin{defi}
Se llama vector residuo $\textbf{r}$ al vector cuyas componentes son $r_i=y_i-\textbf{x}_i\hat{\beta}$, es decir, $\textbf{r}=\textbf{y}-\textbf{X}\hat{\beta}$
\end{defi}

\noindent Para que la distancia entre el vector respuesta y el espacio de datos sea lo menor posible el vector residuo debe ser ortogonal al espacio que generan las columnas de la matriz de datos y esto provoca lo siguiente:
\begin{equation}
\begin{split}
r \perp \textbf{X}\Rightarrow \textbf{X}^T \textbf{r}&=\textbf{X}^T (\textbf{y}-\textbf{X}\hat{\beta})=0\\
\textbf{X}^T\textbf{y}-\textbf{X}^T\textbf{X}\hat{\beta}&=0\Rightarrow \hat{\beta}= (\textbf{X}^T\textbf{X})^{-1} \textbf{X}^T \textbf{y}
\end{split} 
\end{equation}

\noindent Por tanto, el vector de estimadores $\hat{\beta}$ es equivalente al cálculo por el método de los mínimos cuadrados. 
Por lo tanto, teniendo en cuenta que $\hat{\textbf{y}}=\textbf{X}(\textbf{X}^T\textbf{X})^{-1} \textbf{X}^T \textbf{y}$. De esta manera, lo que se está haciendo es tomar la proyección de $\textbf{y}$ sobre el espacio $Col(\textbf{X})$. 

\noindent Teniendo en cuenta esto en \textit{Hastie et.al.} \cite{Hastie 2001} define un algoritmo para calcular el vector de parámetros $\beta$. Teniendo en cuenta esto y que $\textbf{y}=(y_1,\ldots,y_n)^T$ el vector  de observaciones de la variable respuesta y $\textbf{x}=(x_1,\ldots x_n)^T$ el vector de observaciones de una única variable predictora. Entonces se puede expresar el vector de parámetros con el producto escalar: 
\begin{equation}
\hat{\beta}=\dfrac{\langle \textbf{x},\textbf{y} \rangle}{\langle \textbf{x},\textbf{x} \rangle}
\end{equation}

\noindent En el caso general, en el que se tienen $p$ variables predictoras, esto cambia, si todas las columnas de la matriz fuesen ortogonales entre sí cada componente se calcularía de la siguiente manera:
\begin{equation}
\hat{\beta}_j=\dfrac{\langle \textbf{x}_j,\textbf{y}\rangle}{\langle \textbf{x}_j,\textbf{x}_j\rangle}
\end{equation}

\noindent En caso contrario, se deben ortogonalizar los datos, para ello, se utiliza el procedimiento de ortogonalización de Gram-Schmidt



%\noindent Se obtiene el mismo estimador de los parámetros $\beta$ que si se hubiera utilizado el Método de los Mínimos Cuadrados. De esta manera podemos establecer que 
%\begin{equation}
%\hat{\beta}=\dfrac{\sum_{i=1}^n\textbf{x}_i y_i}{\sum_{i=1}^n\textbf{x}_i^2} 
%\end{equation}
%\noindent De esta manera, se puede establecer una manera recursiva para encontrar cada uno de los parámetros $\beta_j$ distinto. Mediante proyecciones sucesivas se obtiene lo siguiente:




\subsection{Regresión sobre varias variables}
\noindent Hasta ahora, se ha considerado una única variable aleatoria respuesta, sea \textbf{y} el vector aleatorio de variables respuesta $\textbf{y}^T=[Y_1,\ldots, Y_K]$ y un vector aleatorio de variables de entrada $\textbf{x}^T=[X_0, X_1,\ldots, X_p]$. Se puede establecer el siguiente modelo análogo:
\begin{align}
Y_k=\beta_{0k}+\sum_{j=1}^p& X_j\beta_{jk}+\varepsilon_k=f_k(\textbf{x})+\varepsilon_k\\
\intertext{Recogidas n muestras tendremos la siguiente expresión matricial: }
\textbf{Y}=&\textbf{X}\textbf{B}+\textbf{E}
\end{align}

\noindent Dentro de esa expresión matricial se tiene que: 
\begin{itemize}
\item $\textbf{Y}$ es la matriz de tamaño $n\times K$ que contiene los valores observados de las variables respuesta
\item $\textbf{X}$ matriz de datos habitual de tamaño $n \times p+1$ 
\item $\textbf{B}$ matriz de tamaño $ p+1 \times K$ que contiene los parámetros de la regresión 
\item $\textbf{E}$ es la matriz de tamaño $ n \times K$ que contiene los errores. 
\end{itemize}

\noindent El proceso de ajuste, en el caso de que los errores $\varepsilon^T=[\varepsilon_1,\ldots \varepsilon_K]$ no estén correlados, de la matriz de parámetros es análogo al de una sola variable respuesta de tal manera que minimizando el error cuadrático acumulado se obtiene $\hat{\textbf{B}}=(\textbf{X}^T\textbf{X})^{-1}\textbf{X}^T\textbf{Y}$. En caso de que los errores tengan una matriz de covarianzas conocida, $\Sigma$, entonces es necesario hacer la siguiente modificación en el $RSS$:
\begin{equation}
RSS(\textbf{B},\Sigma)=\sum_{i=1}^n(y_i-f(\textbf{x}_i))^T \Sigma^{-1} (y_i-f(\textbf{x}_i))
\end{equation}

\noindent En la última expresión se usa la \textit{distancia de Mahalanobis}.
\begin{defi}
Según \textit{Cuadras C.M.}\cite{Cuadras 2014} la \textit{distancia de Mahalanobis} entre dos observaciones $\textbf{x}_i, \textbf{x}_j$ extraídas de una misma población con matriz de covarianzas $\Sigma$ se define de la siguiente manera: 
\begin{equation}
d_M(\textbf{x}_i, \textbf{x}_j)=\sqrt{(\textbf{x}_i- \textbf{x}_j)^T \Sigma^{-1}(\textbf{x}_i-\textbf{x}_j)}
\end{equation}
\end{defi}

\noindent En el caso anterior, $(y_i-f(\textbf{x}_i))=\varepsilon_i$ y se está haciendo la distancia de Mahalanobis respecto de su media, el 0. 


\subsection{Selección de subconjuntos y métodos penalizados}

\noindent Anteriormente, se ha detallado un estadígrafo que permitía hacer un contraste sobre la cantidad de variables a considerar en la regresión. 
Esta reducción de características a considerar permite hacer mucho más interpretable el modelo obtenido durante todo el proceso de ajuste y en algunos casos incluso aumentar la precisión del modelo ya que puede ocurrir que se eliminen variables que introduzcan ruido en el modelo. 

\noindent Se podría seguir un método exhaustivo que calcule cada uno de los subconjuntos posibles para cada número de variables que creamos necesarias y calcular el error cuadrático acumulado de cada uno de los subconjuntos posibles. Pero este método es de una complejidad computacional alta. 

\noindent Otra posibilidad sería utilizar el estadígrafo de contraste $F$ definido en la Ecuación \eqref{ec.F} empezando con una sola variable e ir añadiendo cada variable que mejore el ajuste. También se puede hacer al revés, empezando con el modelo con todas las variables e ir reduciendo la cantidad de estas. 

\noindent El problema de estos métodos de selección de variables es que es un proceso discreto, una variable es o no considerada para el modelo siguiente y esto puede generar sobreajuste o infraajuste, no habiendo un término medio. 

\noindent Para ello, existen los métodos penalizados o de encogimiento, que añaden un término de penalización para que los parámetros $\beta$ no sean muy grandes, en el caso del ajuste por mínimos cuadrados del modelo lineal. 

\begin{equation}
PRSS(\beta)=\sum_{i=1}^n(\textbf{y}_i-\textbf{x}_i^T\beta)^2+\lambda\sum_{j=1}^p\beta_j^2
\end{equation}

\noindent El último término hace que parámetros $\beta_j$ grandes sean considerados perjudiciales, y el parámetro $\lambda$ es una forma de regular cuanta importancia tiene dicha penalización, cuanto mayor sea, este provocará un encogimiento mayor de los parámetros. 
De esta manera, se tiene una forma continua de considerar los pesos no eliminando en de manera total las variables o haciendo el $\beta_j$ correspondiente cercano a 0. 















