\section{Métodos Lineales para regresión}

\noindent El objetivo de la regresión es encontrar como un conjunto de variables respuesta se ven afectadas por otro conjunto de variables predictoras. En este caso, se estudia como una combinación lineal de las variables puede afectar a las variables respuesta. Esto se puede hallar con fines predictivos o con el fin de analizar cómo cada una de las variables predictivas afectan a las variables respuesta. 

\noindent Para ello, se tendrá en cuenta que las observaciones de la variable respuesta siguen la siguiente relación:
\begin{equation}
Y=f(\textbf{x})+\varepsilon
\end{equation}
\noindent Donde $\varepsilon$ es una variable aleatoria con $\mathbb{E}(\varepsilon)=0$ y $Var(\varepsilon)=\sigma^2$, habitualmente $\varepsilon \sim N(0,\sigma^2)$. Además supóngase que el vector de datos de entrada sigue una distribución normal multivariante $N_p(\mathbf{\mu},\mathbf{\Sigma})$ donde $\mathbf{\Sigma }$ es una matriz semidefinido positiva y simétrica \cite{Chatfield 1989}.


\noindent En este tipo de métodos se supone que $f$ es una función lineal de las variables de entrada del vector aleatorio $\textbf{x}=[X_1\ldots X_p]$. De esta manera, se tiene que:  
\begin{equation}
f(\textbf{x})=\beta_0+\sum_{j=1}^p X_j\beta_j
\end{equation}

\begin{defi}
Se llaman \emph{parámetros de regresión} al vector $\beta=[\beta_0, \beta_1, \ldots \beta_p]$ de tamaño $(p+1)$ con los coeficientes necesarios para la regresión. 
\end{defi} 

\noindent Añadiendo al vector $\mathbf{x}$ una nueva variable aleatoria $X_0$ que sea constantemente 1, se puede dar la siguiente expresión matricial de la función $f$:
\begin{equation}
f(\mathbf{x})= \mathbf{x}\beta^T
\end{equation}

\noindent De esta manera, tomando la matriz de datos de entrada de tamaño $N\times (p+1)$ entonces se puede generar un vector de predicciones de tamaño $N$ de la siguiente manera:
\begin{equation}
\mathbf{\hat{y}}=\mathbf{X}\beta^T  
\end{equation}  

\noindent Si en particular se quiere hacer una predicción para una nueva observación  $\mathbf{x}_0$ basta con calcular basta con calcular $f(\textbf{x}_0).$

\noindent Una vez establecida la notación y los supuestos que tomaremos de ahora en adelante, hay que estimar los parámetros de regresión, para ello utilizaremos distintos métodos como el método de los mínimos cuadrados o el de Máxima verosimilitud, a parte de esto, veremos las características inferenciales de los estimadores obtenidos y su interpretación geométrica. 

\subsection{Métodos de ajuste}
\noindent Sea una matriz de datos $\textbf{X}$ de tamaño $N\times (p+1)$ resultado de hacer $N$ observaciones de $p$ variables aleatorias y añadir a la primera componente de cada observación un 1. Sea también un vector de respuestas $\textbf{y}=[y_1,\ldots y_N]$ de tamaño $N$, resultado de observar la variable respuesta $Y$. 

\noindent Ahora podemos sea el vector de parámetros de regresión $\beta$ de tamaño $p+1$ definido como antes, entonces podemos definir el concepto de función de pérdida

\begin{defi}
Se llama \emph{función de pérdida} \cite{Hastie 2001} a una función $L(\textbf{y},\mathbf{\hat{y}})$ monótona creciente de la diferencia entre la predicción y el valor real de una variable
\end{defi}

\noindent Aunque esta definición sea más propia del aprendizaje automático, también se usa para el método de los mínimos cuadrados \cite{Abdi 2007}. 

\begin{defi}
Se llama \emph{error de predicción cuadrático} entre una variable observada, $y$ y la predicción obtenida por un cierto modelo $\hat{y}$ a la expresión $(y-\hat{y})^2$
\end{defi}

\noindent Tomando la suma de los errores cuadrados y minimizando se puede obtener una estimación  del vector de parámetros $\beta$. Utilizando las expresiones anteriores:
\begin{equation}
RSS(\beta)=\sum_{i=1}^n(y_i-\hat{y}_i)^2 = \sum_{i=1}^n(y_i-\textbf{x}_i\beta^T)^2=(\textbf{y}-\textbf{X}\beta^T)(\textbf{y}-\textbf{X}\beta^T)^T\\
\end{equation}

\noindent Para obtener el mínimo se debe hallar los puntos estacionarios, lo que implica calcular la derivada de la suma de residuos cuadrados respecto del vector $\beta$, y como esta es una forma cuadrática general respecto de $\beta$, entonces \cite{Morrison 1976}:
\begin{equation}
\dfrac{\partial RSS(\beta)}{\partial \beta}= -2\mathbf{X}^T(\mathbf{y}-\mathbf{X}\beta^T)^T
\end{equation}

\noindent La segunda derivada respecto del vector de parámetro, entonces la segunda derivada es la matriz $2\mathbf{X}^T\mathbf{X}$ que es semidefinida positiva, ya que todos los valores propios son positivos o nulos. Entonces los $\beta$ en los que $-2\mathbf{X}^T(\mathbf{y}-\mathbf{X}\beta^T)^T=0$ son mínimos.  

\noindent Asumiendo que $\textbf{X}$ es una matriz de rango máximo, de lo contrario, habría dos columnas o más que son combinación lineal la una de la otra, la matriz $\textbf{X}^T\textbf{X}$ es definida positiva, por tanto la solución de la siguiente ecuación:
\begin{equation}
\textbf{X}^T(\textbf{y}-\textbf{X}\beta^T)=0
\end{equation}

\noindent Si además, $\textbf{X}^T\textbf{X}$ es una matriz invertible, entonces tiene solución única y es :
\begin{equation}
\hat{\beta}=(\textbf{X}^T\textbf{X})^{-1}\textbf{X}^T\textbf{y}
\end{equation}

\noindent Para el cálculo de este parámetro se ha utilizado lo que se conoce por el método de los mínimos cuadrados. Aún así hay otros métodos de obtención como el geométrico o el de máxima verosimilitud. 

\noindent Con la siguiente proposición se puede ver la equivalencia entre los mínimos cuadrados y el de máxima verosimilitud:

\begin{propo}
Esta estimación de los parámetros $\beta$ por mínimos cuadrados es equivalente a la estimación de estos mediante el método de máxima verosimilitud.
\begin{proof}
Sea una muestra de tamaño $N$, $y_i, i=1,\ldots N$ de una variable aleatoria $Y$, de manera que su función de probabilidad, $\mathbb{P}_\theta(y)$ depende de los parámetros $\theta$. Entonces el método de máxima verosimilitud busca maximizar la siguiente función.
\begin{equation}
L(\theta)=\sum_{i=1}^n log( \mathbb{P}_{\theta}(y_i))
\end{equation}

\noindent Suponiendo que la variable respuesta cumple como antes que  $Y=f_\theta (\textbf{x})+\varepsilon$, donde $\varepsilon \thicksim N(0,\sigma^2)$, provoca que si se suponen conocidos a priori el parámetro $\theta$ y el vector aleatorio $\textbf{x}$ entonces :
\begin{equation}
\mathbb{P}(Y|\textbf{x},\theta)=N(f_\theta(\textbf{x}), \sigma^2)
\end{equation}

\noindent Teniendo esto en cuenta, la función $L(\theta)$ tiene la siguiente expresión:
\begin{equation}
L(\theta)=-\dfrac{n}{2}log(2\pi)-n log(\sigma)-\dfrac{1}{2\sigma^2}\sum_{i=1}^n (y_i-f_\theta (x_i))^2
\end{equation}
Por tanto, teniendo en cuenta que el último término es $RSS(\theta)$, entonces maximizar $L(\theta)$ es equivalente a minimizar $RSS(\theta)$
\end{proof}
\end{propo}
\begin{coro}
Las estimaciones para cualquier $f_{\theta}$ obtenidas por el método de los mínimos cuadrados son equivalentes a las del método de máxima verosimilitud.  
\end{coro}
\noindent Por tanto, los valores predichos observados $\hat{\textbf{y}}$ se calculan de la siguiente manera:
\begin{equation}
\hat{\textbf{y}}=\textbf{X}\hat{\beta}=\textbf{X}^T(\textbf{X}^T\textbf{X})^{-1}\textbf{X}^T\textbf{y}
\end{equation}

\noindent De esta manera, se está calculando la predicción de la respuesta $\hat{y}_i$ para cada una de las observaciones $\mathbf{x}_i, i =1, \ldots N$. 

\noindent Otra forma de afrontar el ajuste del modelo desde el punto de vista del algebra lineal. Esto se puede hacer mediante proyecciones ortogonales, tomando el subespacio de $\mathbb{R}^N$ generado por las columnas de la matriz de datos.

\noindent En lo sucesivo se considera a no ser que se diga lo contrario que $N>p+1$ y que la matriz de datos $\mathbf{X}$ es de rango máximo, $p+1$. 

\begin{defi}
Se llama  $C_{p+1}(\mathbf{X})$ al subespacio lineal de $\mathbb{R}^{N}$ generado por las columnas de la matriz $\mathbf{X}$ \cite{Cuadras 2014}.
\end{defi}
\begin{propo}
El vector $\hat{\mathbf{e}}=\mathbf{y}-\mathbf{X\hat{\beta}^T}$ es ortogonal al subespacio $C_{p+1}(\mathbf{X})$.
\begin{proof}
Premultiplicando por $\mathbf{X}$:
\begin{equation}
\begin{split}
\mathbf{X\hat{\mathbf{e}}}^T=\mathbf{X}(\mathbf{y-X}\beta^T)^T=\mathbf{Xy}^T-\mathbf{X^T X}\beta=0
\end{split}
\end{equation}
\end{proof}
\end{propo}


\subsection{Inferencias Estadísticas sobre $\hat{\beta}$}

\noindent En esta parte se estudian las propiedades de los estimadores obtenidos mediante el método de los mínimos cuadrados. 

\noindent Hay que tener en cuenta los siguientes supuestos el vector $\mathbf{x}\sim N_p(\mu, \mathbf{\Sigma})$ una distribución multivariante de dimensión $p$, del cual tomamos $N$ observaciones independientes obteniendo la matriz de datos $\mathbf{X}$, que a no ser que se diga lo contrario, se supondrá conocida. Además, supóngase el vector de $N$ observaciones de la variable respuesta sigue el siguiente modelo $\mathbf{y}= \mathbf{X \beta}^T+\varepsilon$ donde $\varepsilon \sim N(0,\sigma^2\mathbf{I}_N)$,es un vector de longitud $N$ en el que cada componente es una normal $N(0,\sigma^2)$  independientes entre sí. Esto nos permite dar la siguiente proposición \cite{Cuadras 2014}
\begin{propo}
El vector aleatorio formado por $N$ observaciones de la variable respuesta conocida la matriz de datos y el vector de parámetros $\beta$ sigue una distribución
$\mathbf{y}\sim N_N(\mathbf{X\beta}^T, \sigma^2\mathbf{I}_N)$, donde $\mathbf{I}_N$ es la matriz identidad de tamaño $N$.  
\begin{proof}
Basta con comprobar que:
\begin{equation}
\mathbb{E}(\mathbf{y})=\mathbb{E}(\mathbf{X\beta}^T)+\mathbb{E}(\varepsilon)=\mathbb{E}(\mathbf{X\beta}^T)=\mathbf{X\beta}^T
\end{equation}
Y la varianza :
\begin{equation}
\begin{split}
\mathbb{E}(\mathbf{X\beta}^T+\varepsilon)(\mathbf{X\beta}^T+\varepsilon)^T)=\mathbf{X\beta}^T\mathbf{\beta X}^T+ Var(\varepsilon
)\\
Var(\mathbf{y})=\mathbf{X\beta}^T\mathbf{\beta X}^T+ Var(\varepsilon
)-\mathbf{X\beta}^T\mathbf{\beta X}^T=Var(\varepsilon)=\sigma^2\mathbf{I}_N
\end{split}
\end{equation}
\noindent Y se concluye ya que el vector $\varepsilon$ es una normal. 
\end{proof}
\end{propo}
\noindent Añadamos a las suposiciones  que la matriz $\mathbf{X}$ es de rango máximo y por tanto, $\mathbf{X}^T\mathbf{X}$ es semidefinido positiva. 
\begin{propo}
El estimador $\hat{\beta}$ es un estimador insesgado \cite{Greene 2008}
\begin{proof}
Teniendo en cuenta las anteriores hipótesis anteriores, incluyendo que la variable respuesta es:  

\noindent De esta manera, se obtiene que $\hat{\beta}=(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T(\mathbf{X}\beta^T+\varepsilon)=\beta+(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T \varepsilon$

\noindent Entonces, se tiene que conocida $\mathbf{X} \Rightarrow \mathbb{E}(\hat{\beta}|\mathbf{X})=\mathbb{E}(\beta+(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T \varepsilon|\mathbf{X})=\beta +\mathbb{E}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T \varepsilon|X)=\beta$ debido a la distribución de $\varepsilon$

\noindent Por tanto como $\mathbb{E}(\mathbb{E}(\hat{\beta}|\mathbf{X}))=\mathbb{E}(\beta)=\beta$
\end{proof}
\end{propo}

\begin{propo}
La varianza del estimador $Var(\hat{\beta})= \sigma^2 (\mathbf{X}^T \mathbf{X})^{-1}$\cite{Hastie 2001},\cite{Greene 2008}
\begin{proof}
\begin{equation}
\begin{split}
Var(\hat{\beta})&=\mathbb{E}((\mathbf{X}^T \mathbf{X})^{-1}\mathbf{X}^T \varepsilon \varepsilon\mathbf{X}(\mathbf{X}^T \mathbf{X})^{-1}|X)\\
&=\mathbb{E}(\varepsilon^2) (\mathbf{X}^T \mathbf{X})^{-1}(\mathbf{X}^T \mathbf{X})(\mathbf{X}^T \mathbf{X})^{-1} \\
&=\sigma^2 (\mathbf{X}^T \mathbf{X})^{-1}
\end{split}
\end{equation}
\end{proof}
\end{propo}

\noindent Por último, tenemos que el parámetro de estimadores $\hat{\beta}= \beta+(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T \varepsilon$, es una combinación afín de $\varepsilon$ una variable con distribución por tanto, tendremos que cada uno de los componentes del vector $\hat{\beta},\hat{\beta}_j\sim N(\beta_j, \sigma^2(\mathbf{X^TX}_{jj}^{-1})) j=1\ldots p$ donde $(\mathbf{X^TX}_{jj}^{-1})$ es el j-ésimo elemento de la diagonal de la matriz $(\mathbf{X}^T\mathbf{X})^{-1}$ \cite{Johnson 2007}.

\noindent Si el parámetro $\sigma^2$ fuera conocido, entonces se podría llevar a cabo los contrastes de manera sencilla. En este caso, $z_j=\dfrac{\hat{\beta}_j}{\sigma \sqrt{v_j}}\sim N(0,1)$

\noindent Para poder hacer los estadígrafos de contrastes se debe construir un estimador de esta varianza, ya que habitualmente no es conocida. Para ello \emph{Hastie et. al.} proponen el siguiente estimador de la varianza, suponiendo que unicamente hay una variable respuesta: 
\begin{equation}
\hat{\sigma}^2=\dfrac{1}{N-p-1}\sum_{i=1}^N (y_i-\hat{y}_i)^2 
\end{equation} 


\begin{propo}
El estimador $\hat{\sigma}^2$ cumple lo siguiente :
\begin{itemize}
\item $\mathbb{E}(\hat{\sigma} ^2)=\sigma^2$ (Es un estimador insesgado de la varianza)
\item  $\hat{\sigma}^2 \sim \frac{\sigma^2}{N-p-1}\chi_{N-p-1}^2 $
\end{itemize}
\begin{proof}
\noindent Sea $\lbrace t_{1}\ldots t_{p+1}, t_{p+2}, \ldots t_{N}\rbrace$ una base ortogonal del espacio $\mathbb{R}^N$ dentro de los cuales $t_{1},\ldots t_{p+1}$ es base de $C_{p+1}(\mathbf{X})$, de tal manera que los últimos $N-p-1$ vectores son ortogonales a la $C_{p+1}(\mathbf{X})$. Por tanto si tomamos la matriz de cambio de base $\mathbf{T}$ con los vectores $\lbrace t_{1}\ldots t_{p+1}, t_{p+2}, \ldots t_{N}\rbrace$ como columnas. 

\noindent Sea el cambio de base $\mathbf{z}=\mathbf{T} \mathbf{y}\Rightarrow \mathbb{E}(z_i)=t_i^T \mathbf{X}\beta^T=0 $ cuando $i\geq p+1$ Por ello, cambiando de base el vector $\hat{\mathbf{e}}\Rightarrow \mathbf{T}\hat{\mathbf{e}}=(\overbrace{0\ldots 0}^{p+1},z_{p+2},\ldots z_{N})^T$.

\noindent La esperanza de $\mathbb{E}(\hat{\mathbf{e}})=\mathbb{E}(\mathbf{T}\hat{\mathbf{e}})=0$ De esta manera, se obtiene lo siguiente:
\begin{equation}
\mathbb{E}(\hat{\mathbf{e}}^T\hat{\mathbf{e}})=\mathbb{E}(\hat{\mathbf{e}}^T \mathbf{T^T T}\hat{\mathbf{e}})=\sum_{i=p+2}^N\mathbb{E}(z_i^2)=
\end{equation}

\noindent De esta manera, como $\sum_{i=1}^N(y_i-\hat{y}_i)^2=\hat{\mathbf{e}}^T\hat{\mathbf{e}}= \hat{\mathbf{e}}^T \mathbf{T^T T} \hat{\mathbf{e}}=\sum_{i=p+2}^N z_i$ entonces el estimador:
\begin{equation}
\hat{\sigma}^2=\dfrac{1}{N-p-1}\sum_{i=p+2}^N z_i^2=\dfrac{1}{N-p-1}\hat{\mathbf{e}}^T\hat{\mathbf{e}}
\end{equation}

\noindent Tomando esperanzas se obtiene que efectivamente el estimador es insesgado. 
Por tanto, tenemos un estimador insesgado que es suma de $N-p-1$ normales estándar multiplicadas por $\sigma^2$ y dividas entre $N-p-1$, por tanto, tenemos que además $\hat{\sigma}^2\sim \dfrac{\sigma^2}{N-p-1}\chi_{N-p-1}$
\end{proof}
\end{propo}

\noindent Por tanto, ahora ya podemos establecer, conocidas las distribuciones de los estimadores construidos que \cite{Greene 2008}, \cite{Hastie 2001}:
\begin{equation}
t_j=\dfrac{\hat{\beta}_j}{\hat{\sigma}^2\sqrt{v_j}}, j=1,\ldots ,p+1 \sim t_{N-p-1}
\end{equation}

\noindent Donde $v_j$ es como antes el $j-$ésimo elemento de la diagonal de $\mathbf{X}^T \mathbf{X}$

\noindent Conocer la distribución de los estimadores permite realizar contrastes sobre los distintos parámetros. De manera habitual, se plantea la hipótesis nula $H_0: \beta_j=0 $. Esta hipótesis busca comprobar si la variable $X_j$ es importante en el modelo lineal. En caso de que se acepte la hipótesis esa variable puede ser eliminada del modelo. 


\noindent Teniendo en cuenta esto,  hay ocasiones en las que se desea comprobar si un subconjunto de variables es más significativo estadísticamente que otro, de manera que se pueda eliminar variables que no aporten información en pos de la sencillez del modelo y su posterior interpretación. 

\noindent Sea $p_1+1$ el número del conjunto más grande de parámetros o de variables a considerar y $RSS_1$ su error cuadrático respectivo, sean respectivamente $RSS_0$ y $p_0+1$ para el segundo conjunto o subconjunto menor, si además contamos que :
\begin{equation}
\frac{RSS(\hat{\beta})}{N-p-1}=\frac{1}{N-p-1}\sum_{i=1}^N(y_i-\hat{y}_i)^2\sim \sigma^2 \chi_{N-p-1}
\end{equation}

\noindent Se puede definir el estadígrafo: 
\begin{equation}\label{ec.F}
F=\dfrac{\dfrac{(RSS_0-RSS_1)}{p_1-p_0}}{\dfrac{RSS_1}{N-p_1-1}} 
\end{equation}

\noindent Que cumple: 
\begin{propo}
El estadígrafo $F\thicksim F_{(p_1-p_0),(N-p_1-1)}$
\begin{proof}
Teniendo en cuenta lo anterior, se puede comprobar que:
\begin{equation}
F\sim \dfrac{\chi^2_{p_1-p_0}}{\chi^2_{N-p_1-1}}=F_{(p_1-p_0),(N-p_1-1)}\qedhere
\end{equation}
\end{proof}
\end{propo}

\noindent Este estadígrafo se referencia en secciones sucesivas con el objetivo de reducir las variables involucradas en la regresión.

\subsection{Regresión multivariante}
\noindent Hasta ahora, se ha considerado una única variable aleatoria respuesta, sea \textbf{y} el vector aleatorio de variables respuesta $\textbf{y}=[Y_1,\ldots, Y_K]$ y un vector aleatorio de variables de entrada $\textbf{x}^T=[X_0, X_1,\ldots, X_p]$. Se puede establecer el siguiente modelo análogo, donde :
\begin{align}
Y_k=\beta_{0k}+\sum_{j=1}^p X_j\beta_{jk}+\varepsilon_k =& f_k(\textbf{x})+\varepsilon_k, \quad k=1,\ldots K \\
\intertext{Donde el término $\varepsilon_k \sim N(0, \sigma_k^2)$ es el error inevitable para la variable $Y_k, k=1\ldots K$.
De esta manera, si se toman $N$ observaciones se puede crear las matrices de datos de respuesta y de entrada y obtener la siguiente expresión: }
\mathbf{Y} =& \mathbf{XB+E}
\end{align}

\noindent Dentro de esa expresión matricial se tiene que: 
\begin{itemize}
\item $\textbf{Y}$ es la matriz de tamaño $N \times K$ que contiene los valores observados de las variables respuesta
\item $\textbf{X}$ matriz de datos habitual de tamaño $N \times (p+1)$ 
\item $\textbf{B}$ matriz de tamaño $ (p+1) \times K$ que contiene los parámetros de la regresión 
\item $\textbf{E}$ es la matriz de tamaño $ N \times K$ que contiene las errores cometidos en cada uno de las variables respuesta. 
\end{itemize}

\noindent El proceso de ajuste, en el caso de que los errores $\varepsilon^T=[\varepsilon_1,\ldots \varepsilon_K]$ no estén correlados, de la matriz de parámetros es análogo al de una sola variable respuesta de tal manera que minimizando el error cuadrático acumulado se obtiene $\hat{\textbf{B}}=(\textbf{X}^T\textbf{X})^{-1}\textbf{X}^T\textbf{Y}$. En caso de que los errores tengan una matriz de covarianzas conocida, $\mathbf{\Sigma}$, entonces es necesario hacer la siguiente modificación en el $RSS$:
\begin{equation}
RSS(\textbf{B},\Sigma)=\sum_{i=1}^n(y_i-f(\textbf{x}_i))^T \Sigma^{-1} (y_i-f(\textbf{x}_i))
\end{equation}

\noindent En la última expresión se usa la \textit{distancia de Mahalanobis}.
\begin{defi}
La \textit{distancia de Mahalanobis} entre dos observaciones $\textbf{x}_i, \textbf{x}_j$ extraídas de una misma población con matriz de covarianzas $\Sigma$ se define de la siguiente manera\cite{Cuadras 2014}: 
\begin{equation}
d_M(\textbf{x}_i, \textbf{x}_j)=\sqrt{(\textbf{x}_i- \textbf{x}_j)^T \mathbf{\Sigma}^{-1}(\textbf{x}_i-\textbf{x}_j)}
\end{equation}
\end{defi}

\noindent En el caso anterior, $(y_i-f(\textbf{x}_i))=\varepsilon_i$ y se está haciendo la distancia de Mahalanobis respecto de su media, el 0. 


\subsection*{Selección de subconjuntos y métodos penalizados}

\noindent Anteriormente, se ha detallado un estadígrafo que permitía hacer un contraste sobre la cantidad de variables a considerar en la regresión. 
Esta reducción de características a considerar permite hacer mucho más interpretable el modelo obtenido durante todo el proceso de ajuste y en algunos casos incluso aumentar la precisión del modelo ya que puede ocurrir que se eliminen variables que introduzcan ruido en el modelo. 

\noindent Se podría seguir un método exhaustivo que calcule cada uno de los subconjuntos posibles para cada número de variables que creamos necesarias y calcular el error cuadrático acumulado de cada uno de los subconjuntos posibles. Pero este método es de una complejidad computacional alta. 

\noindent Otra posibilidad sería utilizar el estadígrafo de contraste $F$ definido en la Ecuación \eqref{ec.F} empezando con una sola variable e ir añadiendo cada variable que mejore el ajuste. También se puede hacer al revés, empezando con el modelo con todas las variables e ir reduciendo la cantidad de estas. A estos algoritmo se les llaman \emph{algoritmos voraces} los cuales buscan una solución óptima en cada paso y no en global. 

\noindent El problema de estos métodos de selección de variables es que es un proceso discreto, una variable es o no considerada para el modelo siguiente y esto puede generar sobreajuste o infraajuste, no habiendo un término medio. 

\noindent Para ello, existen los métodos penalizados o de encogimiento, que añaden un término de penalización para que los parámetros $\beta$ no sean muy grandes.

\begin{defi} 
Llamamos \emph{Errores Cuadrados Acumulados Penalizados, PRSS,} a la suma de los cuadrados de los errores cometidos con el modelo lineal que usa el  vector de parámetros $\beta$, añadiendo un término regulador $\lambda >0$: 
\begin{equation}
PRSS(\beta)=\sum_{i=1}^n(\textbf{y}_i-\textbf{x}_i^T\beta)^2+\lambda\sum_{j=1}^p\beta_j^2
\end{equation}
\end{defi}
\noindent El último término hace que parámetros $\beta_j$ grandes sean considerados perjudiciales, y el parámetro $\lambda$ es una forma de regular cuanta importancia tiene dicha penalización, cuanto mayor sea, este provocará un encogimiento mayor de los parámetros. 
De esta manera, se tiene una forma continua de considerar los pesos no eliminando en de manera total las variables o haciendo el $\beta_j$ correspondiente cercano a 0. 















