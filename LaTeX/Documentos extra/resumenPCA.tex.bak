\newpage
\section{Análisis de componentes principales}
\noindent El Análisis de componentes principales es una de las técnicas multivariantes más antiguas. En primera instancia, esta técnica fue desarrollada en paralelo por Pearson y Hotelling  con distintos enfoques \cite{Pearson 1901, Hotelling 1933}. 

\noindent \emph{Pearson}, en su aproximación, buscó la forma de ajustar de mejor manera puntos de un espacio de dimensión $p$ a una recta o plano \cite{Pearson 1901}. Es decir, Pearson buscó un enfoque de optimización geométrica que llevó a las componentes principales.

\noindent Por el otro lado, \emph{Hotelling}, partiendo de un conjunto de datos provenientes de estudiar un conjunto de variables,  buscaba un subconjunto de variables menor que pudiera determinar de igual manera o parecida los datos \cite{Hotelling 1933}. Hotelling maximizó la contribución que aportaba cada componente a la varianza. Al utilizar para este fin el método de los multiplicadores de Lagrange, obtuvo el problema de valores y vectores propios que se desarrolla en esta sección. 

\noindent Aún así, el trabajo de Hotelling tenia ciertas diferencias con el enfoque que actualmente se le da actualmente a las componentes principales \cite{Jollife 1986}. Por ejemplo, consideraba que las variables originales  debían ser combinaciones de las componentes y no al revés. Tampoco usaba la notación matricial ni la matriz de covarianzas, en su lugar utilizaba la de correlaciones. Añadir que aunque en esta memoria se trabajará con la matriz de covarianzas, Chatfield y Collins desarrollan ambos enfoques \cite{Chatfield 1989}. 

\noindent Según Abdi \cite{Abdi 2010} esta técnica tiene varios objetivos, entre los cuales están el extraer la información más importante de los datos, reduciendo la dimensionalidad y simplificando la descripción en el proceso de los mismos. Y por último, analizar la estructura de los datos. 


\noindent De esta manera, variables bastante correladas, se simplificarán en una que tenga toda la variabilidad de ambas  ya que a fin de cuentas la información que expresan estaría repetida \cite{Chatfield 1989, Everitt 2011}. 

\noindent Para empezar, se detallan cómo se calculan las componentes principales solo teniendo en cuenta las propiedades del vector aleatorio $\mathbf{x}$ de longitud $p$. Tras esto, se tomarán $N$ observaciones con las que poder construir estimadores de las componentes y se analizará como esta reducción de la dimensionalidad es la que permite una mejor reconstrucción de los datos mediante el teorema de Eckart-Young \cite{Eckart 1936}. 

\noindent Para desarrollar esta última parte en la que se habla de la reconstrucción de los datos se da una idea de una medida sobre las matrices, en particular, la norma de Frobenius que detallan Eckart y Young y demuestran que es invariante ante transformaciones ortogonales \cite{Eckart 1936}.

\noindent Como ejemplo de las aplicaciones que puede tener esta técnica Johnson y Wichern  desarrollan un ejemplo sencillo en el cual se estudian 5 variables socioeconómicas de ciertas zonas de Estados Unidos \cite{Johnson 2007}.

\noindent En este ejemplo, las dos primeras componentes explican el $93\%$ de la variabilidad y la primera es una diferencia ponderada de el empleo privado y el empleo público, mientras que la segunda una suma ponderada de las mismas. Por tanto, se puede entender que las variables que mayor importancia tienen en este caso son el porcentaje de funcionarios y de empleados en general.

\noindent Otro ejemplo destacable lo desarrolla Ringnér \cite{Ringnér 2008}. Estudio en el que se reduce un conjunto de datos con más de 8000 variables y 105 observaciones a uno de solo 63 variables para retener el $90\%$ y 105 para retener casi el $100\%$ de la variabilidad total de los datos. Es decir, no siempre se se busca la reducción para representar los datos, en este ejemplo únicamente se busca eliminar datos redundantes en el conjunto total. 

\noindent Otro ejemplo de aplicación sería el que desarrollan Gottumukkal y Asari que mediante una pequeña modificación del algoritmo habitual de las componentes principales en el campo del reconocimiento facial con el objetivo de adaptarse mejor a ciertas condiciones, en las que se tienen grandes variaciones de posición y expresiones \cite {Asari 2004}.

\subsection{Definición y cálculo de las componentes principales}

\noindent Sea un vector aleatorio de longitud $p$, $\mathbf{x}=[X_1,\ldots X_p]$, con matriz de covarianzas $\mathbf{\Sigma}$ con $r=rg(\mathbf{\Sigma})$, se pueden definir las componentes principales \cite{Cuadras 2014}.
\begin{defi}
Se definen las componentes principales 
\begin{equation}
Z_j=v_{1j}X_1+\ldots v_{pj}X_p=\mathbf{v}_j^T\mathbf{x}^T \quad j=1\ldots r
\end{equation}

\noindent Donde $\textbf{v}_j$ es un vector columna con $p$ escalares y la nueva variable aleatoria $Z_j$ cumple lo siguiente:
\begin{itemize}
\item Si $j=1$ $Var(Z_1)$ es máxima restringido a $\mathbf{v}_1^T \mathbf{v}_1=1$
\item Si $j>1$ debe cumplir:
\begin{itemize}
\item $Cov(Z_j,Z_i)=0\quad \forall i\neq j $
\item $\textbf{v}_j^T \textbf{v}_j=1$
\item $Var(Z_j)$ es máxima. 
\end{itemize}
\end{itemize}
\noindent En resumen, lo que se busca es una nueva base que reúna las direcciones de máxima variación y que sean ortogonales respecto a la matriz de covarianzas, es decir, que sean no correladas.
\end{defi}
\noindent \emph{Chatfield} y \emph{Collins}  utilizan el método de los multiplicadores de Lagrange para resolver el problema de maximizar  la varianza de la primera componente sujeto a la restricción $\textbf{v}_1^T\textbf{v}_1=1$ \cite{Chatfield 1989}. Todo esto con el objetivo de calcular el vector $\mathbf{v}_1$. El cálculo de sucesivas componentes cambiará en ciertos aspectos. 

\noindent Aplicando el método de los multiplicadores, la función objetivo se define como la varianza de la combinación lineal, es decir, $f(\mathbf{v})=\mathbf{v}^T \mathbf{\Sigma} \mathbf{v}$ y la restricción aplicada es $g(\textbf{v})=\textbf{v}^T\textbf{v}=1$. 

\noindent Tomando $\mathbf{v}=\textbf{v}_1$ se puede establecer $L(\textbf{v}_1)=\textbf{v}_1^T \mathbf{\Sigma} \textbf{v}_1 - \lambda[\textbf{v}_1^T \textbf{v}_1-1]$. Que al derivarla se obtiene:

\begin{align*}
\dfrac{\partial L}{\partial \textbf{v}_1} &= 2\mathbf{\Sigma} \textbf{v}_1 - 2\lambda\textbf{v}_1\\
& = 2(\mathbf{\Sigma}-\lambda\mathbf{I})\textbf{v}_1 
\end{align*}

\noindent Igualando a 0 tenemos la siguiente ecuación: 
\begin{equation}
\mathbf{\Sigma}\textbf{v}_1=\lambda \textbf{v}_1
\end{equation}

\noindent Para que $\textbf{v}_1$ sea un vector no trivial, se elige $\lambda$ de tal manera que $|\mathbf{\Sigma}-\lambda \mathbf{I}| = 0$, es decir, $\lambda$ es un vector propio de la matriz de covarianzas, $\mathbf{\Sigma}$. Al ser ésta una matriz semidefinido positiva y simétrica, los valores propios son reales y positivos. Por tanto, $\textbf{v}_1$ es un vector propio de la matriz de covarianza.

\noindent La función a maximizar es $Var(Z_1)=Var(\textbf{v}_1^T\textbf{x}^T)=\textbf{v}_1^T\mathbf{\Sigma} \textbf{v}_1=\textbf{v}_1^T\lambda \textbf{v}_1=\lambda$, y para maximizarla basta tomar $\lambda=\max{\lbrace\lambda_1\ldots \lambda_r\rbrace}$. Reordenando si es necesario, se tiene que $\lambda=\lambda_1$. Por tanto, la primera componente viene dada por el vector propio $\mathbf{v}_1$ de valor propio $\lambda_1$ de la matriz de covarianzas $\mathbf{\Sigma}$.

\noindent Una vez calculada la primera componente principal $Z_1$, la segunda componente se calcula de manera análoga, maximizando $Var(Z_2)=Var(\textbf{v}_2^T\textbf{x}^T)$ condicionada por $\textbf{v}_2^T\textbf{v}_2=1$. A esta restricción tenemos que añadir la restricción $Cov(Z_1,Z_2)=0 $

\begin{propo}
La condición $Cov(Z_1,Z_2)=0 $ equivale a la condición \\$\textbf{v}_2^T\textbf{v}_1 = 0$.
\begin{proof}
Utilizando que $Z_j=\textbf{v}_j^T \textbf{x}^T \quad \forall j=1,\ldots p$ , se tiene entonces que :
\begin{align*}
Cov(Z_2,Z_1)&= Cov (\textbf{v}_2^T\mathbf{x}^T,\mathbf{v}_1^T\mathbf{x})\\ 
&= \mathbb{E}(\mathbf{v}_2^T(\mathbf{x}^T-\mu^T)(\mathbf{x}^T-\mu^T)^T \mathbf{v}_1)\\
&= \textbf{v}_2^T \mathbb{E}((\textbf{x}^T-\mu^T)(\mathbf{x}^T-\mu^T)^T) \textbf{v}_1\\
&= \textbf{v}_2^T \mathbf{\Sigma} \textbf{v}_1 \\
&= \textbf{v}_2^T \lambda_1 \textbf{v}_1
\end{align*}
\noindent Donde $\mu$ es el vector de medias del vector aleatorio $\mathbf{x}$. 

\noindent De manera que, si $\mathbf{v}_2^T \lambda_1 \mathbf{v}_1 = 0 \Rightarrow \mathbf{v}_2^T \mathbf{v}_1=0 $, luego son vectores ortogonales entre sí. 
\end{proof}
\end{propo}


\noindent \emph{Observación: } Esta proposición se puede extender de manera simple al caso de tener que calcular la $j$-ésima componente principal habiendo calculado las anteriores de las cuales se sepan los valores propios asociados. 

\begin{coro}
Las componentes principales son todas ortogonales entre sí. 
\end{coro}

\noindent Para $k=2$, se dan dos restricciones, $\textbf{v}_2^T\textbf{v}_2=1$ y además $\textbf{v}_2^T \textbf{v}_1=\textbf{v}_1^T \textbf{v}_2=0$. Para este caso existen $\lambda, \phi$ de manera que la función a maximizar es:
\begin{equation}
 L(\textbf{v}_2)=\textbf{v}_2^T \mathbf{\Sigma} \textbf{v}_2 - \lambda[\textbf{v}_2^T \textbf{v}_2-1]-\phi(\textbf{v}_1^T \textbf{v}_2)
\end{equation}
Que al ser derivado respecto $\textbf{v}_2$ obtenemos:
\begin{equation}
2\mathbf{\Sigma} \textbf{v}_2 - 2\lambda\textbf{v}_2-\phi \textbf{v}_1=0
\end{equation}
Al multiplicar todo por $\mathbf{v}_1^T$, se obtiene que:
\begin{equation}
2\mathbf{v}_1^T\mathbf{\Sigma}\mathbf{v}_2-2\lambda \mathbf{v}_1^T \mathbf{v}_2-\phi \mathbf{v}_1^T\mathbf{v}_1=0
\end{equation}
\noindent Por tanto, teniendo en cuenta las condiciones impuestas al problema, $\phi=0$. De esta manera, se obtiene en la ecuación lo mismo que en el cálculo de la primera. 

\noindent Y de manera análoga al caso de la primera componente, $\lambda=\lambda_2$, el segundo valor propio más grande, y $\mathbf{v}_2$ es el vector propio de valor propio $\lambda_2$ de la matriz de covarianzas $\mathbf{\Sigma}$.

\noindent El proceso para calcular el resto de componentes principales es análogo, únicamente hay que tener en cuenta que se debe dar la ortogonalidad entre las distintas componentes. 

\noindent Por ende, se obtiene que las componentes principales vienen dadas por los vectores propios de la matriz de covarianzas $\mathbf{\Sigma} $. Además sabemos que $Var(\mathbf{v}_j^T \mathbf{x}^T)=\lambda_j,$ $ j=1,\ldots r$ , siendo donde $\lambda_j$ es el $j$-ésimo valor propio más grande no nulo. 

\noindent En esencia, calcular las componentes principales es calcular una base ortonormal que cumple una ciertas condiciones. Por lo tanto, podemos definir lo siguiente: 
\begin{defi}
Se llama matriz de cargas $\mathbf{V}$ a la matriz ortogonal que tiene por columnas a los $\mathbf{v}_j\quad \forall j=1,\ldots, r$. Estos a su vez son un conjunto de vectores ortonormales en el espacio $\mathbb{R}^p$.
\end{defi}

\noindent \emph{Observación: }Se utilizará la condición de que al ser $\mathbf{V}$ una matriz ortogonal $\mathbf{VV}^T=\mathbf{V}^T\mathbf{V}=\mathbf{I}$ y por tanto $\mathbf{V}^{-1}=\mathbf{V}^T$

\noindent Esa matriz nos permite calcular las componentes de la siguiente manera
\begin{equation}
\mathbf{z}=\mathbf{x} \mathbf{V}
\end{equation} 
\noindent Donde el vector $\mathbf{z}=Z_1,\ldots, Z_r$, y donde cada $Z_j=\mathbf{v}_j^T\mathbf{x}, \forall j=1,\ldots r$

\noindent Ahora, veamos como esto se puede aplicar a una matriz de datos observados con la matriz de varianzas muestral.

\noindent Sea $\mathbf{x}$ el vector aleatorio de longitud $p$ definida de la misma manera. A continuación se toman $N$ observaciones independientes de este vector y se obtiene la matriz de datos $\mathbf{X}$ de tamaño $N\times p$. 

\begin{defi}
Dados los datos de $N$ observaciones recogidos sobre $p$ variables $X_1,\ldots, X_p$, entonces $\mathbf{z}_{ij}=\mathbf{x}_i\mathbf{v}_j, ,\forall i=1,\ldots, N,\forall j=1,\ldots p$ es el valor de la $j$-ésima componente para la observación $\mathbf{x}_i$. 

\noindent Donde el vector $\mathbf{v}_j$ cumple las mismas condiciones que para las variables aleatorias. 
\end{defi}

\noindent Por tanto, el proceso que se detalla para un vector aleatorio $\mathbf{x}$ con matriz de covarianzas $\mathbf{\Sigma}$ se puede extender al caso en el conocemos la matriz de covarianzas muestrales \textbf{S}.

\noindent Con el objetivo de hacer las demostraciones más sencillas y compactas tomaremos la matriz $\textbf{X}$ como la matriz centrada $\overline{\textbf{X}}$, es decir: 
\begin{equation}
\overline{x}_{ij}=x_{ij}-\overline{x}_j \quad \forall i=1,\ldots,N,\forall j=1,\ldots,p
\end{equation}
donde $\overline{x}_j$ es la media muestral de la $j$-ésima variable. Esto hace que $\textbf{S}=\frac{1}{n-1}\textbf{X}^T\textbf{X}$. Esto permite hablar de los valores y vectores propios de \textbf{S} y de $\textbf{X}^T \textbf{X}$ indistintamente, ya que los vectores propios son los mismos y los valores propios son proporcionales. 


\subsection{ACP mediante SVD}
\noindent El problema de la reconstrucción es demostrar que la matriz reducida, que se definirá más tarde, es la mejor aproximación de rango menor a la matriz de datos. Es decir, que los datos pueden representarse con una dimensionalidad menor con la mínima pérdida.

\noindent Otra visión del problema es que se quiere buscar una proyección sobre un subespacio de dimensión $m<p$ que contenga la máxima variabilidad posible de los datos y que brinde una mayor capacidad de interpretación de los datos, ya que en el caso de que $m=2$ o $m=3$ se podrán hacer representaciones gráficas de manera sencilla. 
Antes de desarrollar los cálculos debemos revisar las siguientes conceptos.

\noindent Hay que recordar que toda matriz de tamaño arbitrario se puede descomponer en tres matrices en lo que se llama la descomposición en valores singulares \cite{Abdi 2010}.

\begin{defi}
Dada una matriz $\textbf{X}\in  \mathbb{M}_{N\times p}(\mathbb{R})$ existe la descomposición en valores singulares \textit{(SVD)}:
\begin{equation}
\textbf{X}=\textbf{U}\mathbf{D}\textbf{V}^T
\end{equation}
Donde:
\begin{itemize}
\item \textbf{U} matriz ortogonal y de tamaño $N \times N$
\item $\mathbf{D}$ matriz de tamaño $N \times r$ diagonal, cuyos elementos no nulos son los valores singulares $\sigma_1\geq\ldots\geq \sigma_r\geq 0$, es decir
\begin{equation}
\mathbf{D}=\begin{pmatrix}
\sigma_1 & 0 & \cdots  & \cdots \\
0 & \sigma_2 & 0 & \cdots\\
\vdots & \vdots & \vdots & \vdots\\
\cdots & \cdots & 0 & \sigma_r
\end{pmatrix}
\end{equation}
\item \textbf{V} matriz ortogonal y de tamaño $p \times r$. 
\end{itemize}
\end{defi}

\noindent Cada una de las matrices tiene un origen distinto que hay que detallar \cite{Johnson 1963}.

\begin{propo}
La matriz $\mathbf{V}$ de tamaño $p\times r$ es la matriz que contiene los $r$ vectores para hacer la combinación lineal que definen las componentes principales. Es decir es la matriz con la base de diagonalización de $\mathbf{X}^T\mathbf{X}$.
\begin{proof}
Por la descomposición en valores singulares tenemos que:
\begin{align*}
\textbf{X}^T \textbf{X} &= (\textbf{U}\mathbf{D} \textbf{V}^T)^T (\textbf{U}\mathbf{D} \textbf{V}^T)\\
&= \textbf{V}\mathbf{D} ^T \textbf{U}^T \textbf{U}\mathbf{D} \textbf{V}^T\\
&= \textbf{V}\mathbf{D} ^T \mathbf{D} \textbf{V}^T
\end{align*}
donde la matriz $\mathbf{D} ^T \mathbf{D} $ es una matriz diagonal de tamaño $r \times r$ cuyos elementos son los cuadrados de los valores singulares de \textbf{X}, que son a su vez los valores propios de $\textbf{X}^T \textbf{X}$. 

\noindent Añadiendo la condición de ortogonalidad de $\textbf{V}\Rightarrow \textbf{V}^{-1}=\mathbf{V}^T$ es fácil ver que la matriz \textbf{V} es la matriz cuyas columnas son los $r$ vectores propios de $\textbf{X}^T\textbf{X}$ con los valores propios $\sigma_1,\ldots, \sigma_r$.
\end{proof}
\end{propo}

\begin{propo}
La matriz $\mathbf{U}^T$ es la matriz que contiene la base de diagonalización de la matriz $\mathbf{XX}^T$.  
\begin{proof}
Se sigue un razonamiento análogo pero con la matriz $\mathbf{XX}^T$
\begin{align*}
\textbf{X} \textbf{X}^T  &= (\textbf{U}\mathbf{D} \textbf{V}^T)(\textbf{U}\mathbf{D} \textbf{V}^T)^T\\
&= \mathbf{UDV}^T\mathbf{VD}^T\mathbf{U}^T\\
&= \mathbf{UDD}^T\mathbf{U}^T
\end{align*}
Entonces, tenemos que como es ortonormal $\mathbf{U}^T=\mathbf{U}^{-1}$ de esta manera, se puede comprobar al ser $\mathbf{DD}^T$ una matriz diagonal $N\times N$ que efectivamente se cumple la proposición.\qedhere
\end{proof}
\end{propo}

\begin{coro}
El cálculo de las componentes principales mediante la descomposición mediante valores y vectores propios de la matriz de covarianzas, $\mathbf{X}^T\mathbf{X}$ es equivalente a calcular la descomposición en valores singulares de la misma. 
\end{coro}

\noindent En el problema de reconstrucción de datos hay que dar una métrica en el espacio de las matrices, en particular se da la norma de Frobenius ya que cumple ciertas propiedades interesantes \cite{Golub 1987}.

\begin{defi}
Sea $\textbf{A}\in \mathbb{M}_{N\times p}(\mathbb{R})$ definimos la norma de Frobenius de la matriz \textbf{A} como :
\begin{equation}
||\textbf{A}||_F=(tr(\textbf{A}^T \textbf{A}))^{\frac{1}{2}}=\left(\sum_{i=1}^{N}\sum _{j=1}^{p}a_{ij}^2\right)^{\frac{1}{2}}
\end{equation}
\end{defi}

\begin{propo}
La norma de Frobenius es invariante a transformaciones ortogonales
\begin{proof}
Sea $\mathbf{U}$ una matriz ortogonal, que cumple $\mathbf{U}^T \mathbf{U}=\mathbf{U} \mathbf{U}^T=\textbf{I}$ y  sea una matriz cualquiera $\mathbf{A}$, entonces:
\begin{align*}
||\mathbf{U}  \mathbf{A}||_F^2&=tr((\mathbf{U} \mathbf{A})^T(\mathbf{U} \mathbf{A}))\\
&=tr(\mathbf{A}^T \mathbf{U}^T \mathbf{U} \mathbf{A}))\\
&=tr(\mathbf{A}^T \mathbf{A})\\
&=||\mathbf{A}||_F^2\qedhere
\end{align*}
\end{proof}
\end{propo}

\noindent Además teniendo en cuenta la definición se puede obtener la siguiente propiedad.

\begin{propo}
Dada una matriz de datos \textbf{X} de tamaño $N\times p$ entonces
\begin{equation}
||\textbf{X}||_F^2=(N-1)\sum_{j=1}^p s_{jj}^2
\end{equation}
Donde las $s_{jj}^2$ son las varianzas muestrales pertenecientes a cada una de las variables $X_1,\ldots,X_p$ observadas .
\begin{proof}
Debido a la centralidad impuesta a la matriz \textbf{X}, sabemos que la matriz de covarianzas es $\textbf{S}=\frac{1}{n-1}\textbf{X}^T \textbf{X}$ por tanto, se tiene que utilizar la definición de la norma:
\begin{align*}
||\textbf{X}||_F^2 &= tr(\textbf{X}^T\textbf{X})\\
&= (N-1) tr(\textbf{S})\\
&= (N-1) \sum_{j=1}^p s_{jj}^2 \qedhere
\end{align*}
\end{proof}
\end{propo}

\noindent Por tanto, la norma de Frobenius da una imagen del tamaño de la matriz de datos en función de la varianza total de los datos, lo que concuerda con la idea de buscar una matriz que aproxime la matriz de datos con la mínima pérdida de variación de los datos.  

\begin{defi}
Se llama matriz reducida de orden $m\leq p$ de $\textbf{X}$ y se denota como $\textbf{X}_m$, a la matriz $N\times m $ resultado de:
\begin{equation}
\textbf{X}_m=\textbf{U}_m\mathbf{D}_m\textbf{V}^T_m
\end{equation}
Donde:
\begin{itemize}
\item $\textbf{U}_m$ matriz ortogonal de tamaño $N \times m$, resultado de tomar de \textbf{U} únicamente la matriz las $m$ primeras columnas. 
\item $\mathbf{D}_m$  matriz cuadrada de tamaño $m$ diagonal con los $m$ primeros valores singulares. 
\item $\textbf{V}_m$ matriz ortogonal de tamaño $p \times m$ obtenida al tomar las $m$ primeras columnas de \textbf{V}.
\end{itemize}
Es decir, es la matriz de rango $m$ tras tomar las  $m$ primeras columnas y el resto hacerlas nulas o combinaciones lineales, en este caso.
\end{defi}

\noindent Una vez definidos los anteriores conceptos, Eckart y Young desarrollan el siguiente teorema que permite afirmar que la matriz reducida es la más próxima a la matriz de datos\cite{Eckart 1936, Golub 1987}.

\begin{teorema}[De Eckart-Young]
Sea \textbf{A} una matriz de coeficientes reales de tamaño $N\times p$ y rango $r$  entonces se cumple que :
\begin{equation}
||\textbf{A}-\textbf{B}||_F\leq ||\textbf{A}-\textbf{A}_m||_F \quad \forall \textbf{B}/ rg(\textbf{B})=m \leq r
\end{equation} 
\end{teorema}

\noindent Johnson desarrolla la demostración del teorema \cite{Johnson 1963}. 

\noindent Por tanto, la matriz reducida brinda la mejor aproximación de la matriz de datos teniendo un criterio de aproximación basado en la variación de los datos. En consecuencia se puede 

\noindent Por otro lado, se puede definir un criterio para elegir el orden de la matriz reducida $m$. Se puede entonces definir la variación acumulada de la siguiente manera \cite{Chatfield 1989}:
\begin{equation}
t_m=\dfrac{\sum_{j=1}^{m}\lambda_j} {\sum_{j=1}^{r}\lambda_j}
\end{equation}
\noindent donde los $\lambda_j$ son los valores propios de la matriz $\mathbf{S}$.

\noindent Ahora el número de componentes $m$ a considerar se puede elegir de varias maneras, ya sea representando en un gráfico el número de componentes frente a la proporción de variabilidad explicada $t_j, j=1,\ldots, p$ \cite{Jollife 1986, Peña 2002}. O se puede establecer una cota de variabilidad explicada acumulada mínima que deben representar. Normalmente se consideran cotas de un $0.8$ o $0.9$ de la variabilidad. Sin embargo, estas condiciones suelen ser arbitrarias y elegidas en cada caso a criterio del investigador y del contexto. 

%\noindent Todos estas condiciones se suelen resumir en no considerar las componentes que no aporten a la representación. 

\noindent Los valores de la matriz de cargas se pueden interpretar como las contribuciones de cada una de las variables a explicar la mayoría de la variabilidad de los datos.



