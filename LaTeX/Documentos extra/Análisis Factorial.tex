\newpage
\section{Análisis Factorial}

\noindent El análisis factorial es un conjunto de técnicas que en primera instancia fue desarrollado en el campo de la psicología ya que estos buscaban encontrar las causas que provocaban la variabilidad de los datos que recogían \cite{Vincent 1953}. 

\noindent En 1888, Galton escuchó la exposición de un articulo que relacionaba las sociedades que habían abandonado un modelo matriarcal con una estructura más compleja sociológicamente hablando. Ante esto, Galton no estuvo de acuerdo, ya que detectó un fallo en el razonamiento de su compañero y es por esta razón escribió un artículo  dando pie a los trabajos sobre el estudio de las correlaciones \cite{Galton 1889}.   

\noindent Es en 1904 cuando Spearman  viendo el trabajo de Galton, aplicó esas ideas al estudio de la inteligencia, buscando una medida objetiva de la misma \cite{Spearman 1904}. En este desarrollo, dio las primeras nociones de qué era un factor y calculó una aproximación para un modelo factorial. A partir de este momento, fue cuando el análisis factorial empezó a ser importante y varios psicólogos comenzaron a usarlo de forma habitual. 
Desde hace años el análisis factorial, al igual que otras técnicas multivariantes, ha experimentado un nuevo impulso en el desarrollo teórico y práctico debido al auge de la computación. 

\noindent El análisis factorial se puede usar tanto con fines exploratorios como confirmatorios. Es decir, se puede usar para encontrar nuevas informaciones no anticipadas o para confirmar una hipótesis planteada, por ejemplo, Pearson buscaba confirmar que había una estructura latente en la inteligencia \cite{Hair 1995}.

\noindent El objetivo específico del análisis factorial es, dado un conjunto de variables aleatorias, construir un modelo en el cual con un conjunto de nuevas variables llamadas \emph{factores comunes}, que expliquen la mayor variabilidad posible de las variables iniciales. De esta manera, se descompondrá la varianza de cada una de las variables en una parte común llamada \emph{comunalidad}  y otra específica denominada \emph{especificidad}. 

\noindent La mayor diferencia con el análisis de componentes principales en el que se busca nuevas direcciones que maximicen la varianza explicada, es que en este caso, son de interés tanto las comunalidades como las especifidades ya que nos permiten analizar si una variable da información de manera común al resto o tiene algo diferencial. 

\noindent Para empezar, empezaremos formalizando el modelo factorial, en particular el modelo multifactorial ortogonal \cite{Johnson 2007} sin pérdida de generalidad. Tras la modelización, se desarrolla el método de estimación de la matriz de covarianzas  mediante el método del factor principal \cite{Peña 2002}. Seguidamente, se darán algunos detalles sobre el problema de la no unicidad del modelo y soluciones ante ello. Y por último, se hará una introducción a la interpretación de los datos que arroja el modelo factorial. 


\noindent Si se quieren observar un estudio completo aplicando el análisis factorial léase \cite{Galindo 2015}, estudio en el que se estudia los factores de riesgo psicológico en deportistas universitarios.  También es interesante leer \cite{Diez 2002} en el que se estudia los factores que pueden llevar al consumo de drogas en la preadolescencia. Otro ejemplo sería \cite{Pages 2005}, en este caso para analizar las características de ciertas denominaciones  enológicas en el Valle del Loira, en particular, se utiliza el analísis factorial con el fin de reducir la dimensionalidad. 



\subsection{Formalización}
\noindent Supóngase un vector aleatorio $\mathbf{x}= [X_1, \ldots, X_p]$ de longitud $p$ con una distribución $N(0,\Sigma)$, centrada sin pérdida de generalidad, donde la matriz de covarianzas $\mathbf{\Sigma}$ es simétrica y definido positiva. El modelo factorial se define \cite{Chatfield 1989}:  
\begin{equation}\label{eq Fact}
X_j= \lambda_{1j}F_1+\ldots+\lambda_{mj}F_m+\psi_j U_j\quad j=1\ldots p 
\end{equation}
\noindent Donde:
\begin{itemize}
\item $\lambda_{kj}$ es la saturaciónes   de la $j$-ésima variable en el factor común $k$-ésimo. 
\item $F_k$ es el $k$-ésimo factor común
\item $\psi_j$ es la saturación  específica de la variable $X_j$ en el factor único. 
\item $U_j$ es el factor específico para la variable $X_j$.
\end{itemize}

\noindent El análisis factorial se apoya en las siguientes hipótesis \cite{Cuadras 2014}:
\begin{itemize}
\item Los factores comunes $F_k$ son variables aleatorias que siguen una distribución marginal $N(0,1)$. Además se supondrá que $Cov(F_k,F_{k'})=0, k\neq k',\\ \forall k,k'=1,\ldots, m$. Se suponen completamente independientes de los factores específicos. 

\item Los factores específicos $U_j$ son variables aleatorias con una distribución normal $N(0,1)$ no correladas. Se suponen  completamente independientes de los factores comunes. 
\end{itemize}

\noindent En consecuencia, se pueden definir las siguientes matrices: 

\begin{defi}
Llamaremos matriz de saturaciones de los factores o matriz de saturaciones, $\mathbf{\Lambda}$ de tamaño $p \times m$ a la matriz :
\begin{equation}
\mathbf{\Lambda}=\begin{pmatrix}
\lambda_{11} & \cdots & \lambda_{1 m}\\
\vdots & \ddots & \vdots\\
\lambda_{p1} & \cdots & \lambda_{pm}
\end{pmatrix}
\end{equation}
\end{defi}

\begin{defi}
Llamaremos matriz específica a la matriz diagonal $\mathbf{\Psi}$ de tamaño $p\times p$ a aquella que contiene los términos $\psi_j$ donde $j=1,\ldots , p$:
\begin{equation}
\mathbf{\Psi}=\begin{pmatrix}
    \psi_1 & 0 & \dots & 0 \\
    0 & \psi_2 & \dots & 0 \\
    \vdots & \vdots & \ddots & \vdots \\
    0 & 0 & \dots & \psi_p
\end{pmatrix}
\end{equation}
\end{defi}

\noindent \emph{Observación:} En distintas fuentes no aparece la matriz $\mathbf{\Psi}$ ya que los factores específicos $U_j, (j=1,\ldots,p)$ no se consideran variables estandarizadas no correladas, como sería el caso descrito. Por ejemplo, Mardia et.al. denotan como $\mathbf{\Psi}$ a la matriz de covarianzas del vector $\mathbf{u}$ \cite{Mardia 1979}.

\noindent De esta manera, se puede dar una versión matricial de la expresión \ref{eq Fact}:
\begin{equation}
\mathbf{x}^T=\Lambda \mathbf{f}^T+ \mathbf{\Psi}\mathbf{u}^T
\end{equation}
Ya que se consideran los vectores aleatorios $\mathbf{x}=[X_1,\ldots,X_p], \enspace\mathbf{f}=[F_1,\ldots,F_m], \\\mathbf{u}=[U_1, \ldots, U_p]$ vectores ordenados en filas. 

\begin{propo}
La varianza de la variable aleatoria $X_j$, $\sigma_j^2$, se puede descomponer de la siguiente manera:
\begin{equation}
\sigma_j^2 = \sum_{k=1}^{m}\lambda_{kj}^2+\psi_j^2\quad \forall j=1,\ldots,p
\end{equation}
\begin{proof}
\begin{align*}
\sigma_j^2&=Var(X_j)=\mathbb{E}(X_j^2)-\mathbb{E}(X_j)^2=\mathbb{E}(X_j^2)=\\ &\sum_{k=1}^{m}\lambda_{kj}^2\mathbb{E}(F_k
^2)+\sum_{k=1}^{m}\sum_{n=1,n\neq k}^{m} \lambda_{kj}\lambda_{nj}\mathbb{E}(F_k\cdot F_n)\\
&+\sum_{k=1}^{m}\lambda_{kj}\psi_j \mathbb{E}(F_k\cdot U_j)+\psi_j^2\mathbb{E}(U_j^2)
\intertext{Utilizando las hipótesis del modelo:}
 \sigma_j^2&=\sum_{k=1}^{m}\lambda_{kj}^2+\psi_j^2
\end{align*}
\end{proof}
\end{propo}
\noindent Teniendo en cuenta esta descomposición se puede definir el siguiente concepto \cite{Peña 2002}.
\begin{defi}
Se llama comunalidad de la variable $X_j$, a $h_j^2=\sum_{k=1}^m \lambda_{kj}^2 $ .
\end{defi} 

\noindent Además de la comunalidad se puede definir el concepto de especifidad o unicidad \cite{Cuadras 2014}.
\begin{defi}
Se llama especificidad o unicidad de la variable $X_j$ a $\psi_j^2,\\ \forall j=1,\ldots, p$ 
\end{defi}
\noindent Es decir, la parte de la varianza de una variable $X_j, \forall j=1,\ldots, p$ explicada por los factores comunes es la comunalidad.
\noindent Esto permite interpretar la varianza de la variable como la varianza explicada por los factores comunes por un lado y la variabilidad específica de la propia variable. Pero también se puede interpretar las covarianzas en función de las saturaciones de los factores comunes \cite{Morrison 1976, Chatfield 1989}.
\begin{propo}
La covarianza de dos variables $Cov(X_j, X_{j'})$, $\sigma_{jj'}$ cumple que 
\begin{equation}
\sigma_{jj'}=\sigma_{j'j}=\sum_{k=1}^{m}\lambda_{kj}\lambda_{kj'}
\end{equation}
\begin{proof}
\begin{align}
\sigma_{jj'} = \sum_{k=1}^{m}\lambda_{kj}\lambda_{kj'}\mathbb{E}(F_k
^2)&+\sum_{k=1}^{m}\sum_{n=1,n\neq k}^{m} \lambda_{kj}\lambda_{nj'}\mathbb{E}(F_k\cdot F_n)\\
+ \sum_{k=1}^{m}\lambda_{kj}\psi_{j'} \mathbb{E}(F_k\cdot U_{j'})&+\sum_{k=1}^{m}\lambda_{kj'}\psi_j\mathbb{E}(F_k\cdot U_j)+\psi_j\psi_{j'}\mathbb{E}(U_j\cdot U_{j'})
\intertext{De nuevo, teniendo en cuenta las hipótesis}
\sigma_{jj'}&=\sigma_{j'j}=\sum_{k=1}^{m}\lambda_{kj}\lambda_{kj'}
\end{align}
\end{proof}
\end{propo}

\noindent Y tomando las proposiciones anteriores se puede tener que la matriz $\mathbf{\Sigma}$, cumple lo siguiente:
\begin{teorema}\label{Descomposición Varianza}
La matriz $\mathbf{\Sigma}=\mathbf{\Lambda}\mathbf{\Lambda}^T+\mathbf{\Psi}^2$
\end{teorema}

\noindent Este teorema es el más importante del análisis factorial, ya que   prueba que el modelo factorial nos permite descomponer la variabilidad de un grupo de variables $X_j,j=1, \ldots , p$ en una parte común y una parte específica a cada una de las variables. 

\subsection{Obtención de la matriz de saturaciones}
\noindent Supóngase el vector aleatorio $\mathbf{x}$ como antes del cual se recogen $N$ observaciones en la matriz de datos $\mathbf{X}$ de tamaño $N\times p$ que se supondrá centrada sin pérdida de generalidad. Esto da lugar a la matriz $\mathbf{S}=\mathbf{\Lambda}\mathbf{\Lambda}^T+\mathbf{\Psi}^2$ \cite{Peña 2002}. 

\noindent Entonces el objetivo inicial es estimar la matriz de cargas $\mathbf{\Lambda}$. Para ello se puede tomar la matriz $\mathbf{S}-\mathbf{\Psi}^2$. Si de esta matriz se obtiene la descomposición espectral:
\begin{equation}
\mathbf{S}-\mathbf{\Psi}^2=\mathbf{U}\mathbf{D}\mathbf{U}^T
\end{equation}

\noindent donde la matriz $\mathbf{D}$ es una matriz diagonal de tamaño $r\times r$ con los $r=rg(\mathbf{X})$ valores propios no nulos, y la matriz $\mathbf{U}$ de tamaño $p\times r$ de manera que sus columnas son los vectores propios asociados a los valores propios no nulos. 

\noindent De esta manera, Cuadras  toma como estimador de $\mathbf{\hat{\Lambda}}=\mathbf{UD}^{\frac{1}{2}}$, ya que \\ $\mathbf{\hat{\Lambda}}\mathbf{\hat{\Lambda}}^T=\mathbf{U}\mathbf{D}\mathbf{U}^T= \mathbf{S-\Psi^2}$ \cite{Cuadras 2014}. El problema que surge aquí es que la estimación $\mathbf{\hat{\Psi}}$ no es conocida, pero se desarrollará en lo siguiente. 

\noindent De esta manera, supóngase que es conocido el número de factores, $m$. Entonces se tiene que obtener una matriz reducida de rango $m<rg(\mathbf{X})$ de la siguiente manera. 

\begin{equation}
\mathbf{\hat{\Lambda}}=(\sqrt{\theta_1}e_1,\ldots,\sqrt{\theta_m}e_m)=\mathbf{U}_m\mathbf{D}_m^{\frac{1}{2}}
\end{equation}

\noindent Donde la pareja $(\theta_k, e_k), k=1, \ldots, m$ es el vector propio $e_k$ en formato columna y $\theta_k$ (Se utiliza la letra $\theta$ para no confundirla con la saturación, $\lambda_{kj}$), su valor propio asociado. Además la matriz $\mathbf{U}_m$ es la matriz que tiene como columnas los $m$ vectores propios de la matriz $\mathbf{S}-\mathbf{\Psi}^2$ que tienen asociados los $m$ valores propios más grandes. 

\noindent En el caso de considerar que $\mathbf{\Psi}=0$, estaríamos ante el caso de la descomposición en valores y vectores propios que se utilizaba en el análisis de componentes principales. Por tanto, la matriz $\mathbf{\hat{\Lambda}}$ tendría por columnas vectores que son proporcionales a los vectores propios de $\mathbf{S}$, lo que provoca que entonces sean proporcionales a las componentes principales \cite{Rencher 2002}. 

\noindent El mayor problema en este punto es que no se conoce la matriz específica.  $\hat{\Psi}$. Se puede resolver planteando el siguiente método iterativo. En primer lugar, se introduce la siguiente notación :
\begin{itemize}
\item $\mathbf{S}_i^*=\mathbf{S}-\hat{\mathbf{\Psi}}_i^2$ es la matriz reducida en la $i$-ésima iteración del método
\item $\mathbf{\hat{\Lambda}}_i$ es la matriz de cargas obtenida en la $i$-ésima iteración del método. 
\end{itemize}
\noindent El método se inicia con $\mathbf{\hat{\Psi}}_0=0$ de tal manera que $\mathbf{S}_0^*=\mathbf{S}$. En cada iteración se efectúan las siguientes operaciones \cite{Cuadras 2014, Johnson 2007, Peña 2002}: 
\begin{enumerate}
\item Se calcula $\mathbf{S}_i^*=\mathbf{S-\hat{\Psi}}_i^2$
\item Se calcula la descomposición $\mathbf{\hat{\Lambda}}_i$ como se ha detallado antes obteniendo $\mathbf{U}_i$ y $\mathbf{D}_i$ de tamaños $p\times m $ y $m\times m$. 
\item Se obtiene la nueva estimación de la $\mathbf{\hat{\Psi}}_{i+1}^2=\mathbf{S}-\mathbf{\hat{\Lambda}}_i\mathbf{\hat{\Lambda}}_i^T$
\end{enumerate}

\noindent El proceso se para cuando no haya cambios en las matrices $\mathbf{\hat{\Psi}}_i,\mathbf{\hat{\Psi}}_{i+1}$ o cuando $\mathbf{\hat{\Lambda}}\mathbf{\hat{\Lambda}}^T+\mathbf{\Psi}^2$ sea lo más parecido posible a la matriz de covarianzas $\mathbf{S}$.

\noindent \emph{Peña D} \cite{Peña 2002} lo generaliza utilizando la función $F=tr(\mathbf{S}-\mathbf{\Lambda}\mathbf{\Lambda}^T-\mathbf{\Psi})^2$. Que en esencia es minimizar la distancia de Frobenius, implicando un razonamiento desde la optimización, ya que se busca minimizar  una función de pérdida, la distancia entre ambas matrices en este caso. 

\noindent Una vez estimadas las saturaciones de los factores, se pueden estimar las puntuaciones en los factores, que son los valores de los factores en cada una las observaciones. Es decir, las  puntuaciones de las observaciones en el espacio de los factores comunes. (Para más detalles de esta parte véase  la sección 9.5 de \cite{Johnson 2007} o la 12.7 de \cite{Peña 2002})

\subsection{Unicidad del modelo}

\noindent Un detalle en el que no se ha profundizado, es que el modelo factorial, no es único, es decir, teniendo en cuenta las suposiciones hechas anteriormente, todas las rotaciones de los factores $\mathbf{Tf}^T$, donde $\mathbf{T}$ es una matriz ortogonal de tamaño $m\times m$, cumplen las hpótesis del modelo planteado. Entonces al transformar la matriz $\mathbf{\Lambda}$ a la matriz $\mathbf{\Lambda T}^T$ se cumple que \cite{Mardia 1979}:
\begin{equation}
\mathbf{x}^T=(\mathbf{\Lambda T}^T)(\mathbf{Tf}^T)+\Psi\mathbf{u}^T
\end{equation}

\noindent Sigue manteniendo las características y suposiciones hechas en el modelo \ref{eq Fact}.

\noindent Para evitar estas indeterminaciones se suelen imponer dos restricciones \cite{Mardia 1979}:
\begin{equation}
\mathbf{\Lambda}^T \mathbf{\Psi}^{-1}\mathbf{\Lambda} \text{ ó } \mathbf{\Lambda}^T \mathbf{\Lambda} \text{ son diagonales con los valores en orden decreciente }
\end{equation}
\noindent En particular, la segunda restricción es la que se toma en el método del factor principal, ya que $\mathbf{\hat{\Lambda}}=\mathbf{UD}^{\frac{1}{2}}$, donde $\mathbf{U}$, es una matriz ortogonal, por tanto, $\mathbf{\hat{\Lambda}^T \hat{\Lambda}}=\mathbf{D}^{\frac{1}{2}}\mathbf{U}^T\mathbf{U}\mathbf{D}^{\frac{1}{2}}=\mathbf{D}$. 

\noindent Peña demuestra que la segunda restricción hace el modelo único \cite{Peña 2002}.

\begin{propo}
La segunda restricción hace que el modelo sea único . 
\begin{proof}
Supongamos que $\mathbf{\Lambda}^T\mathbf{\Lambda}$ no es diagonal. Entonces se toma la matriz ortogonal $\mathbf{T}_1$ la matriz que contiene los $p$ vectores propios de la matriz $\mathbf{\Lambda}^T\mathbf{\Lambda}$. El producto $\mathbf{T}_1^T \mathbf{\Lambda}^T \mathbf{\Lambda}\mathbf{T}_1$ es la matriz diagonal que contiene los $p$ valores propios de la matriz $\mathbf{\Lambda}^T\mathbf{\Lambda}$. 

\noindent Sea ahora una nueva matriz $\mathbf{T}_2$ ortogonal, entonces el producto $\mathbf{T}_2^T\mathbf{T}_1^T \mathbf{\Lambda}^T \mathbf{\Lambda}\mathbf{T}_1\mathbf{T}_2$, es diagonal. Y al revés, si se cumple la restricción  al rotarla deja de ser diagonal.  
\end{proof}
\end{propo}
\subsection{Interpretación del modelo factorial}

\noindent Para interpretar el modelo factorial hay que tener en cuenta los siguientes aspectos:
\begin{itemize}
\item Proporción de varianza común explicada: analizar qué parte de la variabilidad explica  cada uno de los factores, cuanto de la variabilidad total de los datos recogen los factores comunes etc\ldots Esto nos puede ayudar a la hora de determinar si el modelo obtenido es o no útil para nuestros objetivos. 
\item Especificidad de cada una de las variables: una alta especificidad implica que poca de la información de la variable es explicada por los factores comunes, es decir, que estos no recogen la información. 
\item Saturaciones de la variable en los factores: puede ocurrir que haya variables que tengan en el mismo factor menor saturación que el resto, esto es significativo ya que en este caso, la variabilidad de esa variable no es recogida por ese factor en específico.

\item Significado de los factores:  analizar una vez obtenidas las cargas, si  éstas representan una medida subyacente notable o no. Detallar el origen del significado que se le otorga en el caso de tenerlo.
\end{itemize}

\noindent Una vez interpretados los resultados, se puede llegar a los objetivos del análisis factorial, estudiar la estructura latente de los datos en términos de variabilidades comunes. 
