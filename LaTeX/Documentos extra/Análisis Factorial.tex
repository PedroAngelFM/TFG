\section{Análisis Factorial}
\noindent El Ánalisis de componentes principales no era más que una descomposición de la varianza de las variables observadas, y luego una transformación de las variables mediante transformaciones ortogonales. 

\noindent De esta manera, no se explora más allá de la varianza la relación de las variables. En el caso del Análisis Factorial el objetivo es encontrar un conjunto de variables nuevas (\emph{que pueden ser combinaciones o transformaciones no lineales }) de las variables iniciales. De tal manera que se consigan detallar variables latentes o subyacentes no medidas. 

\noindent El caso que inició este tipo de análisis fue la comprensión de la inteligencia humana donde Pearson y Spearman intentaron dimensionarla. 

\noindent A lo largo de esta sección seguiremos \emph{Peña, D.;}\cite{Peña 2002} y tomaremos ciertas definiciones de \emph{Chatfield, C. y A.J.;} \cite{Chatfield 1989}. 

\subsection{Formalización}
\noindent Según \emph{Peña, D.} \cite{Peña 2002}, la relación que establece el modelo factorial dado un conjunto de $n$ observaciones sobre $p$ variables aleatorias representadas por la matriz \textbf{X} es la siguiente:
\begin{equation}\label{Eq. Factorial}
\textbf{X}=\textbf{1}\mathbf{\mu}^T+\textbf{F}\mathbf{\Lambda}^T+\textbf{U}
\end{equation}

\noindent Donde:
\begin{itemize}

\item La matriz $\textbf{F}$ de tamaño $(n\times m)$ obtenida de realizar $n$ observaciones del vector aleatorio $\textbf{f}$ que sigue una distribución normal $\mathcal{N}_m(\mathbf{0},\textbf{I})$, a este vector aleatorio se le llama \emph{vector de factores} que son las llamadas variables latentes.  
\item $\Lambda$ es la \emph{matriz de carga} de tamaño $(p\times m)$ que contiene la información de como $\textbf{f}$ se relaciona con $\textbf{x}$.
\item La matriz $\textbf{U}$ es la matriz de perturbaciones de tamaño $(n\times p )$ que contiene las $n$ observaciones del vector aleatorio $\textbf{u}$ cuya información no está recopilada por el vector de factores. 
\end{itemize}

\noindent Por tanto, se puede expresar cada observación $\textbf{x}_0$, como la suma de $m+2$ términos de la siguiente manera :
\begin{equation}
x_{0j}=\mu_j+\lambda_{j1}f_{10}+\ldots\lambda_{jm}f_{m0}+u_{0j}\quad j=1\ldots p
\end{equation}

\noindent Para calcular la varianza teniendo en cuenta las distribuciones que se han supuesto. Entonces, se tiene que:
\begin{equation}
E((\textbf{x}-\mu)\textbf{f}^T)=\Lambda E(\textbf{f }\textbf{f}^T)+E(\textbf{u f}^T)=\Lambda
\end{equation}
\noindent Ya que se ha establecido que los factores son variables incorreladas y con matriz de covarianzas \textbf{I} y además, los factores son independientes de las perturbaciones. En esta linea, tomando $\psi$, la matriz de covarianzas del vector aleatorio de perturbaciones, se puede dar la siguiente expresión de la matriz de covarianzas de los datos:
\begin{equation}
\mathbf{\Sigma}=E[(\textbf{x}-\mu)(\textbf{x}-\mu)^T]=\Lambda E[\textbf{f f}^T]\Lambda^T+E[\textbf{u u}^T] =\Lambda\Lambda^T+\psi
\end{equation}

\noindent Se detallan en \emph{Chatfield, C y Collins, A.J. }\cite{Chatfield 1989} algunos conceptos que ayudan a simplificar la discusión 
\begin{defi}
Llamaremos \emph{comunalidad} a los efectos de los factores en cada una de las variables 
\begin{equation}
h_i^2=\sum_{j=1}^m \lambda_{ij}^2
\end{equation}
\end{defi}
\noindent Donde $\mathbf{\Lambda}=\lbrace\lambda_{ij}\rbrace$. De esta manera, se puede dar una expresión simplificada de la varianza de cada variable: $\sigma^2_i =h_i^2+\psi_i^2$. Es decir, la varianza se divide en lo que explican los factores y lo que no. 

\noindent Ahora, se plantean varios problemas, entre ellos, ¿Es único este modelo?, ¿ Cuántos factores se deben elegir?, ¿ Cuáles son los efectos de los factores y cómo se calculan ? ¿ Cómo se calculan los factores ? 

\subsection{ Unicidad del modelo }

\noindent Sea el modelo con los vectores aleatorios, $\textbf{x}=\mu+\mathbf{\Lambda} \textbf{f}+\textbf{u}$ , entonces tomando una matriz $\textbf{A}$ no singular el modelo anterior es equivalente a $\textbf{x}=\mu+\mathbf{\Lambda} \textbf{A}\textbf{A}^{-1} \textbf{f}+\textbf{u}$, entonces la matriz $\mathbf{\Lambda}'=\mathbf{\Lambda} \textbf{A}$ y $\textbf{f}'=\textbf{A}^{-1}\textbf{f}$ en este caso tenemos un modelo equivalente con factores que pueden ser correlados y con la misma precisión. 

\noindent Además si se toma una matriz de transformación ortogonal, \textbf{H} se obtiene un modelo equivalente en el que los factores son incorrelados, por tanto los modelos son invariantes por rotaciones. 

\noindent Para resolver este problema de indeterminación  se pueden dar dos restricciones:
\begin{equation}
\mathbf{\Lambda}^T\mathbf{\Lambda}=\textbf{D} 
\end{equation} 
\noindent Donde $D$ es una matriz diagonal. 

\noindent En el caso de que la matriz $\mathbf{\Lambda}^T\mathbf{\Lambda}$ no fuese diagonal, se puede tomar la transformación ortogonal $\textbf{H}\Rightarrow \textbf{H}^T=\textbf{H}^{-1}$ donde las columnas aparte de ser ortogonales, son vectores propios de $\mathbf{\Lambda}^T\mathbf{\Lambda}$ entonces, la matriz $\mathbf{\Lambda}\textbf{H}$ si cumple la condición, ya que :
\begin{equation}
(\mathbf{\Lambda}\textbf{H})^T\mathbf{\Lambda}\textbf{H}=\textbf{H}^T\mathbf{\Lambda}^T \mathbf{\Lambda}\textbf{H}=\textbf{H}^{-1}\mathbf{\Lambda}^T \mathbf{\Lambda}\textbf{H}=\textbf{D}
\end{equation}

\begin{coro}
La matriz $\textbf{H}$ es la única que hace que $\mathbf{\Lambda}$ cumpla la condición 
\end{coro}

\begin{propo}
Si la matriz $\mathbf{\Lambda}$ cumple lo anterior, $\mathbf{\Lambda}^T \mathbf{\Lambda}=\textbf{D}$, o en su defecto tomamos su transformada, entonces $\mathbf{\Lambda}$ es la matriz formada por los vectores propios de $\textbf{V}-\psi$
\begin{proof}
Es decir, en la ecuación de la varianza podemos tener que 
\begin{equation}
\begin{split}
\textbf{V}=\mathbf{\Lambda}^T\mathbf{\Lambda}+\psi\\
(\mathbf{\Sigma}-\psi)=\mathbf{\Lambda}^T\mathbf{\Lambda}\\
(\mathbf{\Sigma}-\psi)\mathbf{\Lambda}=\mathbf{\Lambda}\textbf{D}\qedhere
\end{split}
\end{equation}
\end{proof}
\end{propo}

\noindent De esta manera, se restringen, las posibles transformaciones de la matriz, quedando una única transformación a realizar.

\noindent Otra forma de restringir el modelo sería estableciendo la siguiente restricción:
\begin{equation}
\mathbf{\Lambda}^T \psi^{-1}\mathbf{\Lambda}=\textbf{D}
\end{equation}

\begin{propo}
La restricción anterior es válida y hace que el modelo sea único. 
\begin{proof}
En el caso que $\mathbf{\Lambda}^T \psi^{-1}\mathbf{\Lambda}$ no sea diagonal, tomando la matriz ortogonal $\textbf{H}$ que tenga por columnas los vectores propios de la matriz $\mathbf{\Lambda}^T \psi^{-1}\mathbf{\Lambda}$ y transformando de forma que $\mathbf{\Lambda}'=\mathbf{\Lambda}\textbf{H}$  se obtiene un modelo equivalente y que además cumple la restricción anterior y de manera análoga a la otra restricción, $\mathbf{\Lambda}^T\mathbf{\Lambda}=\textbf{D}$, se puede demostrar que es la única ya que ninguna otra resultaría en una matriz diagonal. 
\end{proof}
\end{propo}

\begin{propo}
La matriz $\psi^{-\frac{1}{2}}\mathbf{\Sigma}\psi^{-\frac{1}{2}}$ tiene como vectores propios $\psi^{-\frac{1}{2}}\mathbf{\Lambda}$ y los valores propios $\textbf{D}+\textbf{I}$
\begin{proof}
Basta con a la ecuación $\mathbf{\Sigma}=\mathbf{\Lambda}^T \mathbf{\Lambda}+\psi $ multiplicarla de la siguiente manera por $\psi^{-1}\mathbf{\Lambda}$:
\begin{align}
\mathbf{\Sigma}\psi^{-1}\mathbf{\Lambda}+\mathbf{\Lambda}&=\mathbf{\Lambda}\textbf{D}
\intertext{multiplicando por la izquierda por $\psi^{-\frac{1}{2}}$:}
\psi^{-\frac{1}{2}}\mathbf{\Sigma}\psi^{-\frac{1}{2}}\psi^{-\frac{1}{2}}\mathbf{\Lambda}&=\psi^{-\frac{1}{2}}\mathbf{\Lambda}\textbf{(D+I)}
\end{align}
\noindent Por tanto, $\psi^{-\frac{1}{2}}\mathbf{\Sigma}\psi^{-\frac{1}{2}}$ es la matriz cuyos valores propios son los elementos de $\textbf{D+I}$ y los vectores propios son las columnas de la matriz $\psi^{-\frac{1}{2}}\mathbf{\Lambda}$. 
\end{proof}
\end{propo}

\noindent Esta restricción se utiliza en el método máximo verosímil de estimación de los factores.

\noindent En estos casos lo que se quiere hallar son los términos de la matriz $\mathbf{\Lambda}$ que es un total de $pm$ y los $p$ términos de la matriz $\mathbf{\psi}$, suponiendo que se da la restricción de que $\mathbf{\Lambda}^T \psi^{-1}\mathbf{\Lambda}$, entonces hay $\frac{m(m-1)}{2}$ restricciones. Además usando la matriz de covarianzas muestrales $\textbf{S}$, obtenemos $\frac{p(p+1)}{2}$ ecuaciones de manera que si se desea que el sistema quede  determinado la condición necesaria es que se de la siguiente condición:
\begin{equation}
\begin{split}
p+pm-\dfrac{m(m-1)}{2}&\leq\dfrac{p(p+1)}{2}\\
(p-m)^2&\geq p+m
\end{split} 
\end{equation} 

\noindent Teniendo en cuenta estos conceptos, hay que desarrollar métodos para estimar los parámetros desconocidos, los más comunes son el método del Factor Principal y el método de Máxima Verosimilitud.


\subsection{Método del Factor Principal}

\noindent En el Análisis Factorial hay un principio que se debe seguir:
\begin{defi}
El \emph{Principio de Parsimonia} es aquel por el cual se toma la solución con menos factores comunes con el fin de obtener el modelo menos complejo. 
\end{defi}

\noindent En el caso del método del Factor Principal se busca lo mismo que en el caso de una Componente Principal, que este tenga una varianza máxima y que esté incorrelada. En \emph{Cuadras C.M.}\cite{Cuadras 2014} se detalla el proceso de obtención de la descomposición que detallaremos a continuación, esta es similar a la realizada en la sección de las Componentes Principales. 

\noindent Sea $\mathbf{S}$ la matriz de covarianzas muestrales, de manera que si tenemos en cuenta estimaciones de la matriz de covarianzas de las perturbaciones $\mathbf{\hat{\psi}}$, entonces: 
\begin{equation}
\mathbf{S}-\mathbf{\hat{\psi}}=\mathbf{\Lambda}^T\mathbf{\Lambda}
\end{equation}

\noindent Entonces la matriz $\textbf{S}-\psi$, es simétrica ya que la propia $\textbf{S}$ lo es y la matriz $\hat{\psi}$ es diagonal por hipótesis del modelo, \emph{Cuadras C.M.} \cite{Cuadras 2014}. Teniendo en cuenta estas consideraciones se obtiene la descomposición:
\begin{equation}
\textbf{S}-\psi=(\textbf{HG}^{\frac{1}{2}})(\textbf{HG}^{\frac{1}{2}})^T
\end{equation}

\noindent Donde $\textbf{H}$ es una matriz de tamaño $p\times p$ cuadrada y ortogonal y $\textbf{G}$ es una matriz con la siguiente distribución:
\begin{equation}
\textbf{G}=\left(
\begin{matrix}
\textbf{G}_{1 m\times m} & \mathbf{0}_{m\times(p-m)}\\
\mathbf{0}_{(p-m)\times m} & \mathbf{0}_{(p-m)\times(p-m)}
\end{matrix}
\right)
\end{equation}

\noindent Donde $m$ es el rango de la matriz $\textbf{S}-\psi$. De manera análoga al caso de las Componentes Principales podemos tener una reducción de la matriz de rango $m$ con la mínima pérdida de variabilidad \emph{(Teorema de Eckart-Young)}. Tomando $\textbf{H}_1$ la matriz ortogonal de tamaño $p \times m$ con los vectores propios con valores propios no nulos de $\textbf{G}$

\begin{propo}
\end{propo}
















