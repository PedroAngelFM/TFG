\newpage
\section{Análisis Factorial}

\noindent En primera instancia, \emph{Galton}, intentó ver como una causa común podía afectar a dos variables aleatorias o medidas \cite{Vincent 1953}. 

\noindent Fue el psicólogo inglés Charles Spearman el que tomó la idea de estudiar como una causa común podía afectar a un conjunto de variables. Es por ello que en 1904 se publica el primer artículo en el que aparece la palabra factor como la conoceremos en esta sección. Pero no se da ningún método específico para calcularlos. 

\noindent El análisis factorial es una técnica de analisis multivariante no supervisado que analiza la covarianza con el objetivo de otorgar un conjunto de nuevas variables que expliquen la mayoría de la variabilidad común de los datos llamados factores, además de calcular la variabilidad que es específica de cada una de las variables estudiadas. 

\subsection{Formalización}
\noindent Supóngase un vector aleatorio $\mathbf{x}$ de longitud $p$ con una distribución $N_p(0,\Sigma)$, centrada sin pérdida de generalidad, donde la matriz de covarianzas $\mathbf{\Sigma}$ es simétrica y definido positiva. El modelo factorial afirma lo siguiente \cite{Chatfield 1989}:  
\begin{equation}\label{eq Fact}
X_j= \lambda_{1j}F_1+\ldots+\lambda_{mj}F_m+\psi_j U_j\quad j=1\ldots p 
\end{equation}
\noindent Donde:
\begin{itemize}
\item $\lambda_{jk}$ son las \emph{cargas del factor común} $k$-ésimo de la $j$-ésima variable.
\item $F_k$ es el $k$-ésimo factor común
\item $\psi_j$ es la \emph{carga específica} de la variable $X_j$
\item $U_j$ es el \emph{factor específico} para la variable $X_j$
\end{itemize}

\noindent En este caso haremos las siguientes suposiciones\cite{Cuadras C.M. }:
\begin{itemize}
\item Los factores comunes $F_k$ son variables aleatorias que siguen una distribución marginal $N(0,1)$. Además se supondrá que $Cov(F_k,F_{k'})=0, k\neq k'$, de manera que el vector aleatorio $\mathbf{f}$ de longitud $m$ con distribución $N_m(0,\mathbf{I})$. Y son completamente independientes de los factores específicos. 

\item Los factores específicos $U_j$ son variables aleatorias con una distribución normal $N(0,1)$ no correladas, entre ellas  que forman el vector aleatorio $\mathbf{u}\sim N_p(0,\mathbf{I})$ de longitud $p$. Son completamente independientes de los factores comunes. 
\end{itemize}

\noindent De esta manera se pueden definir las siguientes matrices: 

\begin{defi}
Llamaremos \emph{matriz de cargas de los factores o matriz de cargas}, $\mathbf{\Lambda}$ de tamaño $p \times m$ que la matriz es:
\begin{equation}
\Lambda=\begin{pmatrix}
\lambda_{11} & \cdots & \lambda_{1 m}\\
\vdots & \ddots & \vdots\\
\lambda_{p1} & \cdots & \lambda_{pm}
\end{pmatrix}
\end{equation}
\end{defi}

\begin{defi}
Llamaremos matriz específica a la matriz diagonal $\mathbf{\Psi}$ de tamaño $p\times p$ a aquella que tiene los términos $\psi_j$ donde $j=1,\ldots , p$:
\begin{equation}
\mathbf{\Psi}=\begin{pmatrix}
    \psi_1 & 0 & \dots & 0 \\
    0 & \psi_2 & \dots & 0 \\
    \vdots & \vdots & \ddots & \vdots \\
    0 & 0 & \dots & \psi_p
\end{pmatrix}
\end{equation}
\end{defi}

\noindent De esta manera, se puede dar una versión matricial de la expresión \ref{eq Fact}:
\begin{equation}
\mathbf{x}^T=\Lambda \mathbf{f}^T+ \mathbf{\Psi}\mathbf{u}^T
\end{equation}
Ya que se consideran los vectores aleatorios $\mathbf{x}, \mathbf{f}, \mathbf{u}$ vectores ordenados como filas. 

\begin{propo}
La varianza de la variable aleatoria $X_j$, $\sigma_j^2$ se puede descomponer de la siguiente manera:
\begin{equation}
\sigma_j^2 = \sum_{k=1}^{m}\lambda_{kj}^2+\psi_j^2
\end{equation}
\begin{proof}
\begin{align*}
\mathbb{E}(X_j^2)&=\sigma_j^2 = \sum_{k=1}^{m}\lambda_{kj}^2\mathbb{E}(F_k
^2)+\sum_{k=1}^{m}\sum_{n=1,n\neq k}^{m} \lambda_{kj}\lambda_{nj}\mathbb{E}(F_k\cdot F_n)\\
&+\sum_{k=1}^{m}\lambda_{kj}\psi_j \mathbb{E}(F_k\cdot U_j)+\psi_j^2\mathbb{E}(U_j^2)
\intertext{Utilizando las hipótesis del modelo:}
&\ \sigma_j^2=\sum_{k=1}^{m}\lambda_{kj}^2+\psi_j^2
\end{align*}
\end{proof}
\end{propo}
\noindent Teniendo en cuenta esta descomposición se forma el siguiente concepto.
\begin{defi}
Se llama \emph{comunalidad} de la variable $X_j$, $h_j^2=\sum_{k=1}^m \lambda_{kj}^2 $ \cite{Peña 2002}
\end{defi} 
\noindent Esto permite interpretar la varianza de la variable como la varianza explicada por los factores comunes por un lado y la variabilidad específica de la propia variable. 
\begin{propo}
La covarianza de dos variables $Cov(X_j, X_j')$, $\sigma_{jj'}$ cumple que \cite{Morrison 1976, Chatfield 1989}: 
\begin{equation}
\sigma_{jj'}=\sigma_{j'j}=\sum_{k=1}^{m}\lambda_{kj}\lambda_{kj'}
\end{equation}
\begin{proof}
\begin{align}
\sigma_{jj'} &= \sum_{k=1}^{m}\lambda_{kj}\lambda_{kj'}\mathbb{E}(F_k
^2)+\sum_{k=1}^{m}\sum_{n=1,n\neq k}^{m} \lambda_{kj}\lambda_{nj'}\mathbb{E}(F_k\cdot F_n)\\
&+ \sum_{k=1}^{m}\lambda_{kj}\psi_{j'} \mathbb{E}(F_k\cdot U_{j'})+\sum_{k=1}^{m}\lambda_{kj'}\psi_j\mathbb{E}(F_k\cdot U_j)+\psi_j\psi_{j'}\mathbb{E}(U_j\cdot U_{j'})
\intertext{De nuevo, teniendo en cuenta las hipótesis}
\sigma_{jj'}=\sigma_{j'j}&=\sum_{k=1}^{m}\lambda_{kj}\lambda_{kj'}
\end{align}
\end{proof}
\end{propo}

\noindent Y tomando estas dos proposiciones se puede tener que la matriz $\mathbf{\Sigma}$, cumple lo siguiente:
\begin{coro}
La matriz $\mathbf{\Sigma}=\mathbf{\Lambda}\mathbf{\Lambda}^T+\mathbf{\Psi}$
\end{coro}

\noindent Bajo esta idea en la qu
\subsection{Estimación de la matriz de cargas}

\noindent Supóngase el vector aleatorio $x$ como antes del cual se recogen $N$ observaciones para obtener la matriz de datos $\mathbf{X}$ que se supondrá centrada sin pérdida de generalidad.

\noindent El objetivo de esta sección es estimar la matriz de cargas $\mathbf{\Lambda}$ a partir de la matriz de covarianzas. 
\noindent Para empezar, supóngase conocido el número de factores $m$ que se desea establecer en el modelo. 

\noindent 

\subsection{Unicidad del modelo}
\subsection{Interpretación del modelo factorial}