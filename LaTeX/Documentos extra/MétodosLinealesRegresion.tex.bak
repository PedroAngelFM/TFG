\section{Métodos Lineales para regresión}

\noindent El objetivo de la regresión es encontrar como un conjunto de variables respuesta se ven afectadas por otro conjunto de variables predictoras. En este caso, se estudia como una combinación lineal de las variables puede afectar a las variables respuesta. Esto se puede hallar con fines predictivos o con el fin de analizar cómo cada una de las variables predictivas afectan a las variables respuesta. 

\noindent Para ello, se tendrá en cuenta que las observaciones de la variable respuesta siguen la siguiente relación:
\begin{equation}
Y=f(\textbf{x})+\varepsilon
\end{equation}
\noindent Donde $\varepsilon$ es una variable aleatoria con $\mathbb{E}(\varepsilon)=0$ y $Var(\varepsilon)=\sigma^2$, habitualmente $\varepsilon \sim N(0,\sigma^2)$. Además supóngase que el vector de datos de entrada sigue una distribución normal multivariante $N_p(\mathbf{\mu},\mathbf{\Sigma})$ donde $\mathbf{\Sigma }$ es una matriz semidefinido positiva y simétrica \cite{Chatfield 1989}.


\noindent En este tipo de métodos se supone que $f$ es una función lineal de las variables de entrada del vector aleatorio $\textbf{x}=[X_1\ldots X_p]$. De esta manera, se tiene que:  
\begin{equation}
f(\textbf{x})=\beta_0+\sum_{j=1}^p X_j\beta_j
\end{equation}

\begin{defi}
Se llaman \emph{parámetros de regresión} al vector $\beta=[\beta_0, \beta_1, \ldots \beta_p]$ de tamaño $(p+1)$ con los coeficientes necesarios para la regresión. 
\end{defi} 

\noindent Añadiendo al vector $\mathbf{x}$ una nueva variable aleatoria $X_0$ que sea constantemente 1, se puede dar la siguiente expresión matricial de la función $f$:
\begin{equation}
f(\mathbf{x})= \mathbf{x}\beta^T
\end{equation}

\noindent De esta manera, tomando la matriz de datos de entrada de tamaño $N\times (p+1)$ entonces se puede generar un vector de predicciones de tamaño $N$ de la siguiente manera:
\begin{equation}
\mathbf{\hat{y}}=\mathbf{X}\beta^T  
\end{equation}  

\noindent Si en particular se quiere hacer una predicción para una nueva observación  $\mathbf{x}_0$ basta con calcular basta con calcular $f(\textbf{x}_0).$

\noindent Una vez establecida la notación y los supuestos que tomaremos de ahora en adelante, hay que estimar los parámetros de regresión, para ello utilizaremos distintos métodos como el método de los mínimos cuadrados o el de Máxima verosimilitud, a parte de esto, veremos las características inferenciales de los estimadores obtenidos y su interpretación geométrica. 

\subsection{Ajuste de los parámetros por mínimos cuadrados}
\noindent Sea una matriz de datos $\textbf{X}$ de tamaño $N\times (p+1)$ resultado de hacer $N$ observaciones de $p$ variables aleatorias y añadir a la primera componente de cada observación un 1. Sea también un vector de respuestas $\textbf{y}=[y_1,\ldots y_N]$ de tamaño $N$, resultado de observar la variable respuesta $Y$. 

\noindent Ahora podemos sea el vector de parámetros de regresión $\beta$ de tamaño $p+1$ definido como antes, entonces podemos definir el concepto de función de pérdida

\begin{defi}
Se llama \emph{función de pérdida} \cite{Hastie 2001} a una función $L(\textbf{y},\mathbf{\hat{y}})$ monótona creciente de la diferencia entre la predicción y el valor real de una variable
\end{defi}

\noindent Aunque esta definición sea más propia del aprendizaje automático, también se usa para el método de los mínimos cuadrados \cite{Abdi 2007}. 

\begin{defi}
Se llama \emph{error de predicción cuadrático} entre una variable observada, $y$ y la predicción obtenida por un cierto modelo $\hat{y}$ a la expresión $(y-\hat{y})^2$
\end{defi}

\noindent Tomando la suma de los errores cuadrados y minimizando se puede obtener una estimación  del vector de parámetros $\beta$. Utilizando las expresiones anteriores:
\begin{equation}
RSS(\beta)=\sum_{i=1}^n(y_i-\hat{y}_i)^2 = \sum_{i=1}^n(y_i-\textbf{x}_i\beta^T)^2=(\textbf{y}-\textbf{X}\beta^T)(\textbf{y}-\textbf{X}\beta^T)^T\\
\end{equation}

\noindent Para obtener el mínimo se debe hallar los puntos estacionarios, lo que implica calcular la derivada de la suma de residuos cuadrados respecto del vector $\beta$, y como esta es una forma cuadrática general respecto de $\beta$, entonces \cite{Morrison 1976}:
\begin{equation}
\dfrac{\partial RSS(\beta)}{\partial \beta}= -2\mathbf{X}^T(\mathbf{y}-\mathbf{X}\beta^T)^T
\end{equation}

\noindent La segunda derivada respecto del vector de parámetro, entonces la segunda derivada es la matriz $2\mathbf{X}^T\mathbf{X}$ que es semidefinida positiva, ya que todos los valores propios son positivos o nulos. Entonces los $\beta$ en los que $-2\mathbf{X}^T(\mathbf{y}-\mathbf{X}\beta^T)^T=0$ son mínimos.  

\noindent Asumiendo que $\textbf{X}$ es una matriz de rango máximo, de lo contrario, habría dos columnas o más que son combinación lineal la una de la otra, la matriz $\textbf{X}^T\textbf{X}$ es definida positiva, por tanto la solución de la siguiente ecuación:
\begin{equation}
\textbf{X}^T(\textbf{y}-\textbf{X}\beta)=0
\end{equation}

\noindent Si además, $\textbf{X}^T\textbf{X}$ es una matriz invertible, entonces tiene solución única y es :
\begin{equation}
\hat{\beta}=(\textbf{X}^T\textbf{X})^{-1}\textbf{X}^T\textbf{y}
\end{equation}

\noindent Para el cálculo de este parámetro se ha utilizado lo que se conoce por el método de los mínimos cuadrados. Aún así hay otros métodos de obtención como el geométrico o el de máxima verosimilitud. 

\noindent Con la siguiente proposición se puede ver la equivalencia entre los mínimos cuadrados y el de máxima verosimilitud:

\begin{propo}
Esta estimación de los parámetros $\beta$ por mínimos cuadrados es equivalente a la estimación de estos mediante el método de máxima verosimilitud.
\begin{proof}
Sea una muestra de tamaño $N$, $y_i, i=1,\ldots N$ de una variable aleatoria $Y$, de manera que su función de probabilidad, $\mathbb{P}_\theta(y)$ depende de los parámetros $\theta$. Entonces el método de máxima verosimilitud busca maximizar la siguiente función.
\begin{equation}
L(\theta)=\sum_{i=1}^n log( \mathbb{P}_{\theta}(y_i))
\end{equation}

\noindent Suponiendo que la variable respuesta cumple como antes que  $Y=f_\theta (\textbf{x})+\varepsilon$, donde $\varepsilon \thicksim N(0,\sigma^2)$, provoca que si se suponen conocidos a priori el parámetro $\theta$ y el vector aleatorio $\textbf{x}$ entonces :
\begin{equation}
\mathbb{P}(Y|\textbf{x},\theta)=N(f_\theta(\textbf{x}), \sigma^2)
\end{equation}

\noindent Teniendo esto en cuenta, la función $L(\theta)$ tiene la siguiente expresión:
\begin{equation}
L(\theta)=-\dfrac{n}{2}log(2\pi)-n log(\sigma)-\dfrac{1}{2\sigma^2}\sum_{i=1}^n (y_i-f_\theta (x_i))^2
\end{equation}
Por tanto, teniendo en cuenta que el último término es $RSS(\theta)$, entonces maximizar $L(\theta)$ es equivalente a minimizar $RSS(\theta)$
\end{proof}
\end{propo}
\begin{coro}
Las estimaciones para cualquier $f_{\theta}$ obtenidas por el método de los mínimos cuadrados son equivalentes a las del método de máxima verosimilitud.  
\end{coro}
\noindent Por tanto, los valores predichos $\hat{\textbf{y}}$ se calculan de la siguiente manera:
\begin{equation}
\hat{\textbf{y}}=\textbf{X}\hat{\beta}=\textbf{X}^T(\textbf{X}^T\textbf{X})^{-1}\textbf{X}^T\textbf{y}
\end{equation}

\noindent Si se toma el espacio $\mathbb{R}^n$ generado por las columnas de la matriz de datos $\textbf{x}_0,\textbf{x}_1,\ldots \textbf{x}_p$, entonces $\hat{\textbf{y}}$ es tal que $\textbf{y}-\hat{\textbf{y}}$ es ortogonal a ese subespacio. 

\subsection*{Inferencias Estadísticas sobre $\hat{\beta}$}

\noindent En esta parte se estudian las propiedades de los estimadores obtenidos mediante el método de los mínimos cuadrados. 

\noindent Para empezar supóngase como antes que el vector $\mathbf{X}$ sigue una distribución normal multivariante con $p$ variables independientes entre ellas. Además supóngase que  la matriz de datos cumple que $\mathbf{X^T X}$ es semidefinido positiva y conocida. 

\begin{propo}
El estimador $\hat{\beta}$ es un estimador insesgado \cite{Greene 2008}
\begin{proof}
Teniendo en cuenta las anteriores hipótesis anteriores, incluyendo que la variable respuesta es $\mathbf{y}=\mathbf{X\beta^T}+\varepsilon$ donde $\varepsilon \sim N(0,\sigma^2)$. 

\noindent De esta manera, se obtiene que $\hat{\beta}=(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T(\mathbf{X}\beta^T+\varepsilon)=\beta+(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T \varepsilon$

\noindent Entonces, se tiene que conocida $\mathbf{X} \Rightarrow \mathbb{E}(\hat{\beta}|\mathbf{X})=\mathbb{E}(\beta+(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T \varepsilon|\mathbf{X})=\beta +\mathbb{E}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T \varepsilon|X)=\beta$ debido a la distribución de $\varepsilon$

\noindent Por tanto como $\mathbb{E}(\mathbb{E}(\hat{\beta}|\mathbf{X}))=\mathbb{E}(\beta)=\beta$
\end{proof}
\end{propo}

\begin{propo}
La varianza del estimador $Var(\hat{\beta})= \sigma^2 (\mathbf{X}^T \mathbf{X})^{-1}$\cite{Hastie 2001},\cite{Greene 2008}
\begin{proof}
\begin{equation}
\begin{split}
Var(\hat{\beta})&=\mathbb{E}((\mathbf{X}^T \mathbf{X})^{-1}\mathbf{X}^T \varepsilon \varepsilon\mathbf{X}(\mathbf{X}^T \mathbf{X})^{-1}|X)\\
&=\mathbb{E}(\varepsilon^2) (\mathbf{X}^T \mathbf{X})^{-1}(\mathbf{X}^T \mathbf{X})(\mathbf{X}^T \mathbf{X})^{-1} \\
&=\sigma^2 (\mathbf{X}^T \mathbf{X})^{-1}
\end{split}
\end{equation}
\end{proof}
\end{propo}

\noindent Por último, tenemos que el parámetro de estimadores $\hat{\beta}= \beta+(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T \varepsilon$, es una combinación afín de $\varepsilon$ una variable con distribución por tanto, tendremos que cada uno de los componentes del vector $\hat{\beta},\hat{\beta}_j\sim N(\beta_j, \sigma^2(\mathbf{X^TX}_{jj}^{-1})) j=1\ldots p$ donde $(\mathbf{X^TX}_{jj}^{-1})$ es el j-ésimo elemento de la diagonal de la matriz $(\mathbf{X}^T\mathbf{X})^{-1}$.

\noindent Esto nos permite establecer estadígrafos de contraste. Por ejemplo se puede plantear la siguiente hipótesis nula:
\begin{equation}
H_0: \beta_j=0
\end{equation}
\noindent De esta manera, se puede 
\subsection{Regresión Múltiple mediante Ortogonalización sucesiva }

\noindent Otra manera de afrontar el problema de ajuste es hacerlo de una manera geométrica, se busca un subespacio que se ajuste lo mejor posible a un vector de respuestas, en particular, se buscan proyecciones ortogonales del vector respuesta o del subespacio respuesta en el subespacio de los datos. 


\noindent Teniendo en cuenta que el vector de respuestas $\textbf{y}$ sigue el modelo $\textbf{y}=\textbf{X}\beta+\varepsilon$, entonces, $\hat{\textbf{y}}=\textbf{X}\hat{\beta}$ pertenece al subespacio generado por las columnas de la matriz de datos $\textbf{X}$. En caso de que $\textbf{y}\in Col(\textbf{X})$, se tiene que existe un $\beta$ tal que $\textbf{y}=\textbf{X}\beta$. 

\begin{defi}
Se llama vector residuo $\textbf{r}$ al vector cuyas componentes son $r_i=y_i-\textbf{x}_i\hat{\beta}$, es decir, $\textbf{r}=\textbf{y}-\textbf{X}\hat{\beta}$
\end{defi}

\noindent Para que la distancia entre el vector respuesta y el espacio de datos sea lo menor posible el vector residuo debe ser ortogonal al espacio que generan las columnas de la matriz de datos y esto provoca lo siguiente:
\begin{equation}
\begin{split}
r \perp \textbf{X}\Rightarrow \textbf{X}^T \textbf{r}&=\textbf{X}^T (\textbf{y}-\textbf{X}\hat{\beta})=0\\
\textbf{X}^T\textbf{y}-\textbf{X}^T\textbf{X}\hat{\beta}&=0\Rightarrow \hat{\beta}= (\textbf{X}^T\textbf{X})^{-1} \textbf{X}^T \textbf{y}
\end{split} 
\end{equation}

\noindent Por tanto, el vector de estimadores $\hat{\beta}$ es equivalente al cálculo por el método de los mínimos cuadrados. 
Por lo tanto, teniendo en cuenta que $\hat{\textbf{y}}=\textbf{X}(\textbf{X}^T\textbf{X})^{-1} \textbf{X}^T \textbf{y}$. De esta manera, lo que se está haciendo es tomar la proyección de $\textbf{y}$ sobre el espacio $Col(\textbf{X})$. 

\noindent Teniendo en cuenta esto en \textit{Hastie et.al.} \cite{Hastie 2001} define un algoritmo para calcular el vector de parámetros $\beta$. Teniendo en cuenta esto y que $\textbf{y}=(y_1,\ldots,y_n)^T$ el vector  de observaciones de la variable respuesta y $\textbf{x}=(x_1,\ldots x_n)^T$ el vector de observaciones de una única variable predictora. Entonces se puede expresar el vector de parámetros con el producto escalar: 
\begin{equation}
\hat{\beta}=\dfrac{\langle \textbf{x},\textbf{y} \rangle}{\langle \textbf{x},\textbf{x} \rangle}
\end{equation}

\noindent En el caso general, en el que se tienen $p$ variables predictoras, esto cambia, si todas las columnas de la matriz fuesen ortogonales entre sí cada componente se calcularía de la siguiente manera:
\begin{equation}
\hat{\beta}_j=\dfrac{\langle \textbf{x}_j,\textbf{y}\rangle}{\langle \textbf{x}_j,\textbf{x}_j\rangle}
\end{equation}

\noindent En caso contrario, se deben ortogonalizar los datos, para ello, se utiliza el procedimiento de ortogonalización de Gram-Schmidt



%\noindent Se obtiene el mismo estimador de los parámetros $\beta$ que si se hubiera utilizado el Método de los Mínimos Cuadrados. De esta manera podemos establecer que 
%\begin{equation}
%\hat{\beta}=\dfrac{\sum_{i=1}^n\textbf{x}_i y_i}{\sum_{i=1}^n\textbf{x}_i^2} 
%\end{equation}
%\noindent De esta manera, se puede establecer una manera recursiva para encontrar cada uno de los parámetros $\beta_j$ distinto. Mediante proyecciones sucesivas se obtiene lo siguiente:




\subsection{Regresión sobre varias variables}
\noindent Hasta ahora, se ha considerado una única variable aleatoria respuesta, sea \textbf{y} el vector aleatorio de variables respuesta $\textbf{y}^T=[Y_1,\ldots, Y_K]$ y un vector aleatorio de variables de entrada $\textbf{x}^T=[X_0, X_1,\ldots, X_p]$. Se puede establecer el siguiente modelo análogo:
\begin{align}
Y_k=\beta_{0k}+\sum_{j=1}^p& X_j\beta_{jk}+\varepsilon_k=f_k(\textbf{x})+\varepsilon_k\\
\intertext{Recogidas n muestras tendremos la siguiente expresión matricial: }
\textbf{Y}=&\textbf{X}\textbf{B}+\textbf{E}
\end{align}

\noindent Dentro de esa expresión matricial se tiene que: 
\begin{itemize}
\item $\textbf{Y}$ es la matriz de tamaño $n\times K$ que contiene los valores observados de las variables respuesta
\item $\textbf{X}$ matriz de datos habitual de tamaño $n \times p+1$ 
\item $\textbf{B}$ matriz de tamaño $ p+1 \times K$ que contiene los parámetros de la regresión 
\item $\textbf{E}$ es la matriz de tamaño $ n \times K$ que contiene los errores. 
\end{itemize}

\noindent El proceso de ajuste, en el caso de que los errores $\varepsilon^T=[\varepsilon_1,\ldots \varepsilon_K]$ no estén correlados, de la matriz de parámetros es análogo al de una sola variable respuesta de tal manera que minimizando el error cuadrático acumulado se obtiene $\hat{\textbf{B}}=(\textbf{X}^T\textbf{X})^{-1}\textbf{X}^T\textbf{Y}$. En caso de que los errores tengan una matriz de covarianzas conocida, $\Sigma$, entonces es necesario hacer la siguiente modificación en el $RSS$:
\begin{equation}
RSS(\textbf{B},\Sigma)=\sum_{i=1}^n(y_i-f(\textbf{x}_i))^T \Sigma^{-1} (y_i-f(\textbf{x}_i))
\end{equation}

\noindent En la última expresión se usa la \textit{distancia de Mahalanobis}.
\begin{defi}
Según \textit{Cuadras C.M.}\cite{Cuadras 2014} la \textit{distancia de Mahalanobis} entre dos observaciones $\textbf{x}_i, \textbf{x}_j$ extraídas de una misma población con matriz de covarianzas $\Sigma$ se define de la siguiente manera: 
\begin{equation}
d_M(\textbf{x}_i, \textbf{x}_j)=\sqrt{(\textbf{x}_i- \textbf{x}_j)^T \Sigma^{-1}(\textbf{x}_i-\textbf{x}_j)}
\end{equation}
\end{defi}

\noindent En el caso anterior, $(y_i-f(\textbf{x}_i))=\varepsilon_i$ y se está haciendo la distancia de Mahalanobis respecto de su media, el 0. 


\subsection{Selección de subconjuntos y métodos penalizados}

\noindent Anteriormente, se ha detallado un estadígrafo que permitía hacer un contraste sobre la cantidad de variables a considerar en la regresión. 
Esta reducción de características a considerar permite hacer mucho más interpretable el modelo obtenido durante todo el proceso de ajuste y en algunos casos incluso aumentar la precisión del modelo ya que puede ocurrir que se eliminen variables que introduzcan ruido en el modelo. 

\noindent Se podría seguir un método exhaustivo que calcule cada uno de los subconjuntos posibles para cada número de variables que creamos necesarias y calcular el error cuadrático acumulado de cada uno de los subconjuntos posibles. Pero este método es de una complejidad computacional alta. 

\noindent Otra posibilidad sería utilizar el estadígrafo de contraste $F$ definido en la Ecuación \eqref{ec.F} empezando con una sola variable e ir añadiendo cada variable que mejore el ajuste. También se puede hacer al revés, empezando con el modelo con todas las variables e ir reduciendo la cantidad de estas. 

\noindent El problema de estos métodos de selección de variables es que es un proceso discreto, una variable es o no considerada para el modelo siguiente y esto puede generar sobreajuste o infraajuste, no habiendo un término medio. 

\noindent Para ello, existen los métodos penalizados o de encogimiento, que añaden un término de penalización para que los parámetros $\beta$ no sean muy grandes, en el caso del ajuste por mínimos cuadrados del modelo lineal. 

\begin{equation}
PRSS(\beta)=\sum_{i=1}^n(\textbf{y}_i-\textbf{x}_i^T\beta)^2+\lambda\sum_{j=1}^p\beta_j^2
\end{equation}

\noindent El último término hace que parámetros $\beta_j$ grandes sean considerados perjudiciales, y el parámetro $\lambda$ es una forma de regular cuanta importancia tiene dicha penalización, cuanto mayor sea, este provocará un encogimiento mayor de los parámetros. 
De esta manera, se tiene una forma continua de considerar los pesos no eliminando en de manera total las variables o haciendo el $\beta_j$ correspondiente cercano a 0. 















