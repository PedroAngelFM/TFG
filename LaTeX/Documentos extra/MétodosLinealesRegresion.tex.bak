\section{Métodos lineales para regresión}

\noindent El objetivo de la regresión lineal es estudiar como un conjunto de variables respuesta continuas están relacionadas con otro conjunto de variables predictoras. En el caso de la regresión lineal, se estudia como una combinación lineal de las variables predictoras puede relacionarse con las variables respuesta. Esto se puede hallar con fines predictivos o con el fin de analizar cómo cada una de las variables predictivas afectan a las variables respuesta \cite{Johnson 2007}. 

\noindent De manera concreta, sea $\mathbf{x}=[X_1,\ldots X_p]$ el vector de variables predictoras. Además supóngase hasta que se diga lo contrario que únicamente hay una variable respuesta $Y$ continua. 

\noindent Teniendo en cuenta lo anterior, se define el modelo de regresión lineal como la siguiente relación estocástica \cite{Hastie 2001, Johnson 2007}:
\begin{equation}
Y=f(\textbf{x})+\varepsilon
\end{equation}
\noindent donde se supone que  $\varepsilon$ es una variable aleatoria con $\mathbb{E}(\varepsilon)=0$ y $Var(\varepsilon)=\sigma^2$. Habitualmente se asume a mayores que  $\varepsilon \sim N(0,\sigma^2)$. Además, supóngase que el vector de variables de entrada sigue una distribución normal multivariante $N_p(\mathbf{\mu},\mathbf{\Sigma})$ donde $\mathbf{\Sigma }$ es una matriz semidefinida positiva y simétrica \cite{Chatfield 1989}. 
\newpage
\noindent En el caso de la regresión lineal, se supondrá que $f$ es una función lineal de las variables de entrada del vector aleatorio $\textbf{x}=[X_1\ldots X_p]$. De esta manera, se tiene que:  
\begin{equation}
f(\textbf{x})=\beta_0+\sum_{j=1}^p X_j\beta_j
\end{equation}

\begin{defi}
Se llaman parámetros de regresión a las componentes del vector columna $\beta=[\beta_0, \beta_1, \ldots \beta_p]^T$ de tamaño $(p+1)$ con los coeficientes necesarios para la regresión. 
\end{defi} 

\noindent Añadiendo al vector $\mathbf{x}$ una nueva variable aleatoria $X_0$ que sea constantemente 1, se puede dar la siguiente expresión matricial de la función $f$:
\begin{equation}
f(\mathbf{x})= \mathbf{x}\beta
\end{equation}

\noindent De esta manera, tomando una matriz de datos de $N$ realizaciones, entonces se puede generar un vector de predicciones de tamaño $N$ de la siguiente manera:
\begin{equation}
\mathbf{\hat{y}}=\mathbf{X}\beta 
\end{equation}  

\noindent Si en particular, se quiere hacer una predicción para una nueva observación  $\mathbf{x}_0$ basta con calcular  $f(\textbf{x}_0).$

\noindent Una vez establecida la notación y los supuestos que se tomarán de ahora en adelante, hay que estimar los parámetros de regresión, $\beta$.  Para ello, se utilizan  distintos métodos como el método de los mínimos cuadrados o el de máxima verosimilitud. A parte de esto, se detallarán las características inferenciales de los estimadores obtenidos y su interpretación geométrica. 

\subsection{Métodos de ajuste}

\noindent Sea una matriz de datos $\textbf{X}$ de tamaño $N\times (p+1)$ resultado de hacer $N$ observaciones de $p$ variables aleatorias y añadir a la primera componente de cada observación un 1. Sea también un vector de respuestas $\textbf{y}=[y_1,\ldots y_N]^T$ de tamaño $N$, resultado de observar la variable respuesta $Y$. 

\noindent Sea el vector de parámetros de regresión $\beta$ de tamaño $p+1$ definido con anterioridad. Para poder hallar una estimación del mismo se debe definir el concepto de función de pérdida \cite{Hastie 2001}. 

\begin{defi}
Se llama función de pérdida $L(Y, \hat{Y})$ a aquella que mide las diferencia  entre el valor real de una variable y su valor predicho por un cierto modelo.  
\end{defi}

\noindent Aunque el término sea más utilizado en el marco del aprendizaje automático \cite{James 2013}, el concepto de error cuadrático o la función de pérdida cuadrática es utilizado en el ajuste de los parámetros de regresión, ya que esta pérdida, al minimizarse, es equivalente al método de los mínimos cuadrados. En particular, el error cuadrático se define de la siguiente manera \cite{Abdi 2007}:

\begin{defi}
Se llama error de predicción cuadrático entre una variable observable $Y$, y la predicción obtenida por un cierto modelo $\hat{Y}$ a la expresión $(Y-\hat{Y})^2$.
\end{defi}

\noindent Tomando la suma de los errores al cuadrado cometidos  en todas las observaciones se obtiene la suma de residuos al cuadrado, \emph{RSS}, y minimizando se puede obtener una estimación del vector de parámetros $\beta$. En particular, si se utilizan las expresiones matriciales  anteriores:
\begin{equation}
RSS(\beta)=\sum_{i=1}^N(y_i-\hat{y}_i)^2 = \sum_{i=1}^N(y_i-\textbf{x}_i\beta)^2=(\textbf{y}-\textbf{X}\beta)(\textbf{y}-\textbf{X}\beta)^T\\
\end{equation}

\noindent Para obtener el mínimo se debe hallar los puntos estacionarios, lo que implica calcular la derivada de la suma de residuos cuadrados respecto del vector $\beta$, y como ésta es una forma cuadrática general respecto de $\beta$:

\begin{equation}
\dfrac{\partial RSS(\beta)}{\partial \beta}= -2\mathbf{X}^T(\mathbf{y}-\mathbf{X}\beta)
\end{equation}

\noindent La segunda derivada respecto del vector de parámetros es $2\mathbf{X}^T\mathbf{X}$, entonces es  semidefinida positiva, ya que todos los valores propios son positivos o nulos. Entonces los $\beta$ en los que $-2\mathbf{X}^T(\mathbf{y}-\mathbf{X}\beta)=0$ son mínimos (para el desarrollo de los anteriores cálculos véase \cite{Morrison 1976}).  

\noindent Asumiendo que $\textbf{X}$ es una matriz de rango máximo $p+1$, y por tanto que $\mathbf{X}^T \mathbf{X}$ es invertible, la siguiente expresión tiene una única solución: 
\begin{equation}
\textbf{X}^T(\textbf{y}-\textbf{X}\beta)=0
\end{equation}

\noindent Esa solución es :
\begin{equation}
\hat{\beta}=(\textbf{X}^T\textbf{X})^{-1}\textbf{X}^T\textbf{y}
\end{equation}

\noindent Para el cálculo de este vector de parámetros se ha utilizado el método de los mínimos cuadrados. Aún así, hay otros métodos de estimación del vector $\hat{\beta}$, como el geométrico o el de máxima verosimilitud. 

\noindent Con la siguiente proposición se puede ver la equivalencia entre el método de los mínimos cuadrados y  de máxima verosimilitud \cite{Hastie 2001}:

\begin{propo}
Esta estimación de los parámetros $\beta$ por mínimos cuadrados es equivalente a la estimación de estos mediante el método de máxima verosimilitud.
\begin{proof}
Sea un conjunto de $N$ observaciones independientes $y_i, $ donde $i=1,\ldots N$ de una variable aleatoria $Y$. Sea su  función de probabilidad  $\mathbb{P}_\theta(y)$ que depende de unos ciertos parámetros $\theta$. Entonces el método de máxima verosimilitud busca maximizar la siguiente función:
\begin{equation}
L(\theta)=\sum_{i=1}^N log( \mathbb{P}_{\theta} (y_i)),
\end{equation}

\noindent que es el logaritmo de la  verosimilitud de la muestra. Suponiendo que la variable respuesta cumple como antes que  $Y=f_\theta (\textbf{x})+\varepsilon$, en el caso de la regresión lineal tendríamos que $f_\theta(\mathbf{x})=\mathbf{x}\theta$, es decir, $\theta$ sería el vector de parámetros de regresión y $\varepsilon \thicksim N(0,\sigma^2)$. En consecuencia, si se suponen conocidos a priori el vector de parámetros $\theta$ y el vector aleatorio $\textbf{x}$ entonces :
\begin{equation}
Y\sim N(f_\theta(\textbf{x}), \sigma^2)
\end{equation}

\noindent Teniendo esto en cuenta, la función de verosimilitud respecto del estimador $L(\theta)$ tiene la siguiente expresión:
\begin{equation}
L(\theta)=-\dfrac{N}{2}log(2\pi)-N log(\sigma)-\dfrac{1}{2\sigma^2}\sum_{i=1}^N (y_i-f_\theta(x_i))^2
\end{equation}

\noindent En particular, la derivada de la función de la verosimilitud queda:
\begin{equation}
\dfrac{\partial L(\theta)}{\partial \theta}=-\dfrac{1}{2\sigma^2}\dfrac{\partial RSS(\theta)}{\partial \theta}
\end{equation}
\noindent Por tanto, es equivalente minimizar el error cuadrático y maximizar la verosimilitud respecto al vector de parámetros. Ambos métodos de estimación obtendrán los mismos resultados. 
\end{proof}
\end{propo}

\noindent Continuando con las estimaciones calculadas por el método de los mínimos cuadrados, los valores predichos obtenidos de las observaciones recogidas en la matriz de datos $\hat{\textbf{y}}$ se calculan de la siguiente manera:
\begin{equation}
\hat{\textbf{y}}=\textbf{X}\hat{\beta}=\textbf{X}(\textbf{X}^T\textbf{X})^{-1}\textbf{X}^T\textbf{y}
\end{equation}

\noindent De esta manera, se está calculando la predicción de la respuesta $\hat{y}_i$ para cada una de las observaciones $\mathbf{x}_i, \forall i =1, \ldots N$. 

\noindent Otra forma de considerar el problema del ajuste sería desde el punto de vista geométrico.

\noindent En lo sucesivo se considera a no ser que se diga lo contrario que $N>p+1$ y que la matriz de datos $\mathbf{X}$ es de rango $r$.


\subsection{Inferencias estadísticas sobre $\hat{\beta}$}

\noindent En esta parte se estudian las propiedades de los estimadores obtenidos mediante el método de los mínimos cuadrados. Tras ello, se formarán estadígrafos para realizar los contrastes de hipótesis necesarios.

\noindent Conocer la distribución de los estimadores permite realizar contrastes sobre los distintos parámetros. De manera habitual, se plantea la hipótesis nula $H_0: \beta_j=0 $. Esta hipótesis busca comprobar si la variable $X_j$ es importante en el modelo lineal. En caso de que se acepte la hipótesis esa variable puede ser eliminada del modelo. 

\noindent Teniendo en cuenta esto,  hay ocasiones en las que se desea comprobar si un subconjunto de variables es más significativo estadísticamente que otro, de manera que se pueda eliminar variables que no aporten información en pos de la sencillez del modelo y su posterior interpretación. 


%\noindent Hay que tener en cuenta los siguientes supuestos, en primer lugar, el vector \\ $\mathbf{x}\sim N_p(\mu, \mathbf{\Sigma})$. De este vector tomamos $N$ observaciones independientes obteniendo la matriz de datos $\mathbf{X}$, que a no ser que se diga lo contrario, se supondrá conocida. Además, supóngase que el vector de $N$ observaciones de la variable respuesta sigue  modelo $\mathbf{y}= \mathbf{X \beta}+\varepsilon$ donde $\varepsilon \sim N(0,\sigma^2\mathbf{I}_N)$, es un vector de longitud $N$ en el que cada componente es una normal $N(0,\sigma^2)$  independientes entre sí. Esto nos permite dar la siguiente proposición \cite{Cuadras 2014}
\newpage
\noindent Hay que tener en cuenta los siguientes supuestos:
\begin{itemize}
\item El vector  $\mathbf{x}\sim N_p(\mu, \mathbf{\Sigma})$. De este vector tomamos $N$ observaciones independientes obteniendo la matriz de datos $\mathbf{X}$, que a no ser que se diga lo contrario, se supondrá conocida.
\item El vector de $N$ observaciones de la variable respuesta sigue el modelo \\$\mathbf{y}= \mathbf{X \beta}+\varepsilon$ donde $\varepsilon \sim N(0,\sigma^2\mathbf{I}_N)$, es un vector de longitud $N$ en el que cada componente es una normal $N(0,\sigma^2)$  independientes entre sí.
\end{itemize}

Esto nos permite dar la siguiente proposición \cite{Cuadras 2014}.
\begin{propo}
El vector aleatorio formado por $N$ observaciones de la variable respuesta conocida la matriz de datos y el vector de parámetros $\beta$ sigue una distribución
$\mathbf{y}\sim N(\mathbf{X\beta}, \sigma^2\mathbf{I}_N)$, donde $\mathbf{I}_N$ es la matriz identidad de tamaño $N \times N$.  
\begin{proof}
Basta con comprobar que:
\begin{equation}
\mathbb{E}(\mathbf{y})=\mathbb{E}(\mathbf{X\beta})+\mathbb{E}(\varepsilon)=\mathbb{E}(\mathbf{X\beta})=\mathbf{X\beta}
\end{equation}
Además :
\begin{align}
\mathbb{E}((\mathbf{X\beta}+\varepsilon)(\mathbf{X\beta}+\varepsilon)^T)&=\mathbf{X\beta}\mathbf{\beta}^T \mathbf{X}^T+ Var(\varepsilon
)\\
\intertext{Entonces, la varianza}
Var(\mathbf{y})=\mathbf{X\beta}\mathbf{\beta}^T \mathbf{X}^T+ Var(\varepsilon
)&-\mathbf{X\beta}\mathbf{\beta}^T \mathbf{X}^T=Var(\varepsilon)=\sigma^2\mathbf{I}_N
\end{align}

\noindent Y se concluye que el vector $\mathbf{y}\sim N(\mathbf{X\beta}, \sigma^2\mathbf{I}_N)$, ya que el vector $\varepsilon\sim N(0,\sigma^2 \mathbf{I}_N)$\\ por hipótesis. 
\end{proof}
\end{propo}

\noindent Añadamos a las suposiciones  que la matriz $\mathbf{X}$ es de rango máximo y por tanto, $\mathbf{X}^T\mathbf{X}$ es semidefinida positiva.
A continuación se detallarán las cualidades inferenciales del vector $\hat{\beta}$, como que es insesgado \cite{Greene 2008}, su varianza \cite{Greene 2008,Hastie 2001,Johnson 2007} y su distribución \cite{Hastie 2001}. 

\begin{propo}
El estimador $\hat{\beta}$ es un estimador insesgado.
\begin{proof}  
\noindent \\ Teniendo en cuenta las hipótesis anteriores se obtiene que:
\begin{equation}
\mathbb{E}(\hat{\beta})=\mathbb{E}((\textbf{X}^T\textbf{X})^{-1}\textbf{X}^T \mathbf{y})=(\textbf{X}^T\textbf{X})^{-1}\textbf{X}^T\mathbb{E}(\mathbf{y})
\end{equation}

\noindent En particular, se sabe que $\mathbb{E}(\mathbf{y})=\mathbf{X}\beta$. Entonces se obtiene lo siguiente:
\begin{equation}
\mathbb{E}(\hat{\beta})=(\textbf{X}^T\textbf{X})^{-1}\textbf{X}^T\mathbb{E}(\mathbf{y})=(\textbf{X}^T\textbf{X})^{-1}\textbf{X}^T \mathbf{X}\beta=\beta
\end{equation}
\end{proof}
\end{propo}
\begin{propo}
La varianza del estimador:
\begin{equation}
Var(\hat{\beta})= \sigma^2 (\mathbf{X}^T \mathbf{X})^{-1}
\end{equation}
\begin{proof}
\begin{equation}
\begin{split}
Var(\hat{\beta})&= \mathbb{E}(\hat{\beta}^T\hat{\beta})\\
&=\mathbb{E}((\mathbf{X}^T \mathbf{X})^{-1}\mathbf{X}^T \mathbf{y}^T\mathbf{y}\mathbf{X}(\mathbf{X}^T \mathbf{X})^{-1}|\mathbf{X})\\
&=\mathbb{E}(\mathbf{y}^T\mathbf{y}) (\mathbf{X}^T \mathbf{X})^{-1}(\mathbf{X}^T \mathbf{X})(\mathbf{X}^T \mathbf{X})^{-1} \\
&=Var(\mathbf{y})(\mathbf{X}^T \mathbf{X})^{-1}\\&=\sigma^2 (\mathbf{X}^T \mathbf{X})^{-1}
\end{split}
\end{equation}
\noindent Por tanto, el vector de estimaciones de los parámetros, denotado como $\hat{\beta}$, se calcula como una combinación lineal del vector de errores $\varepsilon$. Cada componente del vector $\hat{\beta}$, denotado como $\hat{\beta}_j,\quad j=1, \ldots, p$, sigue una distribución normal. Específicamente, $\hat{\beta}_j$ sigue una distribución normal con media $\beta_j$ y varianza $\sigma^2(\mathbf{X}^T\mathbf{X}_{jj}^{-1})$. Aquí, $\beta_j$ representa el valor verdadero del parámetro, $\sigma^2$ es la varianza y $(\mathbf{X}^T\mathbf{X}_{jj}^{-1})$ es el elemento de la diagonal correspondiente a la posición $j$ en la matriz $(\mathbf{X}^T\mathbf{X})^{-1}$.
\end{proof}
\end{propo}



\noindent Si el parámetro $\sigma^2$ fuera conocido, entonces se puede construir un estadígrafo de contraste para cada parámetro de la siguiente manera: 
\begin{equation}
z_j=\dfrac{\hat{\beta}_j}{\sigma \sqrt{(\mathbf{X}^T\mathbf{X}_{jj}^{-1})}}\sim N(0,1)\quad\forall j=1,\ldots, p.
\end{equation}
\noindent donde $(\mathbf{X}^T\mathbf{X}_{jj}^{-1})$ se define como antes. 

\noindent Para poder hacer los estadígrafos de contraste se debe construir primero un estimador de esta varianza, ya que habitualmente no es conocida. Para ello Hastie et. al. y Cuadras proponen el siguiente estimador de la varianza, suponiendo que unicamente hay una variable respuesta \cite{Cuadras 2014, Hastie 2001}: 
\begin{equation}
\hat{\sigma}^2=\dfrac{1}{N-p-1}\sum_{i=1}^N (y_i-\hat{y}_i)^2 
\end{equation} 

\noindent Pero antes de detallar las propiedades del estimador definido hay que explicar el siguiente concepto geométrico \cite{Cuadras 2014}. 
\begin{defi}
Se llama  $C_{r}(\mathbf{X})$ al subespacio lineal de $\mathbb{R}^{N}$ generado por las columnas linealmente independientes de la matriz $\mathbf{X}$ .
\end{defi}

\noindent En el caso de la regresión lineal que estamos describiendo se cumple una importante propiedad. 
\begin{propo}\label{prop ort}
El vector $\hat{\mathbf{\varepsilon}}=\mathbf{y}-\mathbf{X\hat{\beta}}$ es ortogonal al subespacio $C_{r}(\mathbf{X})$.
\end{propo}

\noindent Una vez definido lo anterior, se pueden establecer las propiedades del estimador descrito:
\newpage
\begin{propo}
El estimador $\hat{\sigma}^2$ cumple lo siguiente :
\begin{enumerate}
\item $\mathbb{E}(\hat{\sigma} ^2)=\sigma^2$ (Es un estimador insesgado de la varianza)
\item  $\hat{\sigma}^2 \sim \frac{\sigma^2}{N-p-1}\chi_{N-p-1}^2 $
\end{enumerate}
\begin{proof}
\noindent 
\begin{enumerate}
\item El término $RSS(\hat{\beta})=\sum_{i=1}^N (y_i-\hat{y}_i)^2$, se puede expresar como un producto escalar, tomando el vector residuo $\hat{\mathbf{\varepsilon}}= \mathbf{y}-\mathbf{X \hat{\beta}}=\mathbf{y}-\mathbf{\hat{y}}$, se puede ver que es $RSS(\hat{\beta})=\hat{\mathbf{\varepsilon}}^T\hat{\mathbf{\varepsilon}}$. 

\noindent Sabiendo que $\mathbf{\hat{\varepsilon}}\in \mathbb{R}^N$, consideremos una nueva base del espacio,\\ $\lbrace t_1,\ldots t_{p+1},t_{p+2},\ldots, t_N \rbrace$, de tal manera que los $p+1$ primeros son una base de $C_{p+1}(\mathbf{X})$ y que sean ortonormales entre si, entonces, se puede tomar la matriz de cambio de base $\mathbf{T}$, es una matriz ortogonal, de manera que 
\\$\mathbf{T}^T\mathbf{T}=\mathbf{TT}^T=\mathbf{I}$. 

\noindent Entonces, el vector $\mathbf{T\hat{\varepsilon}}=(\overbrace{0\ldots 0}^{p+1},\mathbf{•}'_{p+2},\ldots \mathbf{e}'_{N})^T$ por la proposición \ref{prop ort}, y se cumple que $\mathbb{E}(\mathbf{\hat{\varepsilon}})=\mathbb{E}(\mathbf{T\hat{\varepsilon}})=0$, por la distribución que tiene el vector $\varepsilon$ además $\mathbf{E}((\mathbf{e}'_i)^2)=\sigma^2, i=p+2\ldots N $, es decir, cada $\mathbf{e}_i\sim N(0,\sigma^2)$ ya que para los anteriores es 0 y por último: 
\begin{equation}\label{eq:suma-residuos}
\mathbb{E}(\mathbf{\hat{\varepsilon}}^T\mathbf{\hat{\varepsilon}})=\mathbb{E}(\mathbf{\hat{\varepsilon}}^T\mathbf{T}^T\mathbf{T}\mathbf{\hat{\varepsilon}})=\sum_{p+2}^N \mathbf{(e'_i)^2}=(N-p-1) \sigma^2
\end{equation}

\noindent Por tanto:
\begin{equation}
\mathbb{E}(\hat{\sigma}^2)= \dfrac{1}{N-p-1}\mathbb{E}(RSS(\hat{\beta}))=\sigma^2
\end{equation}

\noindent Se concluye entonces que el estimador $\hat{\sigma}^2$, es insesgado. 
\item \noindent Y para terminar, se puede ver en la ecuación \ref{eq:suma-residuos}  que el estimador es la suma de $N-p-1$ variables que siguen cada una, una distribución normal estándar, multiplicadas por $\sigma^2$ y dividas entre $N-p-1$, por tanto, tenemos que además.
\begin{equation}
\hat{\sigma}^2\sim \dfrac{\sigma^2}{N-p-1}\chi^2_{N-p-1}
\end{equation}
\end{enumerate}
\end{proof}
\end{propo}

\noindent \emph{Observación: } en el caso de que la matriz de datos $\mathbf{X}$ sea de rango $r<p+1$ se debe construir un estimador distinto, para el cual se puede seguir el mismo razonamiento:
\begin{equation}
\hat{\sigma}^2=\dfrac{1}{N-r}\sum_{i=1}^{N}(y_i-\hat{y}_i)^2
\end{equation}

\noindent Por tanto, ahora ya podemos establecer, conocidas las distribuciones de los estimadores construidos que \cite{Greene 2008,Hastie 2001}:
\begin{equation}
t_j=\dfrac{\hat{\beta}_j}{\hat{\sigma}\sqrt{(\mathbf{X}^T\mathbf{X})^{-1}_{jj}}}, j=1,\ldots ,p+1 \sim t_{N-p-1}
\end{equation}

\noindent Este estadígrafo permite hacer contrastes en los que se compobará si una variable es estadísticamente significativa para el modelo. Basta con plantear la hipótesis nula $H_0: \beta_j=0$. 

\noindent Ahora, sea $p_1+1$ el número del conjunto más grande de parámetros o de variables a considerar y $RSS_1$ su error cuadrático respectivo, sean igualmente $RSS_0$ y $p_0+1$ para el segundo conjunto o subconjunto menor que sea subconjunto del anterior, si además contamos que :
\begin{equation}\label{eq: RSS distribucion}
\frac{RSS(\hat{\beta})}{N-p-1}=\frac{1}{N-p-1}\sum_{i=1}^N(y_i-\hat{y}_i)^2\sim \sigma^2 \chi^2_{N-p-1}
\end{equation}

\noindent Se puede definir el estadígrafo: 
\begin{equation}\label{ec.F}
F=\dfrac{\dfrac{(RSS_0-RSS_1)}{p_1-p_0}}{\dfrac{RSS_1}{N-p_1-1}} 
\end{equation}

\noindent Que cumple: 
\begin{propo}
El estadígrafo $F\thicksim F_{(p_1-p_0),(N-p_1-1)}$
\begin{proof}
Teniendo en cuenta lo anterior, se puede comprobar que:
\begin{equation}
F\sim \dfrac{\chi^2_{p_1-p_0}}{\chi^2_{N-p_1-1}}=F_{(p_1-p_0),(N-p_1-1)}
\end{equation}
\noindent Basta tener en cuenta la ecuación \ref{eq: RSS distribucion}. Entonces $RSS_0-RSS_1$ es una suma de $p_1-p_0$ cuadrados de variables que siguen una distribución normal estándar multiplicada por $\sigma^2$. 
\end{proof}
\end{propo}

\noindent Este estadígrafo se referencia en secciones sucesivas con el objetivo de reducir las variables involucradas en la regresión.

\noindent En resumen, podemos hacer contrastes sobre la importancia o  de una única variable o de un conjunto de variables. Así como sobre la varianza del modelo en general. 

\subsection{Regresión multivariante}
\noindent Hasta ahora, se ha considerado una única variable aleatoria respuesta. Sea \textbf{y} el vector aleatorio de variables respuesta $\textbf{y}=[Y_1,\ldots, Y_K]$ y un vector aleatorio de variables de entrada $\textbf{x}=[X_0, X_1,\ldots, X_p]$. Se puede establecer el siguiente modelo análogo, donde:
\begin{align}
Y_k=\beta_{0k}+\sum_{j=1}^p X_j\beta_{jk}+\varepsilon_k =& f_k(\textbf{x})+\varepsilon_k, \quad k=1,\ldots K \\
\intertext{siendo el término $\varepsilon_k \sim N(0, \sigma_k^2)$, el error inevitable para la variable $Y_k, k=1\ldots K$.
De esta manera, si se toman $N$ observaciones se puede considerar las matrices de datos de respuesta y de entrada y obtener la siguiente expresión: }
\mathbf{Y} =& \mathbf{XB+E}
\end{align}

\noindent Dentro de esa expresión matricial se tiene que: 
\begin{itemize}
\item $\textbf{Y}$ es la matriz de tamaño $N \times K$ que contiene los valores observados de las variables respuesta.
\item $\textbf{X}$ es la matriz de datos de las variables explicativas o independientes de tamaño $N \times (p+1)$. 
\item $\textbf{B}$ es una matriz de tamaño $ (p+1) \times K$ que contiene los parámetros de la regresión.
\item $\textbf{E}$ es la matriz de tamaño $ N \times K$ que contiene las errores cometidos en cada uno de las variables respuesta. 
\end{itemize}

\noindent En el caso de que los errores $\varepsilon^T=[\varepsilon_1,\ldots \varepsilon_K]$ no estén correlados, el proceso de ajuste de la matriz de parámetros es análogo al de una sola variable respuesta. De tal manera que, minimizando el error cuadrático acumulado se obtiene $\hat{\textbf{B}}=(\textbf{X}^T\textbf{X})^{-1}\textbf{X}^T\textbf{Y}$. En caso de que los errores tengan una matriz de covarianzas conocida, $\mathbf{\Sigma}$, entonces es necesario hacer la siguiente modificación en el $RSS$:
\begin{equation}
RSS(\textbf{B},\mathbf{\Sigma})=\sum_{i=1}^N(y_i-f(\textbf{x}_i))^T \mathbf{\Sigma}^{-1} (y_i-f(\textbf{x}_i))
\end{equation}

\noindent En la última expresión se usa la \textit{distancia de Mahalanobis}.
\begin{defi}\label{Mahalanobis}
La \textit{distancia de Mahalanobis} entre dos observaciones $\textbf{x}_i, \textbf{x}_j$ extraídas de una misma población con matriz de covarianzas $\mathbf{\Sigma}$ se define de la siguiente manera\cite{Cuadras 2014}: 
\begin{equation}
d_M(\textbf{x}_i, \textbf{x}_j)=\sqrt{(\textbf{x}_i- \textbf{x}_j)^T \mathbf{\Sigma}^{-1}(\textbf{x}_i-\textbf{x}_j)}
\end{equation}
\end{defi}

\subsection*{Selección de subconjuntos y métodos penalizados}

\noindent Anteriormente, se ha detallado un estadígrafo que permitía hacer un contraste sobre la cantidad de variables a considerar en la regresión. 
Esta reducción de características a considerar permite hacer mucho más interpretable el modelo obtenido durante todo el proceso de ajuste. En algunos casos,  incluso aumentar la precisión del modelo ya que puede ocurrir que se eliminen variables que introduzcan ruido en el modelo. 

\noindent Se podría seguir un método exhaustivo que calcule cada uno de los subconjuntos posibles para cada número de variables que creamos necesarias y calcular el error cuadrático acumulado de cada uno de los subconjuntos posibles. Pero este método es de una complejidad computacional alta. 

\noindent Otra posibilidad sería utilizar el estadígrafo de contraste $F$ definido en la ecuación \eqref{ec.F} empezando con una sola variable e ir añadiendo solo aquellas variables que mejoren el ajuste. También se puede hacer al revés, empezando con el modelo con todas las variables e ir reduciendo la cantidad de estas. A estos algoritmos se les llaman algoritmos voraces los cuales buscan una solución óptima en cada paso y no en global. 

\noindent El problema de estos métodos de selección de variables es que es un proceso discreto, una variable es o no considerada para el modelo siguiente y esto puede generar sobreajuste o infraajuste, no habiendo un término medio. 

\noindent Para ello, existen los métodos penalizados o de encogimiento, que añaden un término de penalización para que los parámetros $\beta$ no sean muy grandes \cite{Hastie 2001}.

\begin{defi} 
Llamamos errores cuadrados acumulados penalizados, \emph{PRSS}, a la suma de los cuadrados de los errores cometidos con el modelo lineal que usa el  vector de parámetros $\beta$, añadiendo un término regulador $\lambda >0$: 
\begin{equation}
PRSS(\beta)=\sum_{i=1}^n(\textbf{y}_i-\textbf{x}_i\beta)^2+\lambda\sum_{j=1}^p\beta_j^2
\end{equation}
\end{defi}
\noindent El último término hace que parámetros $\beta_j$ grandes sean considerados perjudiciales. El parámetro $\lambda$ es una forma de regular cuanta importancia tiene dicha penalización. Cuanto mayor sea, mayor encogimiento de los parámetros provocará. 
De esta manera, se tiene una forma continua de considerar los pesos sin eliminar de manera total las variables.














