\section{Introducción}
\noindent Sea un caso en el que tenemos unas variables aleatoria \textit{inputs} que influyen o determinan otras variables \textit{outputs}. De estas variables, tomamos observaciones conjuntas de manera que a la hora de predecir las variables respuesta a partir de las variables de entrada. Una vez recogidas estas muestras se planteará un modelo \textit{(Hay varias opciones como se desarrollará a lo largo de este capítulo)}.
\noindent Sea \textbf{x} el vector aleatorio de longitud $p$ formado por las variables de entrada, sea a su vez las variables respuesta $Y_k$. De esta manera el objetivo de los métodos supervisados es conseguir una función $f$ que consiga que \begin{equation}
Y=f(\textbf{x})+\varepsilon
\end{equation}
En otras palabras, conocidas las variables de entrada de una nueva observación poder predecir sus variables de respuesta con la mayor precisión posible. 
\subsection{Mínimos cuadrados y K-vecinos más cercanos}
\noindent Supóngase que tenemos una relación lineal entre las variables de entrada \textbf{x} y la variable de salida de manera que el predictor $\hat{Y}$ tiene la siguiente expresión:
\begin{equation}
\hat{Y}=\hat{\beta}_0+\sum_{j=1}^p \hat{\beta}_j X_j
\end{equation}
\noindent Esta expresión se puede simplificar añadiendo una variable $X_0$ constante 1. Por tanto, tenemos que:
\begin{equation}
\hat{Y}=\textbf{x}^T\hat{\beta}\quad \text{ donde } \hat{\beta}^T=[\hat{\beta}_0\ldots \hat{\beta}_p ]
\end{equation}

\noindent El modelo mostrado se puede generalizar al caso de que se tengan multiples variables de salida. Pero hay que tener en cuenta que las parejas de elementos $(\textbf{x},\hat{Y})$ forman en este caso lineal un hiperplano afín

\noindent Para este y muchos otros casos el modelo ha de ajustarse a los datos recogidos para las $p$ variables entrada y las variables respuesta se tengan en ese momento . Para ello se puede definir la suma residual de cuadrados. 
\begin{equation}
RSS(\beta)=\sum_{i=1}^N(y_i-\textbf{x}_i^T\beta)^2 
\end{equation} 

\noindent En este caso, se considera que la distancia entre los puntos es euclideana, es decir, tomamos como medida del error la suma de las distancias habituales del espacio $\mathbb{R}$ entre las predicciones y los valores reales. Aún así, se puede tomar otras medidas como por ejemplo, el valor absoluto en $\mathbb{R}$.\\
El problema con esta última, es que no es diferenciable y lo que estamos buscando es minimizar $RSS(\beta)$ en función de los paramétros $\beta$.

\noindent La expresión de $RSS(\beta)$ en forma matricial teniendo en cuenta que $\textbf{X}$ sea la matriz de observaciones de tamaño $(n\times p+1)$ que tiene la primera columna constante 1 y $\textbf{y}$ es el vector columna o matriz que contiene las observaciones de la o las variables respuestas  es la siguiente: 
\begin{equation}
RSS(\beta)=(\textbf{y}-\textbf{X}^T\beta)^T(\textbf{y}-\textbf{X}^T\beta)
\end{equation}
Que al derivarla respecto de $\beta$ e igualarlo a $0$ se obtiene que:
\begin{equation}
\textbf{X}^T(\textbf{y}-\textbf{X}\beta)=0
\end{equation}
Que tiene solución única si la matriz $\textbf{X}^T\textbf{X}$ es invertible resultando en que $\hat{\beta}=(\textbf{X}^T\textbf{X})^{-1}\textbf{X}^T\textbf{y}$.

\noindent Otra forma de crear predictores es mediante los k-vecinos más cercanos que toma como predicción de una nueva observación el valor medio de las variables respuesta de las k observaciones más cercanas a la nueva muestra. 
\begin{equation}
\hat{Y}(x)=\dfrac{1}{k}\sum_{\textbf{x}_i\in N_k(x)}y_i
\end{equation}
\noindent En estos casos se necesita definir una distancia sobre el conjunto de las observaciones para poder establecer los $N_k(x)$ que son los entornos. 

\noindent Estos dos acercamientos tienen problemas a la hora de las suposiciones que se hacen. El primero asume que la estructura de los datos es lineal de manera global y el segundo asume que es constante localmente. 

\noindent Además el método de los k vecinos sufre de lo que se llama la \textit{maldición de la dimensionalidad}. Este suceso consiste en que a medida que aumentamos la dimensión, en este caso la cantidad de variables aleatorias a observar, la densidad de los datos es cada vez menor, en particular es proporcional a $n^{\frac{1}{p}}$, donde $p$ es el número de variables y $n$ el número de observaciones. Por ejemplo, si una muestra densa en $1$ dimensión tiene $n=100$, para mantener dicha densidad en $p=10$ se necesitarían $n = 100^{10}$. 

\subsection{Decisión Estadística}
\noindent Sea un vector aleatorio real de entrada $\textbf{x}\in\mathbb{R}^p$, una variable de salida $Y\in\mathbb{R}$ y sea la probabilidad conjunta $\mathbb{P}(Y,\textbf{x})$ de ambas, entonces se busca una función $f(\textbf{x})$ para predecir $Y$. La búsqueda de esta función requiere una forma de saber como de correcta son esas predicciones. Para ello, se definen las \textit{funciones de pérdida}, la más utilizada es el error cuadrático $L(Y,f(\textbf{x}))=(Y-f(\textbf{x}))^2$
\begin{defi}
Teniendo en cuenta lo anterior, se define el \textit{error de predicción esperado} como la esperanza de la nueva variable que define la función de pérdida.
\begin{equation}
\begin{split}
EPE(f)& = E(Y-f(\textbf{x}))^2\\
&= \int [y-f(\textbf{x})]^2\mathbb{P}(dx,dy)
\end{split}
\end{equation}
\end{defi}

\noindent Para ajustarlo a la situación de predecir el valor de $Y$ para nuevos valores observados de $\textbf{x}$, se puede condicionar respecto del valor de $\textbf{x}$ obteniendo la siguiente expresión:
\begin{equation}
 EPE(f) = E_{\textbf{x}} E_{Y|\textbf{x}}([Y-f(\textbf{x})]^2|\textbf{x})
\end{equation} 
Esta expresión otorga según Hastie \textit{et.al} \cite{Hastie 2001} un criterio para elegir la $f$. Basta con tomar $f$ de manera que: 
\begin{equation}
 f(x)=\text{argmin}_c E_{Y|\textbf{x}}([Y-c]^2|\textbf{x}=x)
\end{equation}

\noindent Por tanto, el que minimiza en el caso de la pérdida establecida anteriormente es $f(x)=E(Y|\textbf{x}=x)$. 
\begin{defi}
\noindent Se llama \textit{función de regresión} a la función $f(x)=E(Y|\textbf{x}=x)$
\end{defi}

\emph{Aquí podría hacer ejemplos el k- vecinos y el lineal}

\noindent Sea el caso de una variable de salida categórica $G$ que toma $k$ valores de manera que el predictor $\hat{G}$ tambien toma esos valores, entonces la única modificación que debemos hacer es la de la función de pérdida que pasa a ser una matriz $\textbf{L}$ de tamaño $k\times k $ donde $L_{i,j}$ es la penalización por categorizar como $\mathcal{G}_j$ algo que en realidad es $\mathcal{G}_i$. Entonces tenemos que \textit{el error de predicción esperado} es: 
\begin{equation}
EPE = E[L(G,\hat{G})]= E_{\textbf{x}}\sum_{i=1}^k L[\mathcal{G}_k, \hat{G}(\textbf{x})]\mathbb{P}(\mathcal{G}_k|\textbf{x})
\end{equation} 
Esta definición otorga un criterio análogo para establecer el predictor $\hat{G}(x)$, que si se utiliza la pérdida 0-1, es decir, aquella que otorga una penalización de 1, se obtiene el \textit{clasificador bayesiano}:
\begin{defi}
Se llama \textit{clasificador bayesiano} al predictor 
\begin{equation}
\hat{G}(x)=\mathcal{G}_k \text{ si } \mathbb{P}(\mathcal{G}_k|\textbf{x}=x)=\max_{g\in \mathcal{G}}\mathbb{P}(g|\textbf{x}=x)
\end{equation}

\noindent Es decir, se otorga la categoría que más probabilidad de ser tiene conociendo el valor de $\textbf{x}$
\end{defi}

\noindent Una característica que se puede intuir es que el problema de encontrar el predictor se puede traducir como un problema de aproximación de funciones en $\mathbb{R}^p$ conocidos $n$ puntos. 

\subsection{Selección del modelo, Sesgo y Varianza}

Una vez que hemos ajustado el modelo a los puntos hay que comprobar su rendimiento, esto se puede hacer teniendo un conjunto de entrenamiento y otro de testing. El conjunto de entrenamiento nos servirá para optimizar los parámetros y el de testing del cual conocemos las x y las y nos permite evaluar su capacidad de generalización. 

\noindent Sea el modelo $Y=f(\textbf{x})+\varepsilon$ donde $E(\varepsilon)=0$ y $Var(\varepsilon)=\sigma^2$. Entonces podemos hacer la siguiente descomposición del error de generalización  esperado en $x_0$, no perteneciente al conjunto de entrenamiento:
\begin{equation}
EPE(x_0)=E(Y-\hat{f}(x_0)|\textbf{x}=x_0)^2=\sigma^2+sesgo^2(\hat{f}(x_0))+Var_{\mathcal{T}}(\hat{f}(x_0))
\end{equation}

\noindent De esta manera, se puede reducir el sesgo y la varianza, pero hay un error que es irreducible que es $\sigma^2$ y los otros dos términos son el resultado de calcular el Error Cuadrático Medio. En términos generales cuanto más complejo es el modelo se tiene 













































































































\newpage