\section{Análisis de Clusters}

\noindent Según \emph{Jain,A.K. y Richard, C.D; }\cite{Jain 1988} el análisis de clusters tiene como objetivo conseguir una clasificación de las observaciones en subconjuntos que tengan sentido de acuerdo al contexto de la investigación. El clustering es un tipo de clasificación de los datos que tiene las siguientes características:
\begin{itemize}
\item \textit{Exclusividad: }Una observación no puede pertenecer a más de un cluster a la vez. 
\item \textit{Es intrínseco: }No hay ninguna etiqueta que permita clasificar las observaciones, son las características de las propias muestras las que servirán como diferenciadores. Lo que hace que sea un método \textit{no supervisado}.
\end{itemize}

\noindent Una vez clarificado las características esenciales del clustering, este se puede dar de manera \emph{jerárquica} o \emph{particional}.

\begin{defi}
Se dice que un método de clustering es \emph{jerárquico} si genera una secuencia de particiones del espacio de observaciones las cuales están anidadas 
\end{defi}

\begin{defi}
Un método de clustering \emph{particional} genera una única partición del espacio. 
\end{defi}

\noindent Por otro lado, los algoritmos se pueden clasificar según  sean \textit{aglomerativos}, en los que se empieza teniendo cada una de las observaciones y se van formando los clusters hasta tener un solo cluster con todas las muestras. Por el contrario, los \textit{algoritmos   divisivos}, se empieza con un solo cluster y se van haciendo subconjuntos del mismo.  

\noindent Otra clasificación puede venir dada por el modo en el que se consideran las  variables, si es \textit{Monotético}, en cada paso se centra en una variable, mientras que si es \textit{Politético} se utilizan todas las variables a la vez.

\noindent También hay que distinguir métodos que usan como fundamento el álgebra matricial, centrándose en los datos que nos puede dar la matriz de distancias entre observaciones o en la Teoría de Grafos. 

\noindent En primer lugar, es necesario definir los conceptos principales de esta sección que es un clustering y un cluster:

\begin{defi}
Se llama \textit{clustering} a la partición que provoca la relación $\mathcal{R}$ y se definen los \textit{clusters} como las clases de equivalencia de dicha relación, $\lbrace c_i\rbrace$. 
\end{defi}

\noindent Por tanto, el análisis de clusters es equivalente a establecer una relación de equivalencia entre las observaciones.  

\subsection{Algoritmos Jerárquicos}
\noindent Los algoritmos jerárquicos son aquellos que utilizan como entrada no la matriz de los datos per sé, en su lugar, estas utilizan como entrada la matriz de distancias o similaridad.

\noindent Uno de los inconvenientes mayores de este tipo de algoritmos es como calcular la matriz de similaridades o distancias. En el caso de que las variables sean numéricas y continuas entonces se puede utilizar la distancia euclídea entre elementos de $\mathbb{R}^p$. Además hay que tener en cuenta si se va a estandarizar ya que en el caso de que si se estandarice se dará el mismo peso a todas las variables, dando igual su escala, en cambio si no se estandariza, la escala provoca que las características con mayores valores tengan mayor peso en las distancias. Por otro lado, si se tienen variables binarias o categóricas hay que tener en cuenta cómo medir sus distancias, todo esto se soluciona con las siguiente definición y consideraciones:

\begin{defi}
Llamaremos similaridad entre dos muestras $i,h$ según la variable $j$ a una función $s_{jih}$ la cual cumpla que:
\begin{itemize}
\item La similaridad de una muestra consigo misma es igual a la unidad $s_{jii}=1 \forall j,i$
\item $0\leq s_{jih} \leq 1\quad \forall j,i,h $
\item Simetría: $s_{jih}=s_{jhi}$
\end{itemize}
\end{defi}

\noindent Una vez considerado esto, se puede crear un coeficiente de similaridad entre dos mediante el coeficiente de \emph{Gower}:
\begin{equation}
s_{ih}=\dfrac{\sum_{j=1}^{p}w_{jih}s_{jih}}{\sum_{j=1}^{p}w_{jih}}
\end{equation}
\noindent Donde la variable $w_{jih}$ puede ser 0 ó 1, dependiendo de si se quiere tomar o no dicha variable o no en la comparación de dichas muestras. 

\noindent Lo más habitual para formalizar las similaridades es tomar lo siguiente:
\begin{itemize}
\item Para continuas se puede tomar el valor $s_{jih}=1-\frac{|x_{ji}-x_{hi}|}{rango(X_j)}$
\item En cambio para variables binarias en el caso de que coincida el atributo $x_{ji}=x_{jh}$ la similaridad es 1 y 0 en el caso contrario. También se puede decir que si hay presencia del atributo y coinciden es 1 y en caso contrario 0. 
\end{itemize}

\noindent Una vez decidida la forma de decir de inicialmente dar la matriz de distancias o similaridades, se da el algoritmo aglomerativo de generación de la jerarquía:
\begin{itemize}
\item Se comienza con un \emph{clustering} que tiene $n$ \emph{clusters} con una observación cada uno y se calcula la matriz de distancias
\item Se toman los elementos que más cercanos están y se forman una nueva clase.
\item Se sustituyen los dos elementos anteriores por la clase y se calcula la distancia entre la clase y el resto de observaciones de acuerdo con uno de los criterios preestablecidos. 
\item Repetir los dos pasos anteriores hasta obtener una única clase. 
\end{itemize}

\noindent Dependiendo de la forma que se de unir los grupos se obtendrá un algoritmo u otro de clustering:
\begin{itemize}
\item Si $f(x,y)=min(x,y)$ es decir, se toma como distancia entre la clase y el resto de observaciones como el mínimo de las que se den, a este algoritmo se le llama \textit{single linkage}.
\item  Si $f(x,y)=max(x,y)$ es análogo al anterior, pero tomando la distancia máxima, a este método se le llama \textit{complete linkage}.
\item  Si $f(x,y)=\dfrac{x+y}{2}$ se hace la media de las distancias de los dos grupos antes de unirlos, a este método se le llama \textit{average linkage}.
\end{itemize}

\noindent Este tipo de métodos aporta una jerarquía en la agrupación de los datos en función de su similitud, de manera que a distintos grados de similitud se pueden establecer distintas reparticiones del espacio de observaciones. Además esto se puede representar fácilmente con un dendograma o diagrama de árboles otorgando una manera sencilla de establecer un criterio de similitud máxima. 

\subsection{Algoritmos Particionales}
\noindent Teniendo en cuenta como deben ser los clusterings el objetivo es conseguir $g$ clusters que provocan una partición exhaustiva del conjunto de observaciones. 
Sea el espacio métrico $(\Omega,d)$ se puede considerar que la \textit{dispersión total de las muestras} dada un clustering con $g$ clusters tiene la siguiente expresión:
\begin{equation}
\textbf{T}=\textbf{B}+\textbf{W}
\end{equation}
\noindent Donde $\mathbf{B}$ es la distancia inter-clusters y $\mathbf{W}$ es la distancia intra-cluster del clustering realizado.\\
\noindent Por tanto, de acuerdo a nuestro objetivo un clustering en $g$ clusters debe ser aquel que minimice $\mathbf{W}$ y maximice $\mathbf{B}$

\noindent El número de posibles clusterings de un mismo conjunto de $n$ observaciones es de $\frac{g^n}{g!}$. Pero el número que siempre minimizará la $tr(\mathbf{W})$ que nos da la suma de las varianzas dentro de los grupos, es utilizar tantos clusters como observaciones, pero \emph{Peña, D.} \cite{Peña 2002} propone hacer un test estadístico con el siguiente estadígrafo, aunque no se suelan dar las hipótesis necesarias para aplicar una \emph{F}:
\begin{equation}
F=\dfrac{tr(\mathbf{W}(g))-tr(\mathbf{W}(g+1))}{\frac{tr(\mathbf{W}(g+1))}{(n-g-1)}}
\end{equation}

\noindent Lo más habitual es introducir otro grupo si este cociente presenta un valor mayor que 10. Este cociente compara la disminución de variabilidad de los datos con la varianza promedio. Por tanto, el procedimiento es aplicar el algoritmo de K-medias para distintos número de clusters y hacer la comparación. 

\noindent 



\noindent \textbf{Algoritmo de K-medias y K-medoides}\\
Este algoritmo requiere que se prefije el número de clusters que se van a utilizar, además de que las variables sean cuantitativas ya que los centroides son las medias aritméticas, operación que con variables no numéricas no es posible. \\
\begin{enumerate}
\hrule

\item Se inicializa con $g$ puntos aleatorios del espacio $\mathbb{R}^p$. Se asigna a cada observación un cluster de acuerdo a cual de los $g$ puntos es más cercano.
\item Se calculan los puntos medios de cada cluster y se reasignan las observaciones de acuerdo a cual está más próximo 
\item Se repite el paso anterior, calculando $\mathbf{W}$ en cada paso y se detiene el proceso cuando no haya mejora ninguna o se de un criterio de parada. 

\end{enumerate}
\hrule

\noindent Este algoritmo es dependiente según \emph{Peña, D.}\cite{Peña 2002} de la asignación inicial de las observaciones, de manera que es conveniente para eliminar esa aleatoriedad hacer varias iteraciones del propio algoritmo. 

\noindent El algoritmo de K-medias es un \emph{algoritmo voraz}, de manera que se busca minimizar de manera heurística la suma de las distancias cuadradas de las observaciones al centro del cluster, es decir, en realidad se busca minimizar la varianza dentro de los grupos, es decir, se busca minimizar $tr(\mathbf{W})$ es por eso que también 


\noindent Además hay que destacar que la restricción de las variables cuantitativas debe ser solventada, esto se hace planteando el método de K-medoides que se desarrolla de la siguiente manera :

\begin{enumerate}
\hrule

\item Para una asignación de clusters se calculan las distancias entre los puntos de cada cluster. Y se toma de cada cluster el que minimice la suma las distancias del cluster. Que serán los medoides de cada cluster. 
\item Se asigna a cada punto el cluster cuyo medoide sea más cercano a dicha observación 
\item Se repiten los pasos anteriores hasta que las asignaciones no cambien. 

\end{enumerate}
\hrule
 
\noindent Por tanto, los puntos de referencia de este algoritmo son observaciones, no como las medias que pueden no serlo. Además es mucho más costoso a nivel computacional. 

\noindent Adicionalmente, este es uno de los algoritmos afectados por la maldición de la dimensionalidad en la que al aumentar la dimensión del espacio de posibles observaciones cada vez es menos denso en información y necesitamos más muestras. 

\noindent Estos métodos particionales permiten establecer una clasificación en grupos homogéneos sin tener que 